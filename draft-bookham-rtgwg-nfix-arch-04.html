<!DOCTYPE html>
<html lang="en" class="Internet-Draft">
<head>
<meta charset="utf-8">
<meta content="Common" name="scripts">
<meta content="initial-scale=1.0" name="viewport">
<title>An Architecture for Network Function Interconnect</title>
<meta content="Colin Bookham" name="author">
<meta content="Andrew Stone" name="author">
<meta content="Jeff Tantsura" name="author">
<meta content="Muhammad Durrani" name="author">
<meta content="Bruno Decraene" name="author">
<meta content="
       The emergence of technologies such as 5G, the Internet of Things
        (IoT), and Industry 4.0, coupled with the move towards network
        function virtualization, means that the service requirements demanded
        from networks are changing. This document describes an architecture
        for a Network Function Interconnect (NFIX) that allows for interworking
        of physical and virtual network functions in a unified and scalable
        manner across wide-area network and data center domains while
        maintaining the ability to deliver against SLAs. 
    " name="description">
<meta content="xml2rfc 3.12.0" name="generator">
<meta content="draft-bookham-rtgwg-nfix-arch-04" name="ietf.draft">
<!-- Generator version information:
  xml2rfc 3.12.0
    Python 3.6.12
    appdirs 1.4.4
    ConfigArgParse 1.5.3
    google-i18n-address 2.5.0
    html5lib 1.1
    intervaltree 3.1.0
    Jinja2 2.11.3
    kitchen 1.2.6
    lxml 4.6.4
    pycountry 20.7.3
    pyflakes 2.4.0
    PyYAML 6.0
    requests 2.26.0
    setuptools 59.5.0
    six 1.16.0
    WeasyPrint 52.5
-->
<link href="/tmp/draft-bookham-rtgwg-nfix-arch-04-vi2stqya.xml" rel="alternate" type="application/rfc+xml">
<link href="#copyright" rel="license">
<style type="text/css">/*

  NOTE: Changes at the bottom of this file overrides some earlier settings.

  Once the style has stabilized and has been adopted as an official RFC style,
  this can be consolidated so that style settings occur only in one place, but
  for now the contents of this file consists first of the initial CSS work as
  provided to the RFC Formatter (xml2rfc) work, followed by itemized and
  commented changes found necssary during the development of the v3
  formatters.

*/

/* fonts */
@import url('https://fonts.googleapis.com/css?family=Noto+Sans'); /* Sans-serif */
@import url('https://fonts.googleapis.com/css?family=Noto+Serif'); /* Serif (print) */
@import url('https://fonts.googleapis.com/css?family=Roboto+Mono'); /* Monospace */

@viewport {
  zoom: 1.0;
  width: extend-to-zoom;
}
@-ms-viewport {
  width: extend-to-zoom;
  zoom: 1.0;
}
/* general and mobile first */
html {
}
body {
  max-width: 90%;
  margin: 1.5em auto;
  color: #222;
  background-color: #fff;
  font-size: 14px;
  font-family: 'Noto Sans', Arial, Helvetica, sans-serif;
  line-height: 1.6;
  scroll-behavior: smooth;
}
.ears {
  display: none;
}

/* headings */
#title, h1, h2, h3, h4, h5, h6 {
  margin: 1em 0 0.5em;
  font-weight: bold;
  line-height: 1.3;
}
#title {
  clear: both;
  border-bottom: 1px solid #ddd;
  margin: 0 0 0.5em 0;
  padding: 1em 0 0.5em;
}
.author {
  padding-bottom: 4px;
}
h1 {
  font-size: 26px;
  margin: 1em 0;
}
h2 {
  font-size: 22px;
  margin-top: -20px;  /* provide offset for in-page anchors */
  padding-top: 33px;
}
h3 {
  font-size: 18px;
  margin-top: -36px;  /* provide offset for in-page anchors */
  padding-top: 42px;
}
h4 {
  font-size: 16px;
  margin-top: -36px;  /* provide offset for in-page anchors */
  padding-top: 42px;
}
h5, h6 {
  font-size: 14px;
}
#n-copyright-notice {
  border-bottom: 1px solid #ddd;
  padding-bottom: 1em;
  margin-bottom: 1em;
}
/* general structure */
p {
  padding: 0;
  margin: 0 0 1em 0;
  text-align: left;
}
div, span {
  position: relative;
}
div {
  margin: 0;
}
.alignRight.art-text {
  background-color: #f9f9f9;
  border: 1px solid #eee;
  border-radius: 3px;
  padding: 1em 1em 0;
  margin-bottom: 1.5em;
}
.alignRight.art-text pre {
  padding: 0;
}
.alignRight {
  margin: 1em 0;
}
.alignRight > *:first-child {
  border: none;
  margin: 0;
  float: right;
  clear: both;
}
.alignRight > *:nth-child(2) {
  clear: both;
  display: block;
  border: none;
}
svg {
  display: block;
}
.alignCenter.art-text {
  background-color: #f9f9f9;
  border: 1px solid #eee;
  border-radius: 3px;
  padding: 1em 1em 0;
  margin-bottom: 1.5em;
}
.alignCenter.art-text pre {
  padding: 0;
}
.alignCenter {
  margin: 1em 0;
}
.alignCenter > *:first-child {
  border: none;
  /* this isn't optimal, but it's an existence proof.  PrinceXML doesn't
     support flexbox yet.
  */
  display: table;
  margin: 0 auto;
}

/* lists */
ol, ul {
  padding: 0;
  margin: 0 0 1em 2em;
}
ol ol, ul ul, ol ul, ul ol {
  margin-left: 1em;
}
li {
  margin: 0 0 0.25em 0;
}
.ulCompact li {
  margin: 0;
}
ul.empty, .ulEmpty {
  list-style-type: none;
}
ul.empty li, .ulEmpty li {
  margin-top: 0.5em;
}
ul.ulBare, li.ulBare {
  margin-left: 0em !important;
}
ul.compact, .ulCompact,
ol.compact, .olCompact {
  line-height: 100%;
  margin: 0 0 0 2em;
}

/* definition lists */
dl {
}
dl > dt {
  float: left;
  margin-right: 1em;
}
/* 
dl.nohang > dt {
  float: none;
}
*/
dl > dd {
  margin-bottom: .8em;
  min-height: 1.3em;
}
dl.compact > dd, .dlCompact > dd {
  margin-bottom: 0em;
}
dl > dd > dl {
  margin-top: 0.5em;
  margin-bottom: 0em;
}

/* links */
a {
  text-decoration: none;
}
a[href] {
  color: #22e; /* Arlen: WCAG 2019 */
}
a[href]:hover {
  background-color: #f2f2f2;
}
figcaption a[href],
a[href].selfRef {
  color: #222;
}
/* XXX probably not this:
a.selfRef:hover {
  background-color: transparent;
  cursor: default;
} */

/* Figures */
tt, code, pre, code {
  background-color: #f9f9f9;
  font-family: 'Roboto Mono', monospace;
}
pre {
  border: 1px solid #eee;
  margin: 0;
  padding: 1em;
}
img {
  max-width: 100%;
}
figure {
  margin: 0;
}
figure blockquote {
  margin: 0.8em 0.4em 0.4em;
}
figcaption {
  font-style: italic;
  margin: 0 0 1em 0;
}
@media screen {
  pre {
    overflow-x: auto;
    max-width: 100%;
    max-width: calc(100% - 22px);
  }
}

/* aside, blockquote */
aside, blockquote {
  margin-left: 0;
  padding: 1.2em 2em;
}
blockquote {
  background-color: #f9f9f9;
  color: #111; /* Arlen: WCAG 2019 */
  border: 1px solid #ddd;
  border-radius: 3px;
  margin: 1em 0;
}
cite {
  display: block;
  text-align: right;
  font-style: italic;
}

/* tables */
table {
  width: 100%;
  margin: 0 0 1em;
  border-collapse: collapse;
  border: 1px solid #eee;
}
th, td {
  text-align: left;
  vertical-align: top;
  padding: 0.5em 0.75em;
}
th {
  text-align: left;
  background-color: #e9e9e9;
}
tr:nth-child(2n+1) > td {
  background-color: #f5f5f5;
}
table caption {
  font-style: italic;
  margin: 0;
  padding: 0;
  text-align: left;
}
table p {
  /* XXX to avoid bottom margin on table row signifiers. If paragraphs should
     be allowed within tables more generally, it would be far better to select on a class. */
  margin: 0;
}

/* pilcrow */
a.pilcrow {
  color: #666; /* Arlen: AHDJ 2019 */
  text-decoration: none;
  visibility: hidden;
  user-select: none;
  -ms-user-select: none;
  -o-user-select:none;
  -moz-user-select: none;
  -khtml-user-select: none;
  -webkit-user-select: none;
  -webkit-touch-callout: none;
}
@media screen {
  aside:hover > a.pilcrow,
  p:hover > a.pilcrow,
  blockquote:hover > a.pilcrow,
  div:hover > a.pilcrow,
  li:hover > a.pilcrow,
  pre:hover > a.pilcrow {
    visibility: visible;
  }
  a.pilcrow:hover {
    background-color: transparent;
  }
}

/* misc */
hr {
  border: 0;
  border-top: 1px solid #eee;
}
.bcp14 {
  font-variant: small-caps;
}

.role {
  font-variant: all-small-caps;
}

/* info block */
#identifiers {
  margin: 0;
  font-size: 0.9em;
}
#identifiers dt {
  width: 3em;
  clear: left;
}
#identifiers dd {
  float: left;
  margin-bottom: 0;
}
/* Fix PDF info block run off issue */
@media print {
  #identifiers dd {
    float: none;
  }
}
#identifiers .authors .author {
  display: inline-block;
  margin-right: 1.5em;
}
#identifiers .authors .org {
  font-style: italic;
}

/* The prepared/rendered info at the very bottom of the page */
.docInfo {
  color: #666; /* Arlen: WCAG 2019 */
  font-size: 0.9em;
  font-style: italic;
  margin-top: 2em;
}
.docInfo .prepared {
  float: left;
}
.docInfo .prepared {
  float: right;
}

/* table of contents */
#toc  {
  padding: 0.75em 0 2em 0;
  margin-bottom: 1em;
}
nav.toc ul {
  margin: 0 0.5em 0 0;
  padding: 0;
  list-style: none;
}
nav.toc li {
  line-height: 1.3em;
  margin: 0.75em 0;
  padding-left: 1.2em;
  text-indent: -1.2em;
}
/* references */
.references dt {
  text-align: right;
  font-weight: bold;
  min-width: 7em;
}
.references dd {
  margin-left: 8em;
  overflow: auto;
}

.refInstance {
  margin-bottom: 1.25em;
}

.references .ascii {
  margin-bottom: 0.25em;
}

/* index */
.index ul {
  margin: 0 0 0 1em;
  padding: 0;
  list-style: none;
}
.index ul ul {
  margin: 0;
}
.index li {
  margin: 0;
  text-indent: -2em;
  padding-left: 2em;
  padding-bottom: 5px;
}
.indexIndex {
  margin: 0.5em 0 1em;
}
.index a {
  font-weight: 700;
}
/* make the index two-column on all but the smallest screens */
@media (min-width: 600px) {
  .index ul {
    -moz-column-count: 2;
    -moz-column-gap: 20px;
  }
  .index ul ul {
    -moz-column-count: 1;
    -moz-column-gap: 0;
  }
}

/* authors */
address.vcard {
  font-style: normal;
  margin: 1em 0;
}

address.vcard .nameRole {
  font-weight: 700;
  margin-left: 0;
}
address.vcard .label {
  font-family: "Noto Sans",Arial,Helvetica,sans-serif;
  margin: 0.5em 0;
}
address.vcard .type {
  display: none;
}
.alternative-contact {
  margin: 1.5em 0 1em;
}
hr.addr {
  border-top: 1px dashed;
  margin: 0;
  color: #ddd;
  max-width: calc(100% - 16px);
}

/* temporary notes */
.rfcEditorRemove::before {
  position: absolute;
  top: 0.2em;
  right: 0.2em;
  padding: 0.2em;
  content: "The RFC Editor will remove this note";
  color: #9e2a00; /* Arlen: WCAG 2019 */
  background-color: #ffd; /* Arlen: WCAG 2019 */
}
.rfcEditorRemove {
  position: relative;
  padding-top: 1.8em;
  background-color: #ffd; /* Arlen: WCAG 2019 */
  border-radius: 3px;
}
.cref {
  background-color: #ffd; /* Arlen: WCAG 2019 */
  padding: 2px 4px;
}
.crefSource {
  font-style: italic;
}
/* alternative layout for smaller screens */
@media screen and (max-width: 1023px) {
  body {
    padding-top: 2em;
  }
  #title {
    padding: 1em 0;
  }
  h1 {
    font-size: 24px;
  }
  h2 {
    font-size: 20px;
    margin-top: -18px;  /* provide offset for in-page anchors */
    padding-top: 38px;
  }
  #identifiers dd {
    max-width: 60%;
  }
  #toc {
    position: fixed;
    z-index: 2;
    top: 0;
    right: 0;
    padding: 0;
    margin: 0;
    background-color: inherit;
    border-bottom: 1px solid #ccc;
  }
  #toc h2 {
    margin: -1px 0 0 0;
    padding: 4px 0 4px 6px;
    padding-right: 1em;
    min-width: 190px;
    font-size: 1.1em;
    text-align: right;
    background-color: #444;
    color: white;
    cursor: pointer;
  }
  #toc h2::before { /* css hamburger */
    float: right;
    position: relative;
    width: 1em;
    height: 1px;
    left: -164px;
    margin: 6px 0 0 0;
    background: white none repeat scroll 0 0;
    box-shadow: 0 4px 0 0 white, 0 8px 0 0 white;
    content: "";
  }
  #toc nav {
    display: none;
    padding: 0.5em 1em 1em;
    overflow: auto;
    height: calc(100vh - 48px);
    border-left: 1px solid #ddd;
  }
}

/* alternative layout for wide screens */
@media screen and (min-width: 1024px) {
  body {
    max-width: 724px;
    margin: 42px auto;
    padding-left: 1.5em;
    padding-right: 29em;
  }
  #toc {
    position: fixed;
    top: 42px;
    right: 42px;
    width: 25%;
    margin: 0;
    padding: 0 1em;
    z-index: 1;
  }
  #toc h2 {
    border-top: none;
    border-bottom: 1px solid #ddd;
    font-size: 1em;
    font-weight: normal;
    margin: 0;
    padding: 0.25em 1em 1em 0;
  }
  #toc nav {
    display: block;
    height: calc(90vh - 84px);
    bottom: 0;
    padding: 0.5em 0 0;
    overflow: auto;
  }
  img { /* future proofing */
    max-width: 100%;
    height: auto;
  }
}

/* pagination */
@media print {
  body {

    width: 100%;
  }
  p {
    orphans: 3;
    widows: 3;
  }
  #n-copyright-notice {
    border-bottom: none;
  }
  #toc, #n-introduction {
    page-break-before: always;
  }
  #toc {
    border-top: none;
    padding-top: 0;
  }
  figure, pre {
    page-break-inside: avoid;
  }
  figure {
    overflow: scroll;
  }
  h1, h2, h3, h4, h5, h6 {
    page-break-after: avoid;
  }
  h2+*, h3+*, h4+*, h5+*, h6+* {
    page-break-before: avoid;
  }
  pre {
    white-space: pre-wrap;
    word-wrap: break-word;
    font-size: 10pt;
  }
  table {
    border: 1px solid #ddd;
  }
  td {
    border-top: 1px solid #ddd;
  }
}

/* This is commented out here, as the string-set: doesn't
   pass W3C validation currently */
/*
.ears thead .left {
  string-set: ears-top-left content();
}

.ears thead .center {
  string-set: ears-top-center content();
}

.ears thead .right {
  string-set: ears-top-right content();
}

.ears tfoot .left {
  string-set: ears-bottom-left content();
}

.ears tfoot .center {
  string-set: ears-bottom-center content();
}

.ears tfoot .right {
  string-set: ears-bottom-right content();
}
*/

@page :first {
  padding-top: 0;
  @top-left {
    content: normal;
    border: none;
  }
  @top-center {
    content: normal;
    border: none;
  }
  @top-right {
    content: normal;
    border: none;
  }
}

@page {
  size: A4;
  margin-bottom: 45mm;
  padding-top: 20px;
  /* The follwing is commented out here, but set appropriately by in code, as
     the content depends on the document */
  /*
  @top-left {
    content: 'Internet-Draft';
    vertical-align: bottom;
    border-bottom: solid 1px #ccc;
  }
  @top-left {
    content: string(ears-top-left);
    vertical-align: bottom;
    border-bottom: solid 1px #ccc;
  }
  @top-center {
    content: string(ears-top-center);
    vertical-align: bottom;
    border-bottom: solid 1px #ccc;
  }
  @top-right {
    content: string(ears-top-right);
    vertical-align: bottom;
    border-bottom: solid 1px #ccc;
  }
  @bottom-left {
    content: string(ears-bottom-left);
    vertical-align: top;
    border-top: solid 1px #ccc;
  }
  @bottom-center {
    content: string(ears-bottom-center);
    vertical-align: top;
    border-top: solid 1px #ccc;
  }
  @bottom-right {
      content: '[Page ' counter(page) ']';
      vertical-align: top;
      border-top: solid 1px #ccc;
  }
  */

}

/* Changes introduced to fix issues found during implementation */
/* Make sure links are clickable even if overlapped by following H* */
a {
  z-index: 2;
}
/* Separate body from document info even without intervening H1 */
section {
  clear: both;
}


/* Top align author divs, to avoid names without organization dropping level with org names */
.author {
  vertical-align: top;
}

/* Leave room in document info to show Internet-Draft on one line */
#identifiers dt {
  width: 8em;
}

/* Don't waste quite as much whitespace between label and value in doc info */
#identifiers dd {
  margin-left: 1em;
}

/* Give floating toc a background color (needed when it's a div inside section */
#toc {
  background-color: white;
}

/* Make the collapsed ToC header render white on gray also when it's a link */
@media screen and (max-width: 1023px) {
  #toc h2 a,
  #toc h2 a:link,
  #toc h2 a:focus,
  #toc h2 a:hover,
  #toc a.toplink,
  #toc a.toplink:hover {
    color: white;
    background-color: #444;
    text-decoration: none;
  }
}

/* Give the bottom of the ToC some whitespace */
@media screen and (min-width: 1024px) {
  #toc {
    padding: 0 0 1em 1em;
  }
}

/* Style section numbers with more space between number and title */
.section-number {
  padding-right: 0.5em;
}

/* prevent monospace from becoming overly large */
tt, code, pre, code {
  font-size: 95%;
}

/* Fix the height/width aspect for ascii art*/
pre.sourcecode,
.art-text pre {
  line-height: 1.12;
}


/* Add styling for a link in the ToC that points to the top of the document */
a.toplink {
  float: right;
  margin-right: 0.5em;
}

/* Fix the dl styling to match the RFC 7992 attributes */
dl > dt,
dl.dlParallel > dt {
  float: left;
  margin-right: 1em;
}
dl.dlNewline > dt {
  float: none;
}

/* Provide styling for table cell text alignment */
table td.text-left,
table th.text-left {
  text-align: left;
}
table td.text-center,
table th.text-center {
  text-align: center;
}
table td.text-right,
table th.text-right {
  text-align: right;
}

/* Make the alternative author contact informatio look less like just another
   author, and group it closer with the primary author contact information */
.alternative-contact {
  margin: 0.5em 0 0.25em 0;
}
address .non-ascii {
  margin: 0 0 0 2em;
}

/* With it being possible to set tables with alignment
  left, center, and right, { width: 100%; } does not make sense */
table {
  width: auto;
}

/* Avoid reference text that sits in a block with very wide left margin,
   because of a long floating dt label.*/
.references dd {
  overflow: visible;
}

/* Control caption placement */
caption {
  caption-side: bottom;
}

/* Limit the width of the author address vcard, so names in right-to-left
   script don't end up on the other side of the page. */

address.vcard {
  max-width: 30em;
  margin-right: auto;
}

/* For address alignment dependent on LTR or RTL scripts */
address div.left {
  text-align: left;
}
address div.right {
  text-align: right;
}

/* Provide table alignment support.  We can't use the alignX classes above
   since they do unwanted things with caption and other styling. */
table.right {
 margin-left: auto;
 margin-right: 0;
}
table.center {
 margin-left: auto;
 margin-right: auto;
}
table.left {
 margin-left: 0;
 margin-right: auto;
}

/* Give the table caption label the same styling as the figcaption */
caption a[href] {
  color: #222;
}

@media print {
  .toplink {
    display: none;
  }

  /* avoid overwriting the top border line with the ToC header */
  #toc {
    padding-top: 1px;
  }

  /* Avoid page breaks inside dl and author address entries */
  .vcard {
    page-break-inside: avoid;
  }

}
/* Tweak the bcp14 keyword presentation */
.bcp14 {
  font-variant: small-caps;
  font-weight: bold;
  font-size: 0.9em;
}
/* Tweak the invisible space above H* in order not to overlay links in text above */
 h2 {
  margin-top: -18px;  /* provide offset for in-page anchors */
  padding-top: 31px;
 }
 h3 {
  margin-top: -18px;  /* provide offset for in-page anchors */
  padding-top: 24px;
 }
 h4 {
  margin-top: -18px;  /* provide offset for in-page anchors */
  padding-top: 24px;
 }
/* Float artwork pilcrow to the right */
@media screen {
  .artwork a.pilcrow {
    display: block;
    line-height: 0.7;
    margin-top: 0.15em;
  }
}
/* Make pilcrows on dd visible */
@media screen {
  dd:hover > a.pilcrow {
    visibility: visible;
  }
}
/* Make the placement of figcaption match that of a table's caption
   by removing the figure's added bottom margin */
.alignLeft.art-text,
.alignCenter.art-text,
.alignRight.art-text {
   margin-bottom: 0;
}
.alignLeft,
.alignCenter,
.alignRight {
  margin: 1em 0 0 0;
}
/* In print, the pilcrow won't show on hover, so prevent it from taking up space,
   possibly even requiring a new line */
@media print {
  a.pilcrow {
    display: none;
  }
}
/* Styling for the external metadata */
div#external-metadata {
  background-color: #eee;
  padding: 0.5em;
  margin-bottom: 0.5em;
  display: none;
}
div#internal-metadata {
  padding: 0.5em;                       /* to match the external-metadata padding */
}
/* Styling for title RFC Number */
h1#rfcnum {
  clear: both;
  margin: 0 0 -1em;
  padding: 1em 0 0 0;
}
/* Make .olPercent look the same as <ol><li> */
dl.olPercent > dd {
  margin-bottom: 0.25em;
  min-height: initial;
}
/* Give aside some styling to set it apart */
aside {
  border-left: 1px solid #ddd;
  margin: 1em 0 1em 2em;
  padding: 0.2em 2em;
}
aside > dl,
aside > ol,
aside > ul,
aside > table,
aside > p {
  margin-bottom: 0.5em;
}
/* Additional page break settings */
@media print {
  figcaption, table caption {
    page-break-before: avoid;
  }
}
/* Font size adjustments for print */
@media print {
  body  { font-size: 10pt;      line-height: normal; max-width: 96%; }
  h1    { font-size: 1.72em;    padding-top: 1.5em; } /* 1*1.2*1.2*1.2 */
  h2    { font-size: 1.44em;    padding-top: 1.5em; } /* 1*1.2*1.2 */
  h3    { font-size: 1.2em;     padding-top: 1.5em; } /* 1*1.2 */
  h4    { font-size: 1em;       padding-top: 1.5em; }
  h5, h6 { font-size: 1em;      margin: initial; padding: 0.5em 0 0.3em; }
}
/* Sourcecode margin in print, when there's no pilcrow */
@media print {
  .artwork,
  .sourcecode {
    margin-bottom: 1em;
  }
}
/* Avoid narrow tables forcing too narrow table captions, which may render badly */
table {
  min-width: 20em;
}
/* ol type a */
ol.type-a { list-style-type: lower-alpha; }
ol.type-A { list-style-type: upper-alpha; }
ol.type-i { list-style-type: lower-roman; }
ol.type-I { list-style-type: lower-roman; }
/* Apply the print table and row borders in general, on request from the RPC,
and increase the contrast between border and odd row background sligthtly */
table {
  border: 1px solid #ddd;
}
td {
  border-top: 1px solid #ddd;
}
tr:nth-child(2n+1) > td {
  background-color: #f8f8f8;
}
/* Use style rules to govern display of the TOC. */
@media screen and (max-width: 1023px) {
  #toc nav { display: none; }
  #toc.active nav { display: block; }
}
/* Add support for keepWithNext */
.keepWithNext {
  break-after: avoid-page;
  break-after: avoid-page;
}
/* Add support for keepWithPrevious */
.keepWithPrevious {
  break-before: avoid-page;
}
/* Change the approach to avoiding breaks inside artwork etc. */
figure, pre, table, .artwork, .sourcecode  {
  break-before: auto;
  break-after: auto;
}
/* Avoid breaks between <dt> and <dd> */
dl {
  break-before: auto;
  break-inside: auto;
}
dt {
  break-before: auto;
  break-after: avoid-page;
}
dd {
  break-before: avoid-page;
  break-after: auto;
  orphans: 3;
  widows: 3
}
span.break, dd.break {
  margin-bottom: 0;
  min-height: 0;
  break-before: auto;
  break-inside: auto;
  break-after: auto;
}
/* Undo break-before ToC */
@media print {
  #toc {
    break-before: auto;
  }
}
/* Text in compact lists should not get extra bottim margin space,
   since that would makes the list not compact */
ul.compact p, .ulCompact p,
ol.compact p, .olCompact p {
 margin: 0;
}
/* But the list as a whole needs the extra space at the end */
section ul.compact,
section .ulCompact,
section ol.compact,
section .olCompact {
  margin-bottom: 1em;                    /* same as p not within ul.compact etc. */
}
/* The tt and code background above interferes with for instance table cell
   backgrounds.  Changed to something a bit more selective. */
tt, code {
  background-color: transparent;
}
p tt, p code, li tt, li code {
  background-color: #f8f8f8;
}
/* Tweak the pre margin -- 0px doesn't come out well */
pre {
   margin-top: 0.5px;
}
/* Tweak the comact list text */
ul.compact, .ulCompact,
ol.compact, .olCompact,
dl.compact, .dlCompact {
  line-height: normal;
}
/* Don't add top margin for nested lists */
li > ul, li > ol, li > dl,
dd > ul, dd > ol, dd > dl,
dl > dd > dl {
  margin-top: initial;
}
/* Elements that should not be rendered on the same line as a <dt> */
/* This should match the element list in writer.text.TextWriter.render_dl() */
dd > div.artwork:first-child,
dd > aside:first-child,
dd > figure:first-child,
dd > ol:first-child,
dd > div:first-child > pre.sourcecode,
dd > table:first-child,
dd > ul:first-child {
  clear: left;
}
/* fix for weird browser behaviour when <dd/> is empty */
dt+dd:empty::before{
  content: "\00a0";
}
/* Make paragraph spacing inside <li> smaller than in body text, to fit better within the list */
li > p {
  margin-bottom: 0.5em
}
/* Don't let p margin spill out from inside list items */
li > p:last-of-type {
  margin-bottom: 0;
}
</style>
<link href="rfc-local.css" rel="stylesheet" type="text/css">
<script type="application/javascript">async function addMetadata(){try{const e=document.styleSheets[0].cssRules;for(let t=0;t<e.length;t++)if(/#identifiers/.exec(e[t].selectorText)){const a=e[t].cssText.replace("#identifiers","#external-updates");document.styleSheets[0].insertRule(a,document.styleSheets[0].cssRules.length)}}catch(e){console.log(e)}const e=document.getElementById("external-metadata");if(e)try{var t,a="",o=function(e){const t=document.getElementsByTagName("meta");for(let a=0;a<t.length;a++)if(t[a].getAttribute("name")===e)return t[a].getAttribute("content");return""}("rfc.number");if(o){t="https://www.rfc-editor.org/rfc/rfc"+o+".json";try{const e=await fetch(t);a=await e.json()}catch(e){t=document.URL.indexOf("html")>=0?document.URL.replace(/html$/,"json"):document.URL+".json";const o=await fetch(t);a=await o.json()}}if(!a)return;e.style.display="block";const s="",d="https://datatracker.ietf.org/doc",n="https://datatracker.ietf.org/ipr/search",c="https://www.rfc-editor.org/info",l=a.doc_id.toLowerCase(),i=a.doc_id.slice(0,3).toLowerCase(),f=a.doc_id.slice(3).replace(/^0+/,""),u={status:"Status",obsoletes:"Obsoletes",obsoleted_by:"Obsoleted By",updates:"Updates",updated_by:"Updated By",see_also:"See Also",errata_url:"Errata"};let h="<dl style='overflow:hidden' id='external-updates'>";["status","obsoletes","obsoleted_by","updates","updated_by","see_also","errata_url"].forEach(e=>{if("status"==e){a[e]=a[e].toLowerCase();var t=a[e].split(" "),o=t.length,w="",p=1;for(let e=0;e<o;e++)p<o?w=w+r(t[e])+" ":w+=r(t[e]),p++;a[e]=w}else if("obsoletes"==e||"obsoleted_by"==e||"updates"==e||"updated_by"==e){var g,m="",b=1;g=a[e].length;for(let t=0;t<g;t++)a[e][t]&&(a[e][t]=String(a[e][t]).toLowerCase(),m=b<g?m+"<a href='"+s+"/rfc/".concat(a[e][t])+"'>"+a[e][t].slice(3)+"</a>, ":m+"<a href='"+s+"/rfc/".concat(a[e][t])+"'>"+a[e][t].slice(3)+"</a>",b++);a[e]=m}else if("see_also"==e){var y,L="",C=1;y=a[e].length;for(let t=0;t<y;t++)if(a[e][t]){a[e][t]=String(a[e][t]);var _=a[e][t].slice(0,3),v=a[e][t].slice(3).replace(/^0+/,"");L=C<y?"RFC"!=_?L+"<a href='"+s+"/info/"+_.toLowerCase().concat(v.toLowerCase())+"'>"+_+" "+v+"</a>, ":L+"<a href='"+s+"/info/"+_.toLowerCase().concat(v.toLowerCase())+"'>"+v+"</a>, ":"RFC"!=_?L+"<a href='"+s+"/info/"+_.toLowerCase().concat(v.toLowerCase())+"'>"+_+" "+v+"</a>":L+"<a href='"+s+"/info/"+_.toLowerCase().concat(v.toLowerCase())+"'>"+v+"</a>",C++}a[e]=L}else if("errata_url"==e){var R="";R=a[e]?R+"<a href='"+a[e]+"'>Errata exist</a> | <a href='"+d+"/"+l+"'>Datatracker</a>| <a href='"+n+"/?"+i+"="+f+"&submit="+i+"'>IPR</a> | <a href='"+c+"/"+l+"'>Info page</a>":"<a href='"+d+"/"+l+"'>Datatracker</a> | <a href='"+n+"/?"+i+"="+f+"&submit="+i+"'>IPR</a> | <a href='"+c+"/"+l+"'>Info page</a>",a[e]=R}""!=a[e]?"Errata"==u[e]?h+=`<dt>More info:</dt><dd>${a[e]}</dd>`:h+=`<dt>${u[e]}:</dt><dd>${a[e]}</dd>`:"Errata"==u[e]&&(h+=`<dt>More info:</dt><dd>${a[e]}</dd>`)}),h+="</dl>",e.innerHTML=h}catch(e){console.log(e)}else console.log("Could not locate metadata <div> element");function r(e){return e.charAt(0).toUpperCase()+e.slice(1)}}window.removeEventListener("load",addMetadata),window.addEventListener("load",addMetadata);</script>
</head>
<body>
<script src="metadata.min.js"></script>
<table class="ears">
<thead><tr>
<td class="left">Internet-Draft</td>
<td class="center">Network Function Interconnect</td>
<td class="right">January 2022</td>
</tr></thead>
<tfoot><tr>
<td class="left">Bookham, et al.</td>
<td class="center">Expires 9 July 2022</td>
<td class="right">[Page]</td>
</tr></tfoot>
</table>
<div id="external-metadata" class="document-information"></div>
<div id="internal-metadata" class="document-information">
<dl id="identifiers">
<dt class="label-workgroup">Workgroup:</dt>
<dd class="workgroup">RTG Working Group</dd>
<dt class="label-internet-draft">Internet-Draft:</dt>
<dd class="internet-draft">draft-bookham-rtgwg-nfix-arch-04</dd>
<dt class="label-published">Published:</dt>
<dd class="published">
<time datetime="2022-01-05" class="published">5 January 2022</time>
    </dd>
<dt class="label-intended-status">Intended Status:</dt>
<dd class="intended-status">Informational</dd>
<dt class="label-expires">Expires:</dt>
<dd class="expires"><time datetime="2022-07-09">9 July 2022</time></dd>
<dt class="label-authors">Authors:</dt>
<dd class="authors">
<div class="author">
      <div class="author-name">C. Bookham, <span class="editor">Ed.</span>
</div>
<div class="org">Nokia</div>
</div>
<div class="author">
      <div class="author-name">A. Stone</div>
<div class="org">Nokia</div>
</div>
<div class="author">
      <div class="author-name">J. Tantsura</div>
<div class="org">Microsoft</div>
</div>
<div class="author">
      <div class="author-name">M. Durrani</div>
<div class="org">Equinix Inc</div>
</div>
<div class="author">
      <div class="author-name">B. Decraene</div>
<div class="org">Orange</div>
</div>
</dd>
</dl>
</div>
<h1 id="title">An Architecture for Network Function Interconnect</h1>
<section id="section-abstract">
      <h2 id="abstract"><a href="#abstract" class="selfRef">Abstract</a></h2>
<p id="section-abstract-1">The emergence of technologies such as 5G, the Internet of Things
        (IoT), and Industry 4.0, coupled with the move towards network
        function virtualization, means that the service requirements demanded
        from networks are changing. This document describes an architecture
        for a Network Function Interconnect (NFIX) that allows for interworking
        of physical and virtual network functions in a unified and scalable
        manner across wide-area network and data center domains while
        maintaining the ability to deliver against SLAs.<a href="#section-abstract-1" class="pilcrow">¶</a></p>
</section>
<section class="note" id="section-note.1">
      <h2 id="name-requirements-language">
<a href="#name-requirements-language" class="section-name selfRef">Requirements Language</a>
      </h2>
<p id="section-note.1-1">The key words "MUST", "MUST NOT",
        "REQUIRED", "SHALL", "SHALL NOT",
        "SHOULD", "SHOULD NOT", "RECOMMENDED",
        "MAY", and "OPTIONAL" in this document are to be
        interpreted as described in BCP 14 <span>[<a href="#RFC2119" class="xref">RFC2119</a>]</span><span>[<a href="#RFC8174" class="xref">RFC8174</a>]</span>
        when, and only when, they appear in all capitals, as shown here.<a href="#section-note.1-1" class="pilcrow">¶</a></p>
</section>
<div id="status-of-memo">
<section id="section-boilerplate.1">
        <h2 id="name-status-of-this-memo">
<a href="#name-status-of-this-memo" class="section-name selfRef">Status of This Memo</a>
        </h2>
<p id="section-boilerplate.1-1">
        This Internet-Draft is submitted in full conformance with the
        provisions of BCP 78 and BCP 79.<a href="#section-boilerplate.1-1" class="pilcrow">¶</a></p>
<p id="section-boilerplate.1-2">
        Internet-Drafts are working documents of the Internet Engineering Task
        Force (IETF). Note that other groups may also distribute working
        documents as Internet-Drafts. The list of current Internet-Drafts is
        at <span><a href="https://datatracker.ietf.org/drafts/current/">https://datatracker.ietf.org/drafts/current/</a></span>.<a href="#section-boilerplate.1-2" class="pilcrow">¶</a></p>
<p id="section-boilerplate.1-3">
        Internet-Drafts are draft documents valid for a maximum of six months
        and may be updated, replaced, or obsoleted by other documents at any
        time. It is inappropriate to use Internet-Drafts as reference
        material or to cite them other than as "work in progress."<a href="#section-boilerplate.1-3" class="pilcrow">¶</a></p>
<p id="section-boilerplate.1-4">
        This Internet-Draft will expire on 9 July 2022.<a href="#section-boilerplate.1-4" class="pilcrow">¶</a></p>
</section>
</div>
<div id="copyright">
<section id="section-boilerplate.2">
        <h2 id="name-copyright-notice">
<a href="#name-copyright-notice" class="section-name selfRef">Copyright Notice</a>
        </h2>
<p id="section-boilerplate.2-1">
            Copyright (c) 2022 IETF Trust and the persons identified as the
            document authors. All rights reserved.<a href="#section-boilerplate.2-1" class="pilcrow">¶</a></p>
<p id="section-boilerplate.2-2">
            This document is subject to BCP 78 and the IETF Trust's Legal
            Provisions Relating to IETF Documents
            (<span><a href="https://trustee.ietf.org/license-info">https://trustee.ietf.org/license-info</a></span>) in effect on the date of
            publication of this document. Please review these documents
            carefully, as they describe your rights and restrictions with
            respect to this document. Code Components extracted from this
            document must include Revised BSD License text as described in
            Section 4.e of the Trust Legal Provisions and are provided without
            warranty as described in the Revised BSD License.<a href="#section-boilerplate.2-2" class="pilcrow">¶</a></p>
</section>
</div>
<div id="toc">
<section id="section-toc.1">
        <a href="#" onclick="scroll(0,0)" class="toplink">▲</a><h2 id="name-table-of-contents">
<a href="#name-table-of-contents" class="section-name selfRef">Table of Contents</a>
        </h2>
<nav class="toc"><ul class="compact toc ulBare ulEmpty">
<li class="compact toc ulBare ulEmpty" id="section-toc.1-1.1">
            <p id="section-toc.1-1.1.1" class="keepWithNext"><a href="#section-1" class="xref">1</a>.  <a href="#name-introduction" class="xref">Introduction</a></p>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.2">
            <p id="section-toc.1-1.2.1" class="keepWithNext"><a href="#section-2" class="xref">2</a>.  <a href="#name-terminology" class="xref">Terminology</a></p>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.3">
            <p id="section-toc.1-1.3.1" class="keepWithNext"><a href="#section-3" class="xref">3</a>.  <a href="#name-motivation" class="xref">Motivation</a></p>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.4">
            <p id="section-toc.1-1.4.1"><a href="#section-4" class="xref">4</a>.  <a href="#name-requirements" class="xref">Requirements</a></p>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5">
            <p id="section-toc.1-1.5.1"><a href="#section-5" class="xref">5</a>.  <a href="#name-theory-of-operation" class="xref">Theory of Operation</a></p>
<ul class="compact toc ulBare ulEmpty">
<li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.1">
                <p id="section-toc.1-1.5.2.1.1"><a href="#section-5.1" class="xref">5.1</a>.  <a href="#name-vnf-assumptions" class="xref">VNF Assumptions</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.2">
                <p id="section-toc.1-1.5.2.2.1"><a href="#section-5.2" class="xref">5.2</a>.  <a href="#name-overview" class="xref">Overview</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.3">
                <p id="section-toc.1-1.5.2.3.1"><a href="#section-5.3" class="xref">5.3</a>.  <a href="#name-use-of-a-centralized-contro" class="xref">Use of a Centralized Controller</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.4">
                <p id="section-toc.1-1.5.2.4.1"><a href="#section-5.4" class="xref">5.4</a>.  <a href="#name-routing-and-lsp-underlay" class="xref">Routing and LSP Underlay</a></p>
<ul class="compact toc ulBare ulEmpty">
<li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.4.2.1">
                    <p id="section-toc.1-1.5.2.4.2.1.1"><a href="#section-5.4.1" class="xref">5.4.1</a>.  <a href="#name-intra-domain-routing" class="xref">Intra-Domain Routing</a></p>
</li>
                  <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.4.2.2">
                    <p id="section-toc.1-1.5.2.4.2.2.1"><a href="#section-5.4.2" class="xref">5.4.2</a>.  <a href="#name-inter-domain-routing" class="xref">Inter-Domain Routing</a></p>
</li>
                  <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.4.2.3">
                    <p id="section-toc.1-1.5.2.4.2.3.1"><a href="#section-5.4.3" class="xref">5.4.3</a>.  <a href="#name-intra-domain-and-inter-doma" class="xref">Intra-Domain and Inter-Domain Traffic-Engineering</a></p>
</li>
                </ul>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.5">
                <p id="section-toc.1-1.5.2.5.1"><a href="#section-5.5" class="xref">5.5</a>.  <a href="#name-service-layer" class="xref">Service Layer</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.6">
                <p id="section-toc.1-1.5.2.6.1"><a href="#section-5.6" class="xref">5.6</a>.  <a href="#name-service-differentiation" class="xref">Service Differentiation</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.7">
                <p id="section-toc.1-1.5.2.7.1"><a href="#section-5.7" class="xref">5.7</a>.  <a href="#name-automated-service-activatio" class="xref">Automated Service Activation</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.8">
                <p id="section-toc.1-1.5.2.8.1"><a href="#section-5.8" class="xref">5.8</a>.  <a href="#name-service-function-chaining" class="xref">Service Function Chaining</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.9">
                <p id="section-toc.1-1.5.2.9.1"><a href="#section-5.9" class="xref">5.9</a>.  <a href="#name-stability-and-availability" class="xref">Stability and Availability</a></p>
<ul class="compact toc ulBare ulEmpty">
<li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.9.2.1">
                    <p id="section-toc.1-1.5.2.9.2.1.1"><a href="#section-5.9.1" class="xref">5.9.1</a>.  <a href="#name-igp-reconvergence" class="xref">IGP Reconvergence</a></p>
</li>
                  <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.9.2.2">
                    <p id="section-toc.1-1.5.2.9.2.2.1"><a href="#section-5.9.2" class="xref">5.9.2</a>.  <a href="#name-data-center-reconvergence" class="xref">Data Center Reconvergence</a></p>
</li>
                  <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.9.2.3">
                    <p id="section-toc.1-1.5.2.9.2.3.1"><a href="#section-5.9.3" class="xref">5.9.3</a>.  <a href="#name-exchange-of-inter-domain-ro" class="xref">Exchange of Inter-Domain Routes</a></p>
</li>
                  <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.9.2.4">
                    <p id="section-toc.1-1.5.2.9.2.4.1"><a href="#section-5.9.4" class="xref">5.9.4</a>.  <a href="#name-controller-redundancy" class="xref">Controller Redundancy</a></p>
</li>
                  <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.9.2.5">
                    <p id="section-toc.1-1.5.2.9.2.5.1"><a href="#section-5.9.5" class="xref">5.9.5</a>.  <a href="#name-path-and-segment-liveliness" class="xref">Path and Segment Liveliness</a></p>
</li>
                </ul>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.10">
                <p id="section-toc.1-1.5.2.10.1"><a href="#section-5.10" class="xref">5.10</a>. <a href="#name-scalability" class="xref">Scalability</a></p>
<ul class="compact toc ulBare ulEmpty">
<li class="compact toc ulBare ulEmpty" id="section-toc.1-1.5.2.10.2.1">
                    <p id="section-toc.1-1.5.2.10.2.1.1"><a href="#section-5.10.1" class="xref">5.10.1</a>.  <a href="#name-asymmetric-model-b-for-vpn-" class="xref">Asymmetric Model B for VPN Families</a></p>
</li>
                </ul>
</li>
            </ul>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.6">
            <p id="section-toc.1-1.6.1"><a href="#section-6" class="xref">6</a>.  <a href="#name-illustration-of-use" class="xref">Illustration of Use</a></p>
<ul class="compact toc ulBare ulEmpty">
<li class="compact toc ulBare ulEmpty" id="section-toc.1-1.6.2.1">
                <p id="section-toc.1-1.6.2.1.1"><a href="#section-6.1" class="xref">6.1</a>.  <a href="#name-reference-topology" class="xref">Reference Topology</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.6.2.2">
                <p id="section-toc.1-1.6.2.2.1"><a href="#section-6.2" class="xref">6.2</a>.  <a href="#name-pnf-to-pnf-connectivity" class="xref">PNF to PNF Connectivity</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.6.2.3">
                <p id="section-toc.1-1.6.2.3.1"><a href="#section-6.3" class="xref">6.3</a>.  <a href="#name-vnf-to-pnf-connectivity" class="xref">VNF to PNF Connectivity</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.6.2.4">
                <p id="section-toc.1-1.6.2.4.1"><a href="#section-6.4" class="xref">6.4</a>.  <a href="#name-vnf-to-vnf-connectivity" class="xref">VNF to VNF Connectivity</a></p>
</li>
            </ul>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.7">
            <p id="section-toc.1-1.7.1"><a href="#section-7" class="xref">7</a>.  <a href="#name-conclusions" class="xref">Conclusions</a></p>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.8">
            <p id="section-toc.1-1.8.1"><a href="#section-8" class="xref">8</a>.  <a href="#name-security-considerations" class="xref">Security Considerations</a></p>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.9">
            <p id="section-toc.1-1.9.1"><a href="#section-9" class="xref">9</a>.  <a href="#name-acknowledgements" class="xref">Acknowledgements</a></p>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.10">
            <p id="section-toc.1-1.10.1"><a href="#section-10" class="xref">10</a>. <a href="#name-contributors" class="xref">Contributors</a></p>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.11">
            <p id="section-toc.1-1.11.1"><a href="#section-11" class="xref">11</a>. <a href="#name-iana-considerations" class="xref">IANA Considerations</a></p>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.12">
            <p id="section-toc.1-1.12.1"><a href="#section-12" class="xref">12</a>. <a href="#name-references" class="xref">References</a></p>
<ul class="compact toc ulBare ulEmpty">
<li class="compact toc ulBare ulEmpty" id="section-toc.1-1.12.2.1">
                <p id="section-toc.1-1.12.2.1.1"><a href="#section-12.1" class="xref">12.1</a>.  <a href="#name-normative-references" class="xref">Normative References</a></p>
</li>
              <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.12.2.2">
                <p id="section-toc.1-1.12.2.2.1"><a href="#section-12.2" class="xref">12.2</a>.  <a href="#name-informative-references" class="xref">Informative References</a></p>
</li>
            </ul>
</li>
          <li class="compact toc ulBare ulEmpty" id="section-toc.1-1.13">
            <p id="section-toc.1-1.13.1"><a href="#appendix-A" class="xref"></a><a href="#name-authors-addresses" class="xref">Authors' Addresses</a></p>
</li>
        </ul>
</nav>
</section>
</div>
<section id="section-1">
      <h2 id="name-introduction">
<a href="#section-1" class="section-number selfRef">1. </a><a href="#name-introduction" class="section-name selfRef">Introduction</a>
      </h2>
<p id="section-1-1">With the introduction of technologies such as 5G, the Internet of
        Things (IoT), and Industry 4.0, service requirements are changing.
        In addition to the ever-increasing demand for more capacity, these
        services have other stringent service requirements that need to be
        met such as ultra-reliable and/or low-latency communication.<a href="#section-1-1" class="pilcrow">¶</a></p>
<p id="section-1-2">Parallel to this, there is a continued trend to move towards network
        function virtualization. Operators are building digitalized
        infrastructure capable of hosting numerous virtualized network
        functions (VNFs). Infrastructure that can scale in and scale out
        depending on the application demand and can deliver flexibility and
        service velocity. Much of this virtualization activity is driven by
        the afore-mentioned emerging technologies as new infrastructure is
        deployed in support of them. To try and meet the new service
        requirements some of these VNFs are becoming more dispersed, so it is
        common for networks to have a mix of centralized medium- or large-sized
        sized data centers together with more distributed smaller 'edge-clouds'.
        VNFs hosted within these data centers require seamless connectivity to
        each other, and to their existing physical network function (PNF)
        counterparts. This connectivity also needs to deliver against
        agreed SLAs.<a href="#section-1-2" class="pilcrow">¶</a></p>
<p id="section-1-3">Coupled with the deployment of virtualization is automation. Many of
        these VNFs are deployed within SDN-enabled data centers where
        automation is simply a must-have capability to improve service
        activation lead-times. The expectation is that services will be
        instantiated in an abstract point-and-click manner and be automatically
        created by the underlying network, dynamically adapting to service
        connectivity changes as virtual entities move between hosts.<a href="#section-1-3" class="pilcrow">¶</a></p>
<p id="section-1-4">This document describes an architecture for a Network Function
        Interconnect (NFIX) that allows for interworking of physical and
        virtual network functions in a unified and scalable manner. It
        describes a mechanism for establishing connectivity across multiple
        discreet domains in both the wide-area network (WAN) and the data
        center (DC) while maintaining the ability to deliver against SLAs. To
        achieve this NFIX works with the underlying topology to build a
        unified over-the-top topology.<a href="#section-1-4" class="pilcrow">¶</a></p>
<p id="section-1-5">The NFIX architecture described in this document does not define
        any new protocols but rather outlines an architecture utilizing a
        collaboration of existing standards-based protocols.<a href="#section-1-5" class="pilcrow">¶</a></p>
</section>
<section id="section-2">
      <h2 id="name-terminology">
<a href="#section-2" class="section-number selfRef">2. </a><a href="#name-terminology" class="section-name selfRef">Terminology</a>
      </h2>
<ul class="normal">
<li class="normal" id="section-2-1.1">A physical network function (PNF) refers to a network device
          such as a Provider Edge (PE) router that connects physically to
          the wide-area network.<a href="#section-2-1.1" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-2-1.2">A virtualized network function (VNF) refers to a network device
          such as a provider edge (PE) router that is hosted on an application
          server. The VNF may be bare-metal in that it consumes the entire
          resources of the server, or it may be one of numerous virtual
          functions instantiated as a VM or number of containers on a given
          server that is controlled by a hypervisor or container management
          platform.<a href="#section-2-1.2" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-2-1.3">A Data Center Border (DCB) router refers to the network function that
          spans the border between the wide-area and the data center networks,
          typically interworking the different encapsulation techniques
          employed within each domain.<a href="#section-2-1.3" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-2-1.4">An Interconnect controller is the controller responsible for
          managing the NFIX fabric and services.<a href="#section-2-1.4" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-2-1.5">A DC controller is the term used for a controller that resides
          within an SDN-enabled data center and is responsible for the
          DC network(s)<a href="#section-2-1.5" class="pilcrow">¶</a>
</li>
      </ul>
</section>
<section id="section-3">
      <h2 id="name-motivation">
<a href="#section-3" class="section-number selfRef">3. </a><a href="#name-motivation" class="section-name selfRef">Motivation</a>
      </h2>
<p id="section-3-1">Industrial automation and business-critical environments use
          applications that are demanding on the network. These applications
          present different requirements from low-latency to high-throughput,
          to application-specific traffic conditioning, or a combination.
          The evolution to 5G equally presents challenges for mobile back-,
          front- and mid-haul networks. The requirement for ultra-reliable
          low-latency communication means that operators need to re-evaluate
          their network architecture to meet these requirements.<a href="#section-3-1" class="pilcrow">¶</a></p>
<p id="section-3-2">At the same time, the service edge is evolving. Where the service
          edge device was historically a PNF, the adoption of virtualization
          means VNFs are becoming more commonplace. Typically, these VNFs are
          hosted in some form of data center environment but require end-to-end
          connectivity to other VNFs and/or other PNFs. This represents a
          challenge because generally transport layer connectivity differs
          between the WAN and the data center environment. The WAN includes
          all levels of hierarchy (core, aggregation, access) that form the
          networks footprint, where transport layer connectivity using IP/MPLS
          is commonplace. In the data center native IP is commonplace,
          utilizing network virtualization overlay (NVO) technologies such
          as virtual extensible LAN (VXLAN) <span>[<a href="#RFC7348" class="xref">RFC7348</a>]</span>, network virtualization
          using generic routing encapsulation (NVGRE) <span>[<a href="#RFC7637" class="xref">RFC7637</a>]</span>, or generic
          network virtualization encapsulation (GENEVE)
          <span>[<a href="#I-D.ietf-nvo3-geneve" class="xref">I-D.ietf-nvo3-geneve</a>]</span>. There is a requirement to seamlessly
          integrate these islands and avoid heavy-lifting at interconnects as
          well as providing a means to provision end-to-end services with a
          single touch point at the edge.<a href="#section-3-2" class="pilcrow">¶</a></p>
<p id="section-3-3">The service edge boundary is also changing. Some functions that
          were previously reasonably centralized are now becoming more
          distributed. One reason for this is to attempt to deal with low
          latency requirements. Another reason is that operators seek to
          reduce costs by deploying low/medium-capacity VNFs closer to the
          edge. Equally, virtualization also sees some of the access network
          moving towards the core. Examples of this include cloud-RAN or
          Software-Defined Access Networks.<a href="#section-3-3" class="pilcrow">¶</a></p>
<p id="section-3-4">Historically service providers have architected data centers
          independently from the wide-area network, creating two independent
          domains or islands. As VNFs become part of the service landscape
          the service data-path must be extended across the WAN into the data
          center infrastructure, but in a manner that still allows operators
          to meet deterministic performance requirements. Methods for stitching
          WAN and DC infrastructures together with some form of
          service-interworking at the data center border have been
          implemented and deployed, but this service-interworking
          approach has several limitations:<a href="#section-3-4" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-3-5.1">The data center environment typically uses encapsulation
              techniques such as VXLAN or NVGRE while the WAN typically uses
              encapsulation techniques such as MPLS <span>[<a href="#RFC3031" class="xref">RFC3031</a>]</span>. Underlying
              optical infrastructure might also need to be programmed.
              These are incompatible and require interworking at the service
              layer.<a href="#section-3-5.1" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-3-5.2">It typically requires heavy-touch service provisioning on the
              data center border. In an end-to-end service, midpoint
              provisioning is undesirable and should be avoided.<a href="#section-3-5.2" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-3-5.3">Automation is difficult; largely due to the first two points
              but with additional contributing factors. In the virtualization
              world automation is a must-have capability.<a href="#section-3-5.3" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-3-5.4">When a service is operating at Layer 3 in a data center with
              redundant interconnects the risk of routing loops exists. There
              is no inherent loop avoidance mechanism when redistributing
              routes between address families so extreme care must be taken.
              Proposals such as the Domain Path (D-PATH) attribute
              <span>[<a href="#I-D.ietf-bess-evpn-ipvpn-interworking" class="xref">I-D.ietf-bess-evpn-ipvpn-interworking</a>]</span> attempt to address
              this issue but as yet are not widely implemented or deployed.<a href="#section-3-5.4" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-3-5.5">Some or all the above make the service-interworking gateway
              cumbersome with questionable scaling attributes.<a href="#section-3-5.5" class="pilcrow">¶</a>
</li>
      </ul>
<p id="section-3-6">Hence there is a requirement to create an open, scalable, and
            unified network architecture that brings together the wide-area
            network and data center domains. It is not an architecture e
            xclusively targeted at greenfield deployments, nor does it require
            a flag day upgrade to deploy in a brownfield network. It is an
            evolutionary step to a consolidated network that uses the
            constructs of seamless MPLS <span>[<a href="#I-D.ietf-mpls-seamless-mpls" class="xref">I-D.ietf-mpls-seamless-mpls</a>]</span> as
            a baseline and extends upon that to include topologies that may not
            be link-state based and to provide end-to-end path control. Overall
            the NFIX architecture aims to deliver the following:<a href="#section-3-6" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-3-7.1">Allows for an evolving service edge boundary without having to
              constantly restructure the architecture.<a href="#section-3-7.1" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-3-7.2">Provides a mechanism for providing seamless connectivity
              between VNF to VNF, VNF to PNF, and PNF to PNF, with
              deterministic SLAs, and with the ability to provide
              differentiated SLAs to suit different service requirements.<a href="#section-3-7.2" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-3-7.3">Delivers a unified transport fabric using Segment Routing (SR)
              <span>[<a href="#RFC8402" class="xref">RFC8402</a>]</span> where service delivery mandates touching only the
              service edge without imposing additional encapsulation
              requirements in the DC.<a href="#section-3-7.3" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-3-7.4">Embraces automation by providing an environment where any
              end-to-end connectivity can be instantiated in a single
              request manner while maintaining SLAs.<a href="#section-3-7.4" class="pilcrow">¶</a>
</li>
      </ul>
</section>
<section id="section-4">
      <h2 id="name-requirements">
<a href="#section-4" class="section-number selfRef">4. </a><a href="#name-requirements" class="section-name selfRef">Requirements</a>
      </h2>
<p id="section-4-1">The following section outlines the requirements that the proposed
           solution must meet. From an overall perspective, the proposed
           generic architecture must:<a href="#section-4-1" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-4-2.1">Deliver end-to-end transport LSPs using traffic-engineering (TE)
              as required to meet appropriate SLAs for the service using(s)
              using those LSPs. End-to-end refers to VNF and/or PNF
              connectivity or a combination of both.<a href="#section-4-2.1" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-4-2.2">Provide a solution that allows for optimal end-to-end path
              placement; where optimal not only meets the requirements of the
              path in question but also meets the global network objectives.<a href="#section-4-2.2" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-4-2.3">Support varying types of VNF physical network attachment and
              logical (underlay/overlay) connectivity.<a href="#section-4-2.3" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-4-2.4">Facilitate automation of service provision. As such the
              solution should avoid heavy-touch service provisioning and
              decapsulation/encapsulation at data center border routers.<a href="#section-4-2.4" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-4-2.5">Provide a framework for delivering logical end-to-end networks
              using differentiated logical topologies and/or constraints.<a href="#section-4-2.5" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-4-2.6">Provide a high level of stability; faults in one domain should
              not propagate to another domain.<a href="#section-4-2.6" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-4-2.7">Provide a mechanism for homogeneous end-to-end OAM.<a href="#section-4-2.7" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-4-2.8">Hide/localize instabilities in the different domains that
              participate in the end-to-end service.<a href="#section-4-2.8" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-4-2.9">Provide a mechanism to minimize the label-stack depth
              required at path head-ends for SR-TE LSPs.<a href="#section-4-2.9" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-4-2.10">Offer a high level of scalability.<a href="#section-4-2.10" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-4-2.11">Although not considered in-scope of the current version of this
              document, the solution should not preclude the deployment of
              multicast. This subject may be covered in later versions of
              this document.<a href="#section-4-2.11" class="pilcrow">¶</a>
</li>
      </ul>
</section>
<section id="section-5">
      <h2 id="name-theory-of-operation">
<a href="#section-5" class="section-number selfRef">5. </a><a href="#name-theory-of-operation" class="section-name selfRef">Theory of Operation</a>
      </h2>
<p id="section-5-1">This section describes the NFIX architecture including the
          building blocks and protocol machinery that is used to form the
          fabric. Where considered appropriate rationale is given for
          selection of an architectural component where other seemingly
          applicable choices could have been made.<a href="#section-5-1" class="pilcrow">¶</a></p>
<section id="section-5.1">
        <h3 id="name-vnf-assumptions">
<a href="#section-5.1" class="section-number selfRef">5.1. </a><a href="#name-vnf-assumptions" class="section-name selfRef">VNF Assumptions</a>
        </h3>
<p id="section-5.1-1">For the sake of simplicity, references to VNF are made in a
         broad sense. Equally, the differences between VNF and Container Network
  Function (CNF) are largely immaterial for the purposes of this document,
  therefore VNF is used to represent both. The way in which a VNF is instantiated
  and provided network connectivity will differ based on environment and
  VNF capability, but for conciseness this is not explicitly detailed
         with every reference to a VNF. Common examples of VNF variants
         include but are not limited to:<a href="#section-5.1-1" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-5.1-2.1">A VNF that functions as a routing device and has full IP routing
          and MPLS capabilities. It can be connected simultaneously to the data
          center fabric underlay and overlay and serves as the NVO tunnel
          endpoint <span>[<a href="#RFC8014" class="xref">RFC8014</a>]</span>. Examples of this might be a virtualized PE
          router, or a virtualized Broadband Network Gateway (BNG).<a href="#section-5.1-2.1" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.1-2.2">A VNF that functions as a device (host or router) with limited IP
          routing capability. It does not connect directly to the data center
          fabric underlay but rather connects to one or more external physical
          or virtual devices that serve as the NVO tunnel endpoint(s). It may
          however have single or multiple connections to the overlay. Examples
          of this might be a mobile network control or management plane
          function.<a href="#section-5.1-2.2" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.1-2.3">A VNF that has no routing capability. It is a virtualized function
          hosted within an application server and is managed by a hypervisor
          or container host. The hypervisor/container host acts as the NVO
          endpoint and interfaces to some form of SDN controller responsible
          for programming the forwarding plane of the virtualization host
          using, for example, OpenFlow. Examples of this might be an
          Enterprise application server or a web server running as a virtual
   machine and front-ended by a virtual routing function such as
   OVS/xVRS/VTF.<a href="#section-5.1-2.3" class="pilcrow">¶</a>
</li>
        </ul>
<p id="section-5.1-3">Where considered necessary exceptions to the examples provided
        above or focus on a particular scenario will be highlighted.<a href="#section-5.1-3" class="pilcrow">¶</a></p>
</section>
<section id="section-5.2">
        <h3 id="name-overview">
<a href="#section-5.2" class="section-number selfRef">5.2. </a><a href="#name-overview" class="section-name selfRef">Overview</a>
        </h3>
<p id="section-5.2-1">The NFIX architecture makes no assumptions about how the network is
         physically composed, nor does it impose any dependencies upon it.
         It also makes no assumptions about IGP hierarchies and the use of
         areas/levels or discrete IGP instances within the WAN is fully
         endorsed to enhance scalability and constrain fault propagation.
  This could apply for instance to a hierarchical WAN from core to
  edge or from WAN to LAN connections. The overall architecture uses
  the constructs of seamless MPLS as a baseline and extends upon that.
  The concept of decomposing the network into multiple domains is one
  that has been widely deployed and has been proven to scale in
  networks with large numbers of nodes.<a href="#section-5.2-1" class="pilcrow">¶</a></p>
<p id="section-5.2-2">The proposed architecture uses segment routing (SR) as its
         preferred choice of transport. Segment routing is chosen for
         construction of end-to-end LSPs given its ability to traffic-engineer
         through source-routing while concurrently scaling exceptionally well
         due to its lack of network state other than the ingress node. This
         document uses SR instantiated on an MPLS forwarding plane(SR-MPLS),
         although it does not preclude the use of SRv6 either now or at some
         point in the future. The rationale for selecting SR-MPLS is simply
         maturity and more widespread applicability across a potentially broad
         range of network devices. This document may be updated in future
         versions to include more description of SRv6 applicability.<a href="#section-5.2-2" class="pilcrow">¶</a></p>
</section>
<section id="section-5.3">
        <h3 id="name-use-of-a-centralized-contro">
<a href="#section-5.3" class="section-number selfRef">5.3. </a><a href="#name-use-of-a-centralized-contro" class="section-name selfRef">Use of a Centralized Controller</a>
        </h3>
<p id="section-5.3-1">It is recognized that for most operators the move towards the use
         of a controller within the wide-area network is a significant change
         in operating model. In the NFIX architecture it is a necessary
         component. Its use is not simply to offload inter-domain path
         calculation from network elements; it provides many more benefits:<a href="#section-5.3-1" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-5.3-2.1">It offers the ability to enforce constraints on paths that
           originate/terminate on different network elements, thereby providing
           path diversity, and/or bidirectionality/co-routing, and/or
           disjointness.<a href="#section-5.3-2.1" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.3-2.2">It avoids collisions, re-tries, and packing problems that has been
          observed in networks using distributed TE path calculation, where
          head-ends make autonomous decisions.<a href="#section-5.3-2.2" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.3-2.3">A controller can take a global view of path placement strategies,
          including the ability to make path placement decisions over a high
          number of LSPs concurrently as opposed to considering each LSP
          independently. In turn, this allows for 'global' optimization of
          network resources such as available capacity.<a href="#section-5.3-2.3" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.3-2.4">A controller can make decisions based on near-real-time network
          state and optimize paths accordingly. For example, if a network link
          becomes congested it may recompute some of the paths transiting that
          link to other links that may not be quite as optimal but do have
          available capacity. Or if a link latency crosses a certain threshold,
          it may select to reoptimize some latency-sensitive paths away from
          that link.<a href="#section-5.3-2.4" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.3-2.5">The logic of a controller can be extended beyond pure path
          computation and placement. If the controller is aware of services,
          service requirements, and available paths within the network it can
          cross-correlate between them and ensure that the appropriate paths
          are used for the appropriate services.<a href="#section-5.3-2.5" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.3-2.6">The controller can provide assurance and verification of the
          underlying SLA provided to a given service.<a href="#section-5.3-2.6" class="pilcrow">¶</a>
</li>
        </ul>
<p id="section-5.3-3">As the main objective of the NFIX architecture is to unify the data
         center and wide-area network domains, using the term controller is not
         sufficiently succinct. The centralized controller may need to
         interface to other controllers that potentially reside within an
         SDN-enabled data center. Therefore, to avoid interchangeably using
         the term controller for both functions, we distinguish between them
         simply by using the terms 'DC controller' which as the name suggests
         is responsible for the DC, and 'Interconnect controller' responsible
         for managing the extended SR fabric and services.<a href="#section-5.3-3" class="pilcrow">¶</a></p>
<p id="section-5.3-4">The Interconnect controller learns wide-area network topology
           information and allocation of segment routing SIDs within that
           domain using BGP link-state <span>[<a href="#RFC7752" class="xref">RFC7752</a>]</span> with appropriate SR extensions.
           Equally it learns data center topology information and Prefix-SID
           allocation using BGP labeled unicast <span>[<a href="#RFC8277" class="xref">RFC8277</a>]</span> with appropriate SR
           extensions, or BGP link-state if a link-state IGP is used within the
           data center. If Route-Reflection is used for exchange of BGP
           link-state or labeled unicast NLRI within one or more domains, then
           the Interconnect controller need only peer as a client with those
           Route-Reflectors in order to learn topology information.<a href="#section-5.3-4" class="pilcrow">¶</a></p>
<p id="section-5.3-5">Where BGP link-state is used to learn the topology of a data center
          (or any IGP routing domain) the BGP-LS Instance Identifier
          (Instance-ID) is carried within Node/Link/Prefix NLRI and is used
          to identify a given IGP routing domain. Where labeled unicast BGP
          is used to discover the topology of one or more data center domains
          there is no equivalent way for the Interconnect controller to achieve
          a level of routing domain correlation. The controller may learn some
          splintered connectivity map consisting of 10 leaf switches, four
          spine switches, and four DCB's, but it needs some form of key to
          inform it that leaf switches 1-5, spine switches 1 and 2, and DCB's 1
          and 2 belong to data center 1, while leaf switches 6-10, spine
          switches 3 and 4, and DCB's 3 and 4 belong to data center 2. What is
          needed is a form of 'data center membership identification' to
          provide this correlation. Optionally this could be achieved at BGP
          level using a standard community to represent each data center, or
          it could be done at a more abstract level where for example the DC
          controller provides the membership identification to the Interconnect
          controller through an application programming interface (API).<a href="#section-5.3-5" class="pilcrow">¶</a></p>
<p id="section-5.3-6">Understanding real-time network state is an important part of the
          Interconnect controllers role, and only with this information is the
          controller able to make informed decisions and take preventive or
          corrective actions as necessary. There are numerous methods
          implemented and deployed that allow for harvesting of network
          state, including (but not limited to) IPFIX <span>[<a href="#RFC7011" class="xref">RFC7011</a>]</span>,
   Netconf/YANG <span>[<a href="#RFC6241" class="xref">RFC6241</a>]</span><span>[<a href="#RFC6020" class="xref">RFC6020</a>]</span>,
   streaming telemetry, BGP link-state <span>[<a href="#RFC7752" class="xref">RFC7752</a>]</span>
          <span>[<a href="#I-D.ietf-idr-te-lsp-distribution" class="xref">I-D.ietf-idr-te-lsp-distribution</a>]</span>, and the
   BGP Monitoring Protocol (BMP) <span>[<a href="#RFC7854" class="xref">RFC7854</a>]</span>.<a href="#section-5.3-6" class="pilcrow">¶</a></p>
</section>
<section id="section-5.4">
        <h3 id="name-routing-and-lsp-underlay">
<a href="#section-5.4" class="section-number selfRef">5.4. </a><a href="#name-routing-and-lsp-underlay" class="section-name selfRef">Routing and LSP Underlay</a>
        </h3>
<p id="section-5.4-1">This section describes the mechanisms and protocols that are used to
         establish end-to-end LSPs; where end-to-end refers to VNF-to-VNF,
  PNF-to-PNF, or VNF-to-PNF.<a href="#section-5.4-1" class="pilcrow">¶</a></p>
<section id="section-5.4.1">
          <h4 id="name-intra-domain-routing">
<a href="#section-5.4.1" class="section-number selfRef">5.4.1. </a><a href="#name-intra-domain-routing" class="section-name selfRef">Intra-Domain Routing</a>
          </h4>
<p id="section-5.4.1-1">In a seamless MPLS architecture domains are based on geographic
             dispersion (core, aggregation, access).  Within this document a
             domain is considered as any entity with a captive topology; be it
             a link-state topology or otherwise. Where reference is made to the
             wide-area network domain, it refers to one or more domains that
             constitute the wide-area network domain.<a href="#section-5.4.1-1" class="pilcrow">¶</a></p>
<p id="section-5.4.1-2">This section discusses the basic building blocks required within
            the wide-area network and the data center, noting from above that
            the wide-area network may itself consist of multiple domains.<a href="#section-5.4.1-2" class="pilcrow">¶</a></p>
<section id="section-5.4.1.1">
            <h5 id="name-wide-area-network-domains">
<a href="#section-5.4.1.1" class="section-number selfRef">5.4.1.1. </a><a href="#name-wide-area-network-domains" class="section-name selfRef">Wide-Area Network Domains</a>
            </h5>
<p id="section-5.4.1.1-1">The wide-area network includes all levels of hierarchy
                (core, aggregation, access) that constitute the networks MPLS
                footprint as well as the data Center border routers.
                Each domain that constitutes part of the wide-area network
                runs a link-state interior gateway protocol (IGP) such as
                ISIS or OSPF, and each domain may use IGP-inherent hierarchy
                (OSPF areas, ISIS levels) with an assumption that visibility
                is domain-wide using, for example, L2 to L1 redistribution.
                Alternatively, or additionally, there may be multiple domains
                that are split by using separate and distinct instances of IGP.
                There is no requirement for IGP redistribution of any link or
                loopback addresses between domains.<a href="#section-5.4.1.1-1" class="pilcrow">¶</a></p>
<p id="section-5.4.1.1-2">Each IGP should be enabled with the relevant extensions for
                segment routing <span>[<a href="#RFC8667" class="xref">RFC8667</a>]</span><span>[<a href="#RFC8665" class="xref">RFC8665</a>]</span>, and each SR-capable router
                should advertise a Node-SID for its loopback address, and an
                Adjacency-SID (Adj-SID) for every connected interface
                (unidirectional adjacency) belonging to the SR domain. SR
                Global Blocks (SRGB) can be allocated to each domain as deemed
                appropriate to specific network requirements. Border routers
                belonging to multiple domains have an SRGB for each domain.<a href="#section-5.4.1.1-2" class="pilcrow">¶</a></p>
<p id="section-5.4.1.1-3">The default forwarding path for intra-domain LSPs
                that do not require TE is simply an SR LSP containing a single
                label advertised by the destination as a Node-SID and
                representing the ECMP-aware shortest path to that destination.
                Intra-domain TE LSPs are constructed as required by
                the Interconnect controller. Once a path is calculated it is
                advertised as an explicit SR Policy
                <span>[<a href="#I-D.ietf-spring-segment-routing-policy" class="xref">I-D.ietf-spring-segment-routing-policy</a>]</span> containing one
                or more paths expressed as one or more segment-lists, which may
 optionally contain binding SIDs if requirements dictate. An
                SR Policy is identified through the tuple
                [headend, color, endpoint] and this tuple is used extensively
                by the Interconnect controller to associate services with an
                underlying SR Policy that meets its objectives.<a href="#section-5.4.1.1-3" class="pilcrow">¶</a></p>
<p id="section-5.4.1.1-4">To provide support for ECMP the Entropy Label <span>[<a href="#RFC6790" class="xref">RFC6790</a>]</span><span>[<a href="#RFC8662" class="xref">RFC8662</a>]</span>
                should be utilized. Entropy Label Capability (ELC) should be 
                advertised into the IGP using the IS-IS Prefix Attributes TLV
                <span>[<a href="#I-D.ietf-isis-mpls-elc" class="xref">I-D.ietf-isis-mpls-elc</a>]</span> or the OSPF Extended
 Prefix TLV <span>[<a href="#I-D.ietf-ospf-mpls-elc" class="xref">I-D.ietf-ospf-mpls-elc</a>]</span> coupled
 with the Node MSD Capability sub-TLV to advertise Entropy Readable
 Label Depth (ERLD) <span>[<a href="#RFC8491" class="xref">RFC8491</a>]</span><span>[<a href="#RFC8476" class="xref">RFC8476</a>]</span>
 and the base MPLS Imposition (BMI). Equally, support for ELC together
 with the supported ERLD should be signaled in BGP using the BGP
 Next-Hop Capability <span>[<a href="#I-D.ietf-idr-next-hop-capability" class="xref">I-D.ietf-idr-next-hop-capability</a>]</span>.
 Ingress nodes and or DCBs should ensure sufficient entropy is applied
 to packets to exercise available ECMP links.<a href="#section-5.4.1.1-4" class="pilcrow">¶</a></p>
</section>
<section id="section-5.4.1.2">
            <h5 id="name-data-center-domain">
<a href="#section-5.4.1.2" class="section-number selfRef">5.4.1.2. </a><a href="#name-data-center-domain" class="section-name selfRef">Data Center Domain</a>
            </h5>
<p id="section-5.4.1.2-1">The data center domain includes all fabric switches, network
                virtualization edge (NVE), and the data center border routers.
                The data center routing design may align with the framework of
                <span>[<a href="#RFC7938" class="xref">RFC7938</a>]</span> running eBGP single-hop sessions established over
                direct point-to-point links, or it may use an IGP for
                dissemination of topology information. This document focuses on the
 former, simply because the ue of an IGP largely makes the data centers
 behaviour analogous to that of a wide-area network domain.<a href="#section-5.4.1.2-1" class="pilcrow">¶</a></p>
<p id="section-5.4.1.2-2">The chosen method of transport or encapsulation within the
                data center for NFIX is SR-MPLS over IP/UDP <span>[<a href="#RFC8663" class="xref">RFC8663</a>]</span> or,
                where possible, native SR-MPLS. The choice of SR-MPLS over
                IP/UDP or native SR-MPLS allows for good entropy to maximize
                the use of equal-cost Clos fabric links. Native SR-MPLS
 encapsulation provides entropy through use of the Entropy Label,
 and, like the wide-area network, support for ELC together with the
 support ERLD should be signaled using the BGP Next-Hop Capability
 attribute. As described in <span>[<a href="#RFC6790" class="xref">RFC6790</a>]</span> the ELC
 is an indication from the egress node of an MPLS tunnel to the
 ingress node of the MPLS tunnel that is is capable of processing
 an Entropy Label. The BGP Next-Hop Capability is a non-transitive
 attribute which is modified or deleted when the next-hop is 
 changed to reflect the capabilities of the new next-hop. If we
 assume that the path of a BGP-signaled LSP transits through 
 multiple ASNs, and/or a single ASN with multiple next-hops, then
 it is not possible for the ingress node to determine the ELC
 of the egress node. Without this end-to-end signaling capability
 the entropy label must only be used when it is explicitly known,
 through configuration or other means, that the egress node has
 support for it. Entropy for SR-MPLS over IP/UDP encapsulation
 uses the source UDP port for IPv4 and the Flow Label for IPv6.
 Again, the ingress network function should ensure sufficient
 entropy is applied to exercise available ECMP links.<a href="#section-5.4.1.2-2" class="pilcrow">¶</a></p>
<p id="section-5.4.1.2-3">Another significant advantage of the use of native SR-MPLS or
     SR-MPLS over IP/UDP is that it allows for a lightweight interworking
     function at the DCB without the requirement for midpoint provisioning;
     interworking between the data center and the wide-area network
     domains becomes an MPLS label swap/continue action.<a href="#section-5.4.1.2-3" class="pilcrow">¶</a></p>
<p id="section-5.4.1.2-4">Loopback addresses of network elements within the data center are
                advertised using labeled unicast BGP with the addition of SR
                Prefix SID extensions <span>[<a href="#RFC8669" class="xref">RFC8669</a>]</span> containing a
 globally unique and persistent Prefix-SID. The data-plane encapsulation
 of SR-MPLS over IP/UDP or native SR-MPLS allows network elements
                within the data center to consume BGP Prefix-SIDs and
                legitimately use those in the encapsulation.<a href="#section-5.4.1.2-4" class="pilcrow">¶</a></p>
</section>
</section>
<section id="section-5.4.2">
          <h4 id="name-inter-domain-routing">
<a href="#section-5.4.2" class="section-number selfRef">5.4.2. </a><a href="#name-inter-domain-routing" class="section-name selfRef">Inter-Domain Routing</a>
          </h4>
<p id="section-5.4.2-1">Inter-domain routing is responsible for establishing connectivity
             between any domains that form the wide-area network, and between
             the wide-area network and data center domains. It is considered
             unlikely that every end-to-end LSP will require a TE path, hence
             there is a requirement for a default end-to-end forwarding path.
             This default forwarding path may also become the path of last
             resort in the event of a non-recoverable failure of a TE path.
             Similar to the seamless MPLS architecture this inter-domain MPLS
             connectivity is realized using labeled unicast BGP [RFC8277] with
             the addition of SR Prefix SID extensions.<a href="#section-5.4.2-1" class="pilcrow">¶</a></p>
<p id="section-5.4.2-2">Within each wide-area network domain all service edge routers,
            DCBs, and ABRs/ASBRs form part of the labeled BGP mesh, which can
            be either full-mesh, or more likely based on the use of
            route-reflection. Each of these routers advertises its respective
            loopback addresses into labeled BGP together with an MPLS label
            and a globally unique Prefix-SID. Routes are advertised between
            wide-area network domains by ABRs/ASBRs that impose next-hop-self
            on advertised routes. The function of imposing next-hop-self for
            labeled routes means that the ABR/ASBR allocates a new label for
            advertised routes and programs a label-swap entry in the
            forwarding plane for received and advertised routes. In short it
            becomes part of the forwarding path.<a href="#section-5.4.2-2" class="pilcrow">¶</a></p>
<p id="section-5.4.2-3">DCB routers have labeled BGP sessions towards the wide-area
            network and labeled BGP sessions towards the data center.
            Routes are bidirectionally advertised between the domains
            subject to policy, with the DCB imposing itself as next-hop
            on advertised routes. As above, the function of imposing
            next-hop-self for labeled routes implies allocation of a new
            label for advertised routes and a label-swap entry being programmed
            in the forwarding plane for received and advertised labels.
            The DCB thereafter becomes the anchor point between the wide-area
            network domain and the data center domain.<a href="#section-5.4.2-3" class="pilcrow">¶</a></p>
<p id="section-5.4.2-4">Within the wide-area network next-hops for labeled unicast
            routes containing Prefix-SIDs are resolved to SR LSPs, and within
            the data center domain next-hops for labeled unicast routes
            containing Prefix-SIDs are resolved to SR LSPs or IP/UDP tunnels.
            This provides end-to-end connectivity without a traffic-engineering
            capability.<a href="#section-5.4.2-4" class="pilcrow">¶</a></p>
<div id="figure1">
<figure id="figure-1">
            <div class="alignLeft art-text artwork" id="section-5.4.2-5.1">
<pre>


      +---------------+   +----------------+   +---------------+
      |  Data Center  |   |   Wide-Area    |   |   Wide-Area   |
      |              +-----+   Domain 1   +-----+  Domain ‘n’  |
      |              | DCB |              | ABR |              |
      |              +-----+              +-----+              |
      |               |   |                |   |               |
      +---------------+   +----------------+   +---------------+
      &lt;-- SR/SRoUDP --&gt;   &lt;---- IGP/SR ----&gt;   &lt;--- IGP/SR ----&gt;
      &lt;--- BGP-LU ---&gt; NHS &lt;--- BGP-LU ---&gt; NHS &lt;--- BGP-LU ---&gt;

</pre>
</div>
<figcaption><a href="#figure-1" class="selfRef">Figure 1</a></figcaption></figure>
</div>
<p id="section-5.4.2-6" class="keepWithPrevious">Default Inter-Domain Forwarding Path<a href="#section-5.4.2-6" class="pilcrow">¶</a></p>
</section>
<section id="section-5.4.3">
          <h4 id="name-intra-domain-and-inter-doma">
<a href="#section-5.4.3" class="section-number selfRef">5.4.3. </a><a href="#name-intra-domain-and-inter-doma" class="section-name selfRef">Intra-Domain and Inter-Domain Traffic-Engineering</a>
          </h4>
<p id="section-5.4.3-1">The capability to traffic-engineer intra- and inter-domain
             end-to-end paths is considered a key requirement in order to meet
             the service objectives previously outlined. To achieve optimal
             end-to-end path placement the key components to be considered are
             path calculation, path activation, and FEC-to-path binding
             procedures.<a href="#section-5.4.3-1" class="pilcrow">¶</a></p>
<p id="section-5.4.3-2">In the NFIX architecture end-to-end path calculation is performed
            by the Interconnect controller. The mechanics of how the objectives
            of each path is calculated is beyond the scope of this document.
            Once a path is calculated based upon its objectives and
            constraints, the path is advertised from the controller to the
            LSP headend as an explicit SR Policy containing one or more paths
            expressed as one or more segment-lists. An SR Policy is identified
            through the tuple [headend, color, endpoint] and this tuple is used
            extensively by the Interconnect controller to associate services
            with an underlying SR Policy that meets its objectives.<a href="#section-5.4.3-2" class="pilcrow">¶</a></p>
<p id="section-5.4.3-3">The segment-list of an SR Policy encodes a source-routed path
            towards the endpoint. When calculating the segment-list the
            Interconnect controller makes comprehensive use of the
            Binding-SID (BSID), instantiating BSID anchors as necessary at path
            midpoints when calculating and activating a path. The use of BSID
            is considered fundamental to segment routing as described in 
 <span>[<a href="#I-D.filsfils-spring-sr-policy-considerations" class="xref">I-D.filsfils-spring-sr-policy-considerations</a>]</span>.
 It provides opacity between domains, ensuring that any segment
 churn is constrained to a single domain. It also reduces the number
 of segments/labels that the headend needs to impose, which is
            particularly important given that network elements within a data
            center generally have limited label imposition capabilities.
            In the context of the NFIX architecture it is also the vehicle
            that allows for removal of heavy midpoint provisioning at the DCB.<a href="#section-5.4.3-3" class="pilcrow">¶</a></p>
<p id="section-5.4.3-4">For example, assume that VNF1 is situated in data center 1, which
            is interconnected to the wide-area network via DCB1. VNF1 requires
            connectivity to VNF2, situated in data center 2, which is
            interconnected to the wide-area network via DCB2. Assuming there
            is no existing TE path that meet VNF1's requirements, the
            Interconnect controller will:<a href="#section-5.4.3-4" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-5.4.3-5.1">Instantiate an SR Policy on DCB1 with BSID n and a
              segment-list containing the relevant segments of a TE path
              to DCB2. DCB1 therefore becomes a BSID anchor.<a href="#section-5.4.3-5.1" class="pilcrow">¶</a>
</li>
            <li class="normal" id="section-5.4.3-5.2">Instantiate an SR Policy on VNF1 with BSID m and a segment-list
              containing segments {DCB1, n, VNF2}.<a href="#section-5.4.3-5.2" class="pilcrow">¶</a>
</li>
          </ul>
<div id="figure2">
<figure id="figure-2">
            <div class="alignLeft art-text artwork" id="section-5.4.3-6.1">
<pre>


       +---------------+  +----------------+  +---------------+
       | Data Center 1 |  |   Wide-Area    |  | Data Center 2 |
       | +----+       +----+      3       +----+       +----+ |
       | |VNF1|       |DCB1|-1   / \   5--|DCB2|       |VNF2| |
       | +----+       +----+  \ /   \ /   +----+       +----+ |
       |               |  |    2     4     |  |               |
       +---------------+  +----------------+  +---------------+
       SR Policy      SR Policy
       BSID m         BSID n
      {DCB1,n,VNF2} {1,2,3,4,5,DCB2}

</pre>
</div>
<figcaption><a href="#figure-2" class="selfRef">Figure 2</a></figcaption></figure>
</div>
<p id="section-5.4.3-7" class="keepWithPrevious">Traffic-Engineered Path using BSID<a href="#section-5.4.3-7" class="pilcrow">¶</a></p>
<p id="section-5.4.3-8">In the above figure a single DCB is used to interconnect two
      domains. Similarly, in the case of two wide-area domains the DCB
  would be represented as an ABR or ASBR. In some single operator
  environments domains may be interconnected using adjacent ASBRs
  connected via a distinct physical link. In this scenario the
  procedures outlined above may be extended to incorporate the 
  mechanisms used in Egress Peer Engineering (EPE) <span>[<a href="#I-D.ietf-spring-segment-routing-central-epe" class="xref">I-D.ietf-spring-segment-routing-central-epe</a>]</span>
             to form a traffic-engineered path spanning distinct domains.<a href="#section-5.4.3-8" class="pilcrow">¶</a></p>
<section id="section-5.4.3.1">
            <h5 id="name-traffic-engineering-and-ecm">
<a href="#section-5.4.3.1" class="section-number selfRef">5.4.3.1. </a><a href="#name-traffic-engineering-and-ecm" class="section-name selfRef">Traffic-Engineering and ECMP</a>
            </h5>
<p id="section-5.4.3.1-1">Where the Interconnect controller is used to place SR policies,
      providing support for ECMP requires some consideration. An SR
  Policy is described with one or more segment-lists, end each of
  those segment-lists may or may not provide ECMP as a sum instruction
  and each SID itself may or may not support ECMP forwarding. When
  an individual SID is a BSID, an ECMP path may or may not also be
  nested within. The Interconnect controller may choose to place a
  path consisting entirely of non-ECMP-aware Adj-SIDs (each SID
  representing a single adjacency) such that the controller has explicit
  hop-by-hop knowledge of where that SR-TE LSP is routed. This is
  beneficial to allow the controller to take corrective action if the
  criteria that was used to initially select a particular link in a
  particular path subsequently changes. For example, if the latency
  of a link increases or a link becomes congested and a path should
  be rerouted. If ECMP-aware SIDs are used in the SR policy segment-list
  (including Node-SIDs, Adj-SIDs representing parallel links, and Anycast
  SIDs) SR routers are able to make autonomous decisions about where
  traffic is forwarded. As a result, it is not possible for the controller
  to fully understand the impact of a change in network state and react
  to it. With this in mind there are a number of approaches that could
  be adopted:<a href="#section-5.4.3.1-1" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-5.4.3.1-2.1">If there is no requirement for the Interconnect controller to
 explicitly track path on a hop-by-hop basis, ECMP-aware SIDs may be
 used in the SR policy segment-list. This approach may require multiple
 [ELI, EL] pairs to be inserted at the ingress node; for example,
 above and below a BSID to provide entropy in multiple domains.<a href="#section-5.4.3.1-2.1" class="pilcrow">¶</a>
</li>
              <li class="normal" id="section-5.4.3.1-2.2">If there is a requirement for the Interconnect controller to
 explicitly track paths on a hop-by-hop to provide the capability
 to reroute them based on changes in network state, SR policy
 segment-lists should be constructed of non-ECMP-aware Adj-SIDs.<a href="#section-5.4.3.1-2.2" class="pilcrow">¶</a>
</li>
              <li class="normal" id="section-5.4.3.1-2.3">A hybrid approach that allows for a level of ECMP (at the
 headend) together with the ability for the Interconnect controller
 to  explicitly track paths is to instantiate an SR policy consisting
 of a set of segment-lists, each containing non-ECMP-aware Adj-SIDs.
 Each segment-list will be assigned a weight to allow for ECMP or
 UCMP. This approach does however imply computation and programing
 of two paths instead of one.<a href="#section-5.4.3.1-2.3" class="pilcrow">¶</a>
</li>
              <li class="normal" id="section-5.4.3.1-2.4">Another hybrid approach might work as follows. Redundant DCBs
 advertise an Anycast-SID 'A' into the data center, and also
 instantiate an SR policy with a segment-list consisting of
 non-ECMP-aware Adj-SIDs meeting the required connectivity and
 SLA. The BSID value of this SR policy 'B' must be common to both
 redundant DCBs, but the calculated paths are diverse. Indeed,
 multiple segment-lists could be used in this SR policy. A VNF
 could then instantiate an SR policy with a segment-list of
 {A, B} to achieve ECMP in the data center and TE in the wide-area
 network with the option of ECMP at the BSID anchor<a href="#section-5.4.3.1-2.4" class="pilcrow">¶</a>
</li>
            </ul>
</section>
</section>
</section>
<section id="section-5.5">
        <h3 id="name-service-layer">
<a href="#section-5.5" class="section-number selfRef">5.5. </a><a href="#name-service-layer" class="section-name selfRef">Service Layer</a>
        </h3>
<p id="section-5.5-1">The service layer is intended to deliver Layer 2 and/or Layer 3 VPN
         connectivity between network functions to create an overlay utilizing
         the routing and LSP underlay described in section 5.4. To do this the
  solution employs the EVPN and/or VPN-IPv4/IPv6 address families to
  exchange Layer 2 and Layer 3 Network Layer Reachability Information
  (NLRI). When these NLRI are exchanged between domains it is typical
  for the border router to set next-hop-self on advertised routes. With
  the proposed routing and LSP underlay however, this is not required
  and EVPN/VPN-IPv4/IPv6 routes should be passed end-to-end without
         transit routers modifying the next-hop attribute.<a href="#section-5.5-1" class="pilcrow">¶</a></p>
<p id="section-5.5-2">Section 5.4.2 describes the use of labeled unicast BGP to exchange
        inter-domain routes to establish a default forwarding path.
        Labeled-unicast BGP is used to exchange prefix reachability between
        service edge routers, with domain border routes imposing next-hop-self
        on routes advertised between domains. This provides a default
        inter-domain forwarding path and provides the required connectivity
        to establish inter-domain BGP sessions between service edges for the
        exchange of EVPN and/or VPN-IPv4/IPv6 NLRI. If route-reflection is
        used for the EVPN and/or VPN-IPv4/IPv6 address families within one
        or more domains, it may be desirable to create inter-domain BGP
        sessions between route-reflectors. In this case the peering addresses
        of the route-reflectors should also be exchanged between domains using
        labeled unicast BGP. This creates a connectivity model analogous to
        BGP/MPLS IP-VPN Inter-AS option C <span>[<a href="#RFC4364" class="xref">RFC4364</a>]</span>.<a href="#section-5.5-2" class="pilcrow">¶</a></p>
<div id="figure3">
<figure id="figure-3">
          <div class="alignLeft art-text artwork" id="section-5.5-3.1">
<pre>



         +----------------+  +----------------+  +----------------+
         |     +----+     |  |     +----+     |  |     +----+     |
       +----+  | RR |    +----+    | RR |    +----+    | RR |   +----+
       | NF |  +----+    | DCI|    +----+    | DCI|    +----+   | NF |
       +----+            +----+              +----+             +----+
         |     Domain     |  |     Domain     |  |     Domain     |
         +----------------+  +----------------+  +----------------+
         &lt;-------&gt; &lt;-----&gt; NHS &lt;-- BGP-LU ---&gt; NHS &lt;-----&gt; &lt;------&gt;
         &lt;-------&gt; &lt;--------- EVPN/VPN-IPv4/v6 ----------&gt; &lt;------&gt;

</pre>
</div>
<figcaption><a href="#figure-3" class="selfRef">Figure 3</a></figcaption></figure>
</div>
<p id="section-5.5-4" class="keepWithPrevious">Inter-Domain Service Layer<a href="#section-5.5-4" class="pilcrow">¶</a></p>
<p id="section-5.5-5">EVPN and/or VPN-IPv4/v6 routes received from a peer in a
             different domain will contain a next-hop equivalent to the router
             that sourced the route. The next-hop of these routes can be
             resolved to labeled-unicast route (default forwarding path) or
             to an SR policy (traffic-engineered forwarding path) as appropriate
             to the service requirements. The exchange of EVPN and/or
             VPN-IPv4/IPv6 routes in this manner implies that
             Route-Distinguisher and Route-Target values remain intact
             end-to-end.<a href="#section-5.5-5" class="pilcrow">¶</a></p>
<p id="section-5.5-6">The use of end-to-end EVPN and/or VPN-IPv4/IPv6 address families
            without the imposition of next-hop-self at border routers
            complements the gateway-less transport layer architecture.
            It negates the requirement for midpoint service provisioning
            and as such provides the following benefits:<a href="#section-5.5-6" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-5.5-7.1">Avoids the translation of MAC/IP EVPN routes to IP-VPN
              routes (and vice versa) that is typically associated with
              service interworking.<a href="#section-5.5-7.1" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.5-7.2">Avoids instantiation of MAC-VRFs and IP-VPNs for each tenant
              resident in the DCB.<a href="#section-5.5-7.2" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.5-7.3">Avoids provisioning of demarcation functions between the data
              center and wide-area network such as QoS, access-control,
              aggregation and isolation.<a href="#section-5.5-7.3" class="pilcrow">¶</a>
</li>
        </ul>
</section>
<section id="section-5.6">
        <h3 id="name-service-differentiation">
<a href="#section-5.6" class="section-number selfRef">5.6. </a><a href="#name-service-differentiation" class="section-name selfRef">Service Differentiation</a>
        </h3>
<p id="section-5.6-1">As discussed in section 5.4.3, the use of TE paths is a key
         capability of the NFIX solution framework described in this document.
         The Interconnect controller computes end-to-end TE paths between
         NFs and programs DC nodes, DCBs, ABR/ASBRs, via SR Policy, with the
         necessary label forwarding entries for each [headend, color, endpoint].
         The collection of [headend, endpoint] pairs for the same color
         constitutes a logical network topology, where each topology satisfies
         a given SLA requirement.<a href="#section-5.6-1" class="pilcrow">¶</a></p>
<p id="section-5.6-2">The Interconnect controller discovers the endpoints associated
          to a given topology (color) upon the reception of EVPN or IPVPN
          routes advertised by the endpoint. The EVPN and IPVPN NLRIs are
          advertised by the endpoint nodes along with a color extended
          community which identifies the topology to which the owner of the
          NLRI belongs. At a coarse level all the EVPN/IPVPN routes of the
          same VPN can be advertised with the same color, and therefore a
          TE topology would be established on a per-VPN basis. At a more
          granular level IPVPN and especially EVPN provide a more granular
          way of coloring routes, that will allow the Interconnect controller
          to associate multiple topologies to the same VPN. For example:<a href="#section-5.6-2" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-5.6-3.1">All the EVPN MAC/IP routes for a given VNF may be advertised with
            the same color. This would allow the Interconnect controller to
            associate topologies per VNF within the same VPN; that is, VNF1
            could be blue (e.g., low-latency topology) and VNF2 could be
            green (e.g., high-throughput).<a href="#section-5.6-3.1" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.6-3.2">The EVPN MAC/IP routes and Inclusive Multicast Ethernet Tag (IMET)
            route for VNF1 may be advertised with different colors, e.g.,
            red and brown, respectively. This would allow the association
            of e.g., a low-latency topology for unicast traffic to VNF1 and
            best-effort topology for BUM traffic to VNF1.<a href="#section-5.6-3.2" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.6-3.3">Each EVPN MAC/IP route or IP-Prefix route from a given VNF may
            be advertised with different color. This would allow the
            association of topologies at the host level or host route
            granularity.<a href="#section-5.6-3.3" class="pilcrow">¶</a>
</li>
        </ul>
</section>
<section id="section-5.7">
        <h3 id="name-automated-service-activatio">
<a href="#section-5.7" class="section-number selfRef">5.7. </a><a href="#name-automated-service-activatio" class="section-name selfRef">Automated Service Activation</a>
        </h3>
<p id="section-5.7-1">The automation of network and service connectivity for instantiation
         and mobility of virtual machines is a highly desirable attribute
         within data centers. Since this concerns service connectivity, it
         should be clear that this automation is relevant to virtual functions
         that belong to a service as opposed to a virtual network function that
         delivers services, such as a virtual PE router.<a href="#section-5.7-1" class="pilcrow">¶</a></p>
<p id="section-5.7-2">Within an SDN-enabled data center, a typical hierarchy from top to
        bottom would include a policy engine (or policy repository), one or
        more DC controllers, numerous hypervisors/container hosts that function
        as NVO endpoints, and finally the virtual machines(VMs)/containers,
        which we'll refer to generically as virtualization hosts.<a href="#section-5.7-2" class="pilcrow">¶</a></p>
<p id="section-5.7-3">The mechanisms used to communicate between the policy engine and DC
        controller, and between the DC controller and hypervisor/container are
        not relevant here and as such they are not discussed further. What is
        important is the interface and information exchange between the
        Interconnect controller and the data center SDN functions:<a href="#section-5.7-3" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-5.7-4.1">The Interconnect controller interfaces with the data center policy
          engine and publishes the available colors, where each color
          represents a topological service connectivity map that meets a set
          of constraints and SLA objectives. This interface is a
          straightforward API.<a href="#section-5.7-4.1" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.7-4.2">The Interconnect controller interfaces with the DC controller to
          learn overlay routes. This interface is BGP and uses the EVPN
          Address Family.<a href="#section-5.7-4.2" class="pilcrow">¶</a>
</li>
        </ul>
<p id="section-5.7-5">With the above framework in place, automation of network and
        service connectivity can be implemented as follows:<a href="#section-5.7-5" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-5.7-6.1">The virtualization host is turned-up. The NVO endpoint notifies
          the DC controller of the startup.<a href="#section-5.7-6.1" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.7-6.2">The DC controller retrieves service information, IP addressing
          information, and service 'color' for the virtualization host from
          the policy engine. The DC controller subsequently programs the
          associated forwarding information on the virtualization host.
          Since the DC controller is now aware of MAC and IP address
          information for the virtualization host, it advertises that
          information as an EVPN MAC Advertisement Route into the overlay.<a href="#section-5.7-6.2" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.7-6.3">The Interconnect controller receives the EVPN MAC Advertisement
          Route (potentially via a Route-Reflector) and correlates it with
          locally held service information and SLA requirements using Route
          Target and Color communities. If the relevant SR policies are not
          already in place to support the service requirements and logical
          connectivity, including any binding-SIDs, they are calculated and
          advertised to the relevant headends.<a href="#section-5.7-6.3" class="pilcrow">¶</a>
</li>
        </ul>
<p id="section-5.7-7">The same automated service activation principles can also be used to
        support the scenario where virtualization hosts are moved between
        hypervisors/container hosts for resourcing or other reasons. We
        refer to this simply as mobility. If a virtualization host is turned
        down the parent NVO endpoint notifies the DC controller, which in
        turn notifies the policy engine and withdraws any EVPN MAC
        Advertisement Routes. Thereafter all associated state is removed. When
        the virtualization host is turned up on a different
        hypervisor/container host, the automated service connectivity
        process outlined above is simply repeated.<a href="#section-5.7-7" class="pilcrow">¶</a></p>
</section>
<section id="section-5.8">
        <h3 id="name-service-function-chaining">
<a href="#section-5.8" class="section-number selfRef">5.8. </a><a href="#name-service-function-chaining" class="section-name selfRef">Service Function Chaining</a>
        </h3>
<p id="section-5.8-1">Service Function Chaining (SFC) defines an ordered set of abstract
         service functions and the subsequent steering of traffic through them.
         Packets are classified at ingress for processing by the required set
         of service functions (SFs) in an SFC-capable domain and are then
         forwarded through each SF in turn for processing. The ability to
         dynamically construct SFCs containing the relevant SFs in the right
         sequence is a key requirement for operators.<a href="#section-5.8-1" class="pilcrow">¶</a></p>
<p id="section-5.8-2">To enable flexible service function deployment models that support
          agile service insertion the NFIX architecture adopts the use of BGP
          as the control plane to distribute SFC information. The BGP control
          plane for Network Service Header (NSH) SFC
          <span>[<a href="#I-D.ietf-bess-nsh-bgp-control-plane" class="xref">I-D.ietf-bess-nsh-bgp-control-plane</a>]</span> is used for this purpose
          and defines two route types; the Service Function Instance Route
          (SFIR) and the Service Function Path Route (SFPR).<a href="#section-5.8-2" class="pilcrow">¶</a></p>
<p id="section-5.8-3">The SFIR is used to advertise the presence of a service function
          instance (SFI) as a function type (i.e. firewall, TCP optimizer)
          and is advertised by the node hosting that SFI. The SFIR is
          advertised together with a BGP Tunnel Encapsulation attribute
          containing details of how to reach that particular service function
          through the underlay network (i.e. IP address and encapsulation
          information).<a href="#section-5.8-3" class="pilcrow">¶</a></p>
<p id="section-5.8-4">The SFPRs contain service function path (SFP) information and
          one SFPR is originated for each SFP. Each SFPR contains the service
          path identifier (SPI) of the path, the sequence of service function
          types that make up the path (each of which has at least one instance
          advertised in an SFIR), and the service index (SI) for each listed
          service function to identify its position in the path.<a href="#section-5.8-4" class="pilcrow">¶</a></p>
<p id="section-5.8-5">Once a Classifier has determined which flows should be mapped to
          a given SFP, it imposes an NSH <span>[<a href="#RFC8300" class="xref">RFC8300</a>]</span> on those packets, setting
          the SPI to that of the selected service path (advertised in an SFPR),
          and the SI to the first hop in the path. As NSH is encapsulation
          agnostic, the NSH encapsulated packet is then forwarded through the
          appropriate tunnel to reach the service function forwarder (SFF)
          supporting that service function instance (advertised in an SFIR).
          The SFF removes the tunnel encapsulation and forwards the packet
          with the NSH to the relevant SF based upon a lookup of the SPI/SI.
          When it is returned from the SF with a decremented SI value, the SFF
          forwards the packet to the next hop in the SFP using the tunnel
          information advertised by that SFI. This procedure is repeated
          until the last hop of the SFP is reached.<a href="#section-5.8-5" class="pilcrow">¶</a></p>
<p id="section-5.8-6">The use of the NSH in this manner allows for service chaining
          with topological and transport independence. It also allows for the
          deployment of SFIs in a condensed or dispersed fashion depending on
          operator preference or resource availability. Service function chains
          are built in their own overlay network and share a common underlay
          network, where that common underlay network is the NFIX fabric
          described in section 5.4.  BGP updates containing an SFIR or
          SFPR are advertised in conjunction with one or more Route
          Targets (RTs), and each node in a service function overlay network
          is configured with one or more import RTs. As a result, nodes will
          only import routes that are applicable and that local policy dictates.
          This provides the ability to support multiple service function
          overlay networks or the construction of service function chains
          within L3VPN or EVPN services.<a href="#section-5.8-6" class="pilcrow">¶</a></p>
<p id="section-5.8-7">Although SFCs are constructed in a unidirectional manner, the BGP
          control plane for NSH SFC allows for the optional association of
          multiple paths (SFPRs). This provides the ability to construct a
          bidirectional service function chain in the presence of multiple
          equal-cost paths between source and destination to avoid problems
          that SFs may suffer with traffic asymmetry.<a href="#section-5.8-7" class="pilcrow">¶</a></p>
<p id="section-5.8-8">The proposed SFC model can be considered decoupled in that the use
          of SR as a transport between SFFs is completely independent of the
          use of NSH to define the SFC. That is, it uses an NSH-based SFC and
          SR is just one of many encapsulations that could be used between
          SFFs. A similar more integrated approach proposes encoding a
          service function as a segment so that an SFC can be constructed as
          a segment-list. In this case it can be considered an SR-based SFC
          with an NSH-based service plane since the SF is unaware of the
          presence of the SR. Functionally both approaches are very similar
          and as such both could be adopted and could work in parallel.
          Construction of SFCs based purely on SR (SF is SR-aware) are not
          considered at this time.<a href="#section-5.8-8" class="pilcrow">¶</a></p>
</section>
<section id="section-5.9">
        <h3 id="name-stability-and-availability">
<a href="#section-5.9" class="section-number selfRef">5.9. </a><a href="#name-stability-and-availability" class="section-name selfRef">Stability and Availability</a>
        </h3>
<p id="section-5.9-1">Any network architecture should have the capability to self-restore
         following the failure of a network element. The time to reconverge
         following the failure needs to be minimal to avoid evident
         disruptions in service. This section discusses protection mechanisms
         that are available for use and their applicability to the proposed
         architecture.<a href="#section-5.9-1" class="pilcrow">¶</a></p>
<section id="section-5.9.1">
          <h4 id="name-igp-reconvergence">
<a href="#section-5.9.1" class="section-number selfRef">5.9.1. </a><a href="#name-igp-reconvergence" class="section-name selfRef">IGP Reconvergence</a>
          </h4>
<p id="section-5.9.1-1">Within the construct of an IGP topology the Topology Independent
            Loop Free Alternate (TI-LFA) <span>[<a href="#I-D.ietf-rtgwg-segment-routing-ti-lfa" class="xref">I-D.ietf-rtgwg-segment-routing-ti-lfa</a>]</span>
            can be used to provide a local repair mechanism that offers both
            link and node protection.<a href="#section-5.9.1-1" class="pilcrow">¶</a></p>
<p id="section-5.9.1-2">TI-LFA is a repair mechanism, and as such it is reactive and
            initially needs to detect a given failure. To provide fast failure
            detection the Bidirectional Forwarding Mechanism (BFD) is used.
            Consideration needs to be given to the restoration capabilities
            of the underlying transmission when deciding values for message
            intervals and multipliers to avoid race conditions, but failure
            detection in the order of 50 milliseconds can reasonably be
            anticipated. Where Link Aggregation Groups (LAG) are used,
            micro-BFD [RFC7130] can be used to similar effect. Indeed, to
            allow for potential incremental growth in capacity it is not
            uncommon for operators to provision all network links as LAG
            and use micro-BFD from the outset.<a href="#section-5.9.1-2" class="pilcrow">¶</a></p>
</section>
<section id="section-5.9.2">
          <h4 id="name-data-center-reconvergence">
<a href="#section-5.9.2" class="section-number selfRef">5.9.2. </a><a href="#name-data-center-reconvergence" class="section-name selfRef">Data Center Reconvergence</a>
          </h4>
<p id="section-5.9.2-1">Clos fabrics are extremely common within data centers, and
            fundamental to a Clos fabric is the ability to load-balance using
            Equal Cost Multipath (ECMP). The number of ECMP paths will vary
            dependent on the number of devices in the parent tier but will
            never be less than two for redundancy purposes with traffic
            hashed over the available paths. In this scenario the availability
            of a backup path in the event of failure is implicit. Commonly
            within the DC, rather than computing protect paths (like LFA),
            techniques such as 'fast rehash' are often utilized. In this
            particular case, the failed next-hop is removed from the multi-path
            forwarding data structure and traffic is then rehashed over the
            remaining active paths.<a href="#section-5.9.2-1" class="pilcrow">¶</a></p>
<p id="section-5.9.2-2">In BGP-only data centers this relies on the implementation of BGP
            multipath. As network elements in the lower tier of a Clos fabric
            will frequently belong to different ASNs, this includes the ability
            to load-balance to a prefix with different AS_PATH attribute values
            while having the same AS_PATH length; sometimes referred to as
            'multipath relax' or 'multipath multiple-AS' [RFC7938].<a href="#section-5.9.2-2" class="pilcrow">¶</a></p>
<p id="section-5.9.2-3">Failure detection relies upon declaring a BGP session down and
            removing any prefixes learnt over that session as soon as the link
            is declared down. As links between network elements predominantly
            use direct point-to-point fiber, a link failure should be detected
            within milliseconds. BFD is also commonly used to detect IP layer
            failures.<a href="#section-5.9.2-3" class="pilcrow">¶</a></p>
</section>
<section id="section-5.9.3">
          <h4 id="name-exchange-of-inter-domain-ro">
<a href="#section-5.9.3" class="section-number selfRef">5.9.3. </a><a href="#name-exchange-of-inter-domain-ro" class="section-name selfRef">Exchange of Inter-Domain Routes</a>
          </h4>
<p id="section-5.9.3-1">Labeled unicast BGP together with SR Prefix-SID extensions are
            used to exchange PNF and/or VNF endpoints between domains to create
            end-to-end connectivity without TE. When advertising between domains
            we assume that a given BGP prefix is advertised by at least two
            border routers (DCBs, ABRs, ASBRs) making prefixes reachable via at
            least two next-hops.<a href="#section-5.9.3-1" class="pilcrow">¶</a></p>
<p id="section-5.9.3-2">BGP Prefix Independent Convergence (PIC)
            <span>[<a href="#I-D.ietf-rtgwg-bgp-pic" class="xref">I-D.ietf-rtgwg-bgp-pic</a>]</span> allows failover to a pre-computed
            and pre-installed secondary next-hop when the primary next-hop
            fails and is independent of the number of destination prefixes
            that are affected by the failure. When the primary BGP next-hop
            fails, it should be clear that BGP PIC depends on the availability o
            f a secondary next-hop in the Pathlist. To ensure that multiple
            paths to the same destination are visible the BGP ADD-PATH <span>[<a href="#RFC7911" class="xref">RFC7911</a>]</span>
            can be used to allow for advertisement of multiple paths for the
            same address prefix. Dual-homed EVPN/IP-VPN prefixes also have the
            alternative option of allocating different Route-Distinguishers (RDs).
            To trigger the switch from primary to secondary next-hop PIC needs
            to detect the failure and many implementations support
            'next-hop tracking' for this purpose. Next-hop tracking monitors
            the routing-table and if the next-hop prefix is removed will
            immediately invalidate all BGP prefixes learnt through that
            next-hop. In the absence of next-hop tracking, multihop BFD
            <span>[<a href="#RFC5883" class="xref">RFC5883</a>]</span> could optionally be used as a fast failure detection
            mechanism.<a href="#section-5.9.3-2" class="pilcrow">¶</a></p>
</section>
<section id="section-5.9.4">
          <h4 id="name-controller-redundancy">
<a href="#section-5.9.4" class="section-number selfRef">5.9.4. </a><a href="#name-controller-redundancy" class="section-name selfRef">Controller Redundancy</a>
          </h4>
<p id="section-5.9.4-1">With the Interconnect controller providing an integral part of the
            networks' capabilities a redundant controller design is clearly
            prudent. To this end we can consider both availability and
            redundancy. Availability refers to the survivability of a single
            controller system in a failure scenario. A common strategy for
            increasing the availability of a single controller system is to
            build the system in a high-availability cluster such that it
            becomes a confederation of redundant constituent parts as opposed
            to a single monolithic system. Should a single part fail, the system
            can still survive without the requirement to failover to a standby
            controller system. Methods for detection of a failure of one or more
            member parts of the cluster are implementation specific.<a href="#section-5.9.4-1" class="pilcrow">¶</a></p>
<p id="section-5.9.4-2">To provide contingency for a complete system failure a
            geo-redundant standby controller system is required. When redundant
            controllers are deployed a coherent strategy is needed that
            provides a master/standby election mechanism, the ability to
            propagate the outcome of that election to network elements as
            required, an inter-system failure detection mechanism, and the
            ability to synchronize state across both systems such that the
            standby controller is fully aware of current state should it need
            to transition to master controller.<a href="#section-5.9.4-2" class="pilcrow">¶</a></p>
<p id="section-5.9.4-3">Master/standby election, state synchronisation, and failure
            detection between geo-redundant sites can largely be considered a
            local implementation matter. The requirement to propagate the
            outcome of the master/standby election to network elements depends
            on a) the mechanism that is used to instantiate SR policies, and
            b) whether the SR policies are controller-initiated or
            headend-initiated, and these are discussed in the following
            sub-sections. In either scenario, state of SR policies should
            be advertised northbound to both master/standby controllers using
            either PCEP LSP State Report messages or SR policy extensions to
            BGP link-state <span>[<a href="#I-D.ietf-idr-te-lsp-distribution" class="xref">I-D.ietf-idr-te-lsp-distribution</a>]</span>.<a href="#section-5.9.4-3" class="pilcrow">¶</a></p>
<section id="section-5.9.4.1">
            <h5 id="name-sr-policy-initiator">
<a href="#section-5.9.4.1" class="section-number selfRef">5.9.4.1. </a><a href="#name-sr-policy-initiator" class="section-name selfRef">SR Policy Initiator</a>
            </h5>
<p id="section-5.9.4.1-1">Controller-initiated SR policies are suited for auto-creation
              of tunnels based on service route discovery and policy-driven
              route/flow programming and are ephemeral. Headend-initiated
              tunnels allow for permanent configuration state to be held on
              the headend and are suitable for static services that are not
              subject to dynamic changes. If all SR policies are
              controller-initiated, it negates the requirement to propagate
              the outcome of the master/standby election to network elements.
              This is because headends have no requirement for unsolicited
              requests to a controller, and therefore have no requirement to
              know which controller is master and which one is standby.
              A headend may respond to a message from a controller, but it
              is not unsolicited.<a href="#section-5.9.4.1-1" class="pilcrow">¶</a></p>
<p id="section-5.9.4.1-2">If some or all SR policies are headend-initiated, then the
              requirement to propagate the outcome of the master/standby
              election exists. This is further discussed in the following
              sub-section.<a href="#section-5.9.4.1-2" class="pilcrow">¶</a></p>
</section>
<section id="section-5.9.4.2">
            <h5 id="name-sr-policy-instantiation-mec">
<a href="#section-5.9.4.2" class="section-number selfRef">5.9.4.2. </a><a href="#name-sr-policy-instantiation-mec" class="section-name selfRef">SR Policy Instantiation Mechanism</a>
            </h5>
<p id="section-5.9.4.2-1">While candidate paths of SR policies may be provided using
              BGP, PCEP, Netconf, or local policy/configuration, this document
              primarily considers the use of PCEP or BGP.<a href="#section-5.9.4.2-1" class="pilcrow">¶</a></p>
<p id="section-5.9.4.2-2">When PCEP <span>[<a href="#RFC5440" class="xref">RFC5440</a>]</span><span>[<a href="#RFC8231" class="xref">RFC8231</a>]</span><span>[<a href="#RFC8281" class="xref">RFC8281</a>]</span> is used for instantiation
              of candidate paths of SR policies
              <span>[<a href="#I-D.barth-pce-segment-routing-policy-cp" class="xref">I-D.barth-pce-segment-routing-policy-cp</a>]</span> every
              headend/PCC should establish a PCEP session with the master
              and standby controllers. To signal standby state to the PCC
              the standby controller may use a PCEP Notification message to
              set the PCEP session into overload state. While in this overload
              state the standby controller will accept path computation LSP
              state report (PCRpt) messages without delegation but will reject
              path computation requests (PCReq) and any path computation
              reports (PCRpt) with the delegation bit set. Further, the standby
              controller will not path computation originate initiate
              messages (PCInit) or path computation update request messages
              (PCUpd). In the event of the failure of the master controller,
              the standby controller will transition to active and remove the
              PCEP overload state. Following expiration of the PCEP
              redelegation timeout at the PCC any LSPs will be redelegated to
              the newly transitioned active controller. LSP state is not
              impacted unless redelegation is not possible before the state
              timeout interval expires.<a href="#section-5.9.4.2-2" class="pilcrow">¶</a></p>
<p id="section-5.9.4.2-3">When BGP is used for instantiation of SR policies every headend
              should establish a BGP session with the master and standby
              controller capable of exchanging SR TE Policy SAFI. Candidate
              paths of SR policies are advertised only by the active
              controller. If the master controller should experience a failure,
              then SR policies learnt from that controller may be removed before
              they are re-advertised by the standby (or newly-active)
              controller. To minimize this possibility BGP speakers that 
   advertise and instantiate SR policies can implement Long Lived
   Graceful Retart (LLGR) <span>[<a href="#I-D.ietf-idr-long-lived-gr" class="xref">I-D.ietf-idr-long-lived-gr</a>]</span>,
   also known as BGP persistence, to retain existing routes treated
   as least-preferred until the new route arrives. In the absence of
   LLGR, two other alternatives are possible:<a href="#section-5.9.4.2-3" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-5.9.4.2-4.1">Provide a static backup SR policy.<a href="#section-5.9.4.2-4.1" class="pilcrow">¶</a>
</li>
              <li class="normal" id="section-5.9.4.2-4.2">Fallback to the default forwarding path.<a href="#section-5.9.4.2-4.2" class="pilcrow">¶</a>
</li>
            </ul>
</section>
</section>
<section id="section-5.9.5">
          <h4 id="name-path-and-segment-liveliness">
<a href="#section-5.9.5" class="section-number selfRef">5.9.5. </a><a href="#name-path-and-segment-liveliness" class="section-name selfRef">Path and Segment Liveliness</a>
          </h4>
<p id="section-5.9.5-1">When using traffic-engineered SR paths only the ingress router
              holds any state. The exception here is where BSIDs are used,
              which also implies some state is maintained at the BSID anchor.
              As there is no control plane set-up, it follows that there is
              no feedback loop from transit nodes of the path to notify the
              headend when a non-adjacent point of the SR path fails. The
              Interconnect controller however is aware of all paths that are
              impacted by a given network failure and should take the
              appropriate action. This action could include withdrawing an
              SR policy if a suitable candidate path is already in place, or
              simply sending a new SR policy with a different segment-list and
              a higher preference value assigned to it.<a href="#section-5.9.5-1" class="pilcrow">¶</a></p>
<p id="section-5.9.5-2">Verification of data plane liveliness is the responsibility of
              the path headend. A given SR policy may be associated with
              multiple candidate paths and for the sake of clarity, we'll
              assume two for redundancy purposes (which can be diversely
              routed). Verification of the liveliness of these paths can be
              achieved using seamless BFD (S-BFD)<span>[<a href="#RFC7880" class="xref">RFC7880</a>]</span>,
   which provides an in-band failure detection mechanism capable of
   detecting failure in the order of tens of milliseconds. Upon
   failure of the active path, failover to a secondary candidate
   path can be activated at the path headend. Details of the actual
   failover and revert mechanisms are a local implementation
   matter.<a href="#section-5.9.5-2" class="pilcrow">¶</a></p>
<p id="section-5.9.5-3">S-BFD provides a fast and scalable failure detection mechanism
              but is unlikely to be implemented in many VNFs given their
              inability to offload the process to purpose-built hardware. In
              the absence of an active failure detection mechanism such as
              S-BFD the failover from active path to secondary candidate
              path can be triggered using continuous path validity checks.
              One of the criteria that a candidate path uses to determine its
              validity is the ability to perform path resolution for the first
              SID to one or more outgoing interface(s) and next-hop(s). From
              the perspective of the VNF headend the first SID in the
              segment-list will very likely be the DCB (as BSID anchor) but
              could equally be another Prefix-SID hop within the data center.
              Should this segment experience a non-recoverable failure, the
              headend will be unable to resolve the first SID and the path
              will be considered invalid. This will trigger a failover action
              to a secondary candidate path.<a href="#section-5.9.5-3" class="pilcrow">¶</a></p>
<p id="section-5.9.5-4">Injection of S-BFD packets is not just constrained to the
              source of an end-to-end LSP. When an S-BFD packet is injected
              into an SR policy path it is encapsulated with the label stack
              of the associated segment-list. It is possible therefore to
              run S-BFD from a BSID anchor for just that section of the
              end-to-end path (for example, from DCB to DCB). This allows a
              BSID anchor to detect failure of a path and take corrective
              action, while maintaining opacity between domains.<a href="#section-5.9.5-4" class="pilcrow">¶</a></p>
</section>
</section>
<section id="section-5.10">
        <h3 id="name-scalability">
<a href="#section-5.10" class="section-number selfRef">5.10. </a><a href="#name-scalability" class="section-name selfRef">Scalability</a>
        </h3>
<p id="section-5.10-1">There are many aspects to consider regarding scalability of the
            NFIX architecture. The building blocks of NFIX are standards-based
            technologies individually designed to scale for internet provider
            networks. When combined they provide a flexible and scalable
            solution:<a href="#section-5.10-1" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-5.10-2.1">BGP has been proven to scale and operate with millions of
                routes being exchanged. Specifically, BGP labeled unicast has
                been deployed and proven to scale in existing seamless-MPLS
                networks.<a href="#section-5.10-2.1" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.10-2.2">By placing forwarding instructions in the header of a packet,
                segment routing reduces the amount of state required in the
                network allowing the scale of greater number of transport
                tunnels. This aids in the feasibility of the NFIX architecture
                to permit the automated aspects of SR policy creation without
                having an impact on the state in the core of the network.<a href="#section-5.10-2.2" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.10-2.3">The choice of utilizing native SR-MPLS or SR over IP in the
                data center continues to permit horizontal scaling without
                introducing new state inside of the data center fabric while
                still permitting seamless end to end path forwarding
                integration.<a href="#section-5.10-2.3" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.10-2.4">BSIDs play a key role in the NFIX architecture as their use
                provides the ability to traffic-engineer across large network
                topologies consisting of many hops regardless of hardware
                capability at the headend. From a scalability perspective
                the use of BSIDs facilitates better scale due to the fact
                that detailed information about the SR paths in a domain has
                been abstracted and localized to the BSID anchor point only.
                When BSIDs are re-used amongst one or many headends they
                reduce the amount of path calculation and updates required
                at network edges while still providing seamless end to end
                path forwarding.<a href="#section-5.10-2.4" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.10-2.5">The architecture of NFIX continues to use an independent DC
                controller. This allows continued independent scaling of data
                center management in both policy and local forwarding
                functions, while off-loading the end-to-end optimal path
                placement and automation to the Interconnect controller.
                The optimal path placement is already a scalable function
                provided in a PCE architecture. The Interconnect controller
                must compute paths, but it is not burdened by the management
                of virtual entity lifecycle and associated forwarding
                policies.<a href="#section-5.10-2.5" class="pilcrow">¶</a>
</li>
        </ul>
<p id="section-5.10-3">It must be acknowledged that with the amalgamation of the
            technology building blocks and the automation required by NFIX,
            there is an additional burden on the Interconnect controller.
            The scaling considerations are dependent on many variables, but an
            implementation of a Interconnect controller shares many overlapping
            traits and scaling concerns as PCE, where the controller and
            PCE both must:<a href="#section-5.10-3" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-5.10-4.1">Discover and listen to topological state changes of the
                IP/MPLS topology.<a href="#section-5.10-4.1" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.10-4.2">Compute traffic-engineered intra and inter domain paths across
                large service provider topologies.<a href="#section-5.10-4.2" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.10-4.3">Synchronize, track and update thousands of LSPs to network
                devices upon network state changes.<a href="#section-5.10-4.3" class="pilcrow">¶</a>
</li>
        </ul>
<p id="section-5.10-5">Both entail topologies that contain tens of thousands of nodes
            and links. The Interconnect controller in an NFIX architecture
            takes on the additional role of becoming end to end service aware
            and discovering data center entities that were traditionally
            excluded from a controllers scope. Although not exhaustive, an
            NFIX Interconnect controller is impacted by some of the following:<a href="#section-5.10-5" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-5.10-6.1">The number of individual services, the number of endpoints
                that may exist in each service, the distribution of endpoints
                in a virtualized environment, and how many data centers may
                exist. Medium or large sized data centers may be capable to
                host more virtual endpoints per host, but with the move to
                smaller edge-clouds the number of headends that require
                inter-connectivity increases compared to the density of
                localized routing in a centralized data center model. The
                outcome has an impact on the number of headend devices which
                may require tunnel management by the Interconnect controller.<a href="#section-5.10-6.1" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.10-6.2">Assuming a given BSID satisfies SLA, the ability to re-use
                BSIDs across multiple services reduces the number of paths
                to track and manage. However, the number of color or unique
                SLA definitions, and criteria such as bandwidth constraints
                impacts WAN traffic distribution requirements. As BSIDs play
                a key role for VNF connectivity, this potentially increases
                the number of BSID paths required to permit appropriate traffic
                distribution. This also impacts the number of tunnels which may
                be re-used on a given headend for different services.<a href="#section-5.10-6.2" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.10-6.3">The frequency of virtualized hosts being created and
                destroyed and the general activity within a given service.
                The controller must analyze, track, and correlate the activity
                of relevant BGP routes to track addition and removal of
                service host or host subnets, and determine whether new SR
                policies should be instantiated, or stale unused SR policies
                should be removed from the network.<a href="#section-5.10-6.3" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.10-6.4">The choice of SR instantiation mechanism impacts the number of
                communication sessions the controller may require. For example,
                the BGP based mechanism may only require a small number of
                sessions to route reflectors, whereas PCEP may require a
                connection to every possible leaf in the network and any
                BSID anchors.<a href="#section-5.10-6.4" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.10-6.5">The number of hops within one or many WAN domains may affect
                the number of BSIDs required to provide transit for VNF/PNF,
                PNF/PNF, or VNF/VNF inter-connectivity.<a href="#section-5.10-6.5" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-5.10-6.6">Relative to traditional WAN topologies, traditional data
                centers are generally topologically denser in node and link
                connectivity which is required to be discovered by the
                Interconnect controller, resulting in a much larger,
                dense link-state database on the Interconnect controller.<a href="#section-5.10-6.6" class="pilcrow">¶</a>
</li>
        </ul>
<section id="section-5.10.1">
          <h4 id="name-asymmetric-model-b-for-vpn-">
<a href="#section-5.10.1" class="section-number selfRef">5.10.1. </a><a href="#name-asymmetric-model-b-for-vpn-" class="section-name selfRef">Asymmetric Model B for VPN Families</a>
          </h4>
<p id="section-5.10.1-1">With the instantiation of multiple TE paths between any two
                VNFs in the NFIX network, the number of SR Policy
                (remote endpoint, color) routes, BSIDs and labels to support
                on VNFs becomes a choke point in the architecture. The fact
                that some VNFs are limited in terms of forwarding resources
                makes this aspect an important scale issue.<a href="#section-5.10.1-1" class="pilcrow">¶</a></p>
<p id="section-5.10.1-2">As an example, if VNF1 and VNF2 in Figure 1 are associated
                to multiple topologies 1..n, the Interconnect controller will
                instantiate n TE paths in VNF1 to reach VNF2:<a href="#section-5.10.1-2" class="pilcrow">¶</a></p>
<p id="section-5.10.1-3">[VNF1,color-1,VNF2] --&gt; BSID 1<a href="#section-5.10.1-3" class="pilcrow">¶</a></p>
<p id="section-5.10.1-4">[VNF1,color-2,VNF2] --&gt; BSID 2<a href="#section-5.10.1-4" class="pilcrow">¶</a></p>
<p id="section-5.10.1-5">...<a href="#section-5.10.1-5" class="pilcrow">¶</a></p>
<p id="section-5.10.1-6">[VNF1,color-n,VNF2] --&gt; BSID n<a href="#section-5.10.1-6" class="pilcrow">¶</a></p>
<p id="section-5.10.1-7">Similarly, m TE paths may be instantiated on VNF1 to reach
               VNF3, another p TE paths to reach VNF4, and so on for all the
               VNFs that VNF1 needs to communicate with in DC2. As it can be
               observed, the number of forwarding resources to be instantiated
               on VNF1 may significantly grow with the number of remote
               [endpoint, color] pairs, compared with a best-effort
               architecture in which the number forwarding resources in
               VNF1 grows with the number of endpoints only.<a href="#section-5.10.1-7" class="pilcrow">¶</a></p>
<p id="section-5.10.1-8">This scale issue on the VNFs can be relieved by the use of
            an asymmetric model B service layer. The concept is illustrated
            in Figure 3.<a href="#section-5.10.1-8" class="pilcrow">¶</a></p>
<div id="figure4">
<figure id="figure-4">
            <div class="alignLeft art-text artwork" id="section-5.10.1-9.1">
<pre>



                                                +------------+
          &lt;-------------------------------------|    WAN     |
          |  SR Policy      +-------------------| Controller |
          |  BSID m         |   SR Policy       +------------+
          v  {DCI1,n,DCI2}  v   BSID n
                                {1,2,3,4,5,DCI2}
         +----------------+  +----------------+  +----------------+
         |     +----+     |  |                |  |     +----+     |
       +----+  | RR |    +----+              +----+    | RR |   +----+
       |VNF1|  +----+    |DCI1|              |DCI2|    +----+   |VNF2|
       +----+            +----+              +----+             +----+
         |       DC1      |  |       WAN      |  |       DC2      |
         +----------------+  +----------------+  +----------------+

         &lt;-------- &lt;-------------------------- NHS &lt;------ &lt;------
                              EVPN/VPN-IPv4/v6(colored)

         +-----------------------------------&gt;     +-------------&gt;
                   TE path to DCI2                ECMP path to VNF2
               (BSID to segment-list
                expansion on DCI1)


</pre>
</div>
<figcaption><a href="#figure-4" class="selfRef">Figure 4</a></figcaption></figure>
</div>
<p id="section-5.10.1-10" class="keepWithPrevious">Asymmetric Model B Service Layer<a href="#section-5.10.1-10" class="pilcrow">¶</a></p>
<p id="section-5.10.1-11">Consider the different n topologies needed between VNF1 and VNF2
            are really only relevant to the different TE paths that exist in
            the WAN. The WAN is the domain in the network where there can be
            significant differences in latency, throughput or packet loss
            depending on the sequence of nodes and links the traffic goes
            through. Based on that assumption, for traffic from VNF1 to DCB2
            in Figure 4, traffic from DCB2 to VNF2 can simply take an ECMP
            path. In this case an asymmetric model B Service layer can
            significantly relieve the scale pressure on VNF1.<a href="#section-5.10.1-11" class="pilcrow">¶</a></p>
<p id="section-5.10.1-12">From a service layer perspective, the NFIX architecture
            described up to now can be considered 'symmetric', meaning that
            the EVPN/IPVPN advertisements from e.g., VNF2 in Figure 2, are
            received on VNF1 with the next-hop of VNF2, and vice versa for
            VNF1's routes on VNF2. SR Policies to each VNF2 [endpoint, color]
            are then required on the VNF1.<a href="#section-5.10.1-12" class="pilcrow">¶</a></p>
<p id="section-5.10.1-13">In the 'asymmetric' service design illustrated in Figure 4, VNF2's
            EVPN/IPVPN routes are received on VNF1 with the next-hop of DCB2,
            and VNF1's routes are received on VNF2 with next-hop of DCB1. Now
            SR policies instantiated on VNFs can be reduced to only the number
            of TE paths required to reach the remote DCB. For example,
            considering n topologies, in a symmetric model VNF1 has to be
            instantiated with n SR policy paths per remote VNF in DC2, whereas
            in the asymmetric model of Figure 4, VNF1 only requires n SR policy
            paths per DC, i.e., to DCB2.<a href="#section-5.10.1-13" class="pilcrow">¶</a></p>
<p id="section-5.10.1-14">Asymmetric model B is a simple design choice that only requires
            the ability (on the DCB nodes) to set next-hop-self on the
            EVPN/IPVPN routes advertised to the WAN neighbors and not do
            next-hop-self for routes advertised to the DC neighbors. With
            this option, the Interconnect controller only needs to establish
            TE paths from VNFs to remote DCBs, as opposed to VNFs to remote
            VNFs.<a href="#section-5.10.1-14" class="pilcrow">¶</a></p>
</section>
</section>
</section>
<section id="section-6">
      <h2 id="name-illustration-of-use">
<a href="#section-6" class="section-number selfRef">6. </a><a href="#name-illustration-of-use" class="section-name selfRef">Illustration of Use</a>
      </h2>
<p id="section-6-1">For the purpose of illustration, this section provides some
          examples of how different end-to-end tunnels are instantiated
          (including the relevant protocols, SID values/label stacks etc.)
          and how services are then overlaid onto those LSPs.<a href="#section-6-1" class="pilcrow">¶</a></p>
<section id="section-6.1">
        <h3 id="name-reference-topology">
<a href="#section-6.1" class="section-number selfRef">6.1. </a><a href="#name-reference-topology" class="section-name selfRef">Reference Topology</a>
        </h3>
<p id="section-6.1-1">The following network diagram illustrates the reference network
            topology that is used for illustration purposes in this section.
            Within the data centers leaf and spine network elements may be
            present but are not shown for the purpose of clarity.<a href="#section-6.1-1" class="pilcrow">¶</a></p>
<div id="figure5">
<figure id="figure-5">
          <div class="alignLeft art-text artwork" id="section-6.1-2.1">
<pre>


                    +----------+
                    |Controller|
                    +----------+
                      /  |  \
             +----+          +----+          +----+     +----+
     ~ ~ ~ ~ | R1 |----------| R2 |----------| R3 |-----|AGN1| ~ ~ ~ ~
     ~       +----+          +----+          +----+     +----+       ~
     ~   DC1    |                            /  |         |    DC2   ~
   +----+       |      L=5   +----+   L=5   /   |       +----+    +----+
   | Sn |       |    +-------| R4 |--------+    |       |AGN2|    | Dn |
   +----+       |   /  M=20  +----+  M=20       |       +----+    +----+
     ~          |  /                            |         |          ~
     ~       +----+     +----+    +----+     +----+     +----+       ~
     ~ ~ ~ ~ | R5 |-----| R6 |----| R7 |-----| R8 |-----|AGN3| ~ ~ ~ ~
             +----+     +----+    +----+     +----+     +----+


</pre>
</div>
<figcaption><a href="#figure-5" class="selfRef">Figure 5</a></figcaption></figure>
</div>
<p id="section-6.1-3" class="keepWithPrevious">Reference Topology<a href="#section-6.1-3" class="pilcrow">¶</a></p>
<p id="section-6.1-4">The following applies to the reference topology in figure 5:<a href="#section-6.1-4" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-6.1-5.1">Data center 1 and data center 2 both run BGP/SR. Both data
              centers run leaf/spine topologies, which are not shown for
              the purpose of clarity.<a href="#section-6.1-5.1" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.1-5.2">R1 and R5 function as data center border routers for DC 1.
              AGN1 and AGN3 function as data center border routers for DC 2.<a href="#section-6.1-5.2" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.1-5.3">Routers R1 through R8 form an independent ISIS-OSPF/SR
              instance.<a href="#section-6.1-5.3" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.1-5.4">Routers R3, R8, AGN1, AGN2, and AGN2 form an independent
              ISIS-OSPF/SR instance.<a href="#section-6.1-5.4" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.1-5.5">All IGP link metrics within the wide area network are metric
              10 except for links R5-R4 and R4-R3 which are both metric 20.<a href="#section-6.1-5.5" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.1-5.6">All links have a unidirectional latency of 10 milliseconds
              except for links R5-R4 and R4-R3 which both have a unidirectional
              latency of 5 milliseconds.<a href="#section-6.1-5.6" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.1-5.7">Source 'Sn' and destination 'Dn' represent one or more network
              functions.<a href="#section-6.1-5.7" class="pilcrow">¶</a>
</li>
        </ul>
</section>
<section id="section-6.2">
        <h3 id="name-pnf-to-pnf-connectivity">
<a href="#section-6.2" class="section-number selfRef">6.2. </a><a href="#name-pnf-to-pnf-connectivity" class="section-name selfRef">PNF to PNF Connectivity</a>
        </h3>
<p id="section-6.2-1">The first example demonstrates the simplest form of connectivity;
            PNF to PNF. The example illustrates the instantiation of a
            unidirectional TE path from R1 to AGN2 and its consumption by an
            EVPN service. The service has a requirement for high-throughput
            with no strict latency requirements. These service requirements
            are catalogued and represented using the color blue.<a href="#section-6.2-1" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-6.2-2.1">An EVPN service is provisioned at R1 and AGN2.<a href="#section-6.2-2.1" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.2-2.2">The Interconnect controller computes the path from R1 to
                AGN2 and calculates that the optimal path based on the service
                requirements and overall network optimization is
                R1-R5-R6-R7-R8-AGN3-AGN2. The segment-list to represent the
                calculated path could be constructed in numerous ways. It could
                be strict hops represented by a series of Adj-SIDs. It could be
                loose hops using ECMP-aware Node-SIDs, for example {R7, AGN2},
                or it could be a combination of both Node-SIDs and Adj-SIDs.
                Equally, BSIDs could be used to reduce the number of labels
                that need to be imposed at the headend. In this example, strict
                Adj-SID hops are used with a BSID at the area border router R8,
                but this should not be interpreted as the only way a path and
                segment-list can be represented.<a href="#section-6.2-2.2" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.2-2.3">The Interconnect controller advertises a BGP SR Policy to R8
                with BSID 1000, and a segment-list containing segments
                {AGN3, AGN2}.<a href="#section-6.2-2.3" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.2-2.4">The Interconnect controller advertises a BGP SR Policy to R1
                with BSID 1001, and a segment-list containing segments
                {R5, R6, R7, R8, 1000}. The policy is identified using
                the tuple [headed = R1, color = blue, endpoint = AGN2].<a href="#section-6.2-2.4" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.2-2.5">AGN2 advertises an EVPN MAC Advertisement Route for MAC M1,
                which is learned by R1. The route has a next-hop of AGN2, an
                MPLS label of L1, and it carries a color extended community
                with the value blue.<a href="#section-6.2-2.5" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.2-2.6">R1 has a valid SR policy [color = blue, next-hop = AGN2]
                with segment-list {R5, R6, R7, R8, 1000}. R1 therefore
                associates the MAC address M1 with that policy and programs
                the relevant information into the forwarding path.<a href="#section-6.2-2.6" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.2-2.7">The Interconnect controller also learns the EVPN MAC Route
                advertised by AGN2. The purpose of this is two-fold. It allows
                the controller to correlate the service overlay with the
                underlying transport LSPs, thus creating a service
                connectivity map. It also allows the controller to dynamically
                create LSPs based upon service requirements if they do not
                already exist, or to optimize them if network conditions
                change.<a href="#section-6.2-2.7" class="pilcrow">¶</a>
</li>
        </ul>
</section>
<section id="section-6.3">
        <h3 id="name-vnf-to-pnf-connectivity">
<a href="#section-6.3" class="section-number selfRef">6.3. </a><a href="#name-vnf-to-pnf-connectivity" class="section-name selfRef">VNF to PNF Connectivity</a>
        </h3>
<p id="section-6.3-1">The next example demonstrates VNF to PNF connectivity and
            illustrates the instantiation of a unidirectional TE path from S1
            to AGN2. The path is consumed by an IP-VPN service that has a basic
            set of service requirements and as such simply uses IGP metric as
            a path computation objective. These basic service requirements
            are cataloged and represented using the color red.<a href="#section-6.3-1" class="pilcrow">¶</a></p>
<p id="section-6.3-2">In this example S1 is a VNF with full IP routing and MPLS
            capability that interfaces to the data center underlay/overlay
            and serves as the NVO tunnel endpoint.<a href="#section-6.3-2" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-6.3-3.1">An IP-VPN service is provisioned at S1 and AGN2.<a href="#section-6.3-3.1" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.3-3.2">The Interconnect controller computes the path from S1 to
              AGN2 and calculates that the optimal path based on IGP metric
              is R1-R2-R3-AGN1-AGN2.<a href="#section-6.3-3.2" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.3-3.3">The Interconnect controller advertises a BGP SR Policy to R1
              with BSID 1002, and a segment-list containing segments
              {R2, R3, AGN1, AGN2}.<a href="#section-6.3-3.3" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.3-3.4">The Interconnect controller advertises a BGP SR Policy to S1
              with BSID 1003, and a segment-list containing segments
              {R1, 1002}. The policy is identified using the tuple
              [headend = S1, color = red, endpoint = AGN2].<a href="#section-6.3-3.4" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.3-3.5">Source S1 learns an VPN-IPv4 route for prefix P1, next-hop
              AGN2. The route has an VPN label of L1, and it carries a color
              extended community with value red.<a href="#section-6.3-3.5" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.3-3.6"> S1 has a valid SR policy [color = red, endpoint = AGN2]
              with segment-list {R1, 1002} and BSID 1003. S1 therefore
              associates the VPN-IPv4 prefix P1 with that policy and programs
              the relevant information into the forwarding path.<a href="#section-6.3-3.6" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.3-3.7">As in the previous example the Interconnect controller also
              learns the VPN-IPv4 route advertised by AGN2 in order to
              correlate the service overlay with the underlying transport
              LSPs, creating or optimizing them as required.<a href="#section-6.3-3.7" class="pilcrow">¶</a>
</li>
        </ul>
</section>
<section id="section-6.4">
        <h3 id="name-vnf-to-vnf-connectivity">
<a href="#section-6.4" class="section-number selfRef">6.4. </a><a href="#name-vnf-to-vnf-connectivity" class="section-name selfRef">VNF to VNF Connectivity</a>
        </h3>
<p id="section-6.4-1">The last example demonstrates VNF to VNF connectivity and
            illustrates the instantiation of a unidirectional TE path from S2
            to D2. The path is consumed by an EVPN service that requires low
            latency as a service requirement and as such uses latency as a path
            computation objective. This service requirement is cataloged and
            represented using the color green.<a href="#section-6.4-1" class="pilcrow">¶</a></p>
<p id="section-6.4-2">In this example S2 is a VNF that has no routing capability. It is
            hosted by hypervisor H1 that in turn has an interface to a DC
            controller through which forwarding instructions are programmed.
            H1 serves as the NVO tunnel endpoint and overlay next-hop.<a href="#section-6.4-2" class="pilcrow">¶</a></p>
<p id="section-6.4-3">D2 is a VNF with partial routing capability that is connected to
            a leaf switch L1. L1 connects to underlay/overlay in data center
            2 and serves as the NVO tunnel endpoint for D2. L1 advertises
            BGP Prefix-SID 9001 into the underlay.<a href="#section-6.4-3" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-6.4-4.1">The relevant details of the EVPN service are entered in the
              data center policy engines within data center 1 and 2.<a href="#section-6.4-4.1" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.4-4.2">Source S2 is turned-up. Hypervisor H1 notifies its parent DC
              controller, which in turn retrieves the service (EVPN)
              information, color, IP and MAC information from the policy
              engine and subsequently programs the associated forwarding
              entries onto S2. The DC controller also dynamically advertises
              an EVPN MAC Advertisement Route for S2's IP and MAC into the
              overlay with next-hop H1. (This would trigger the return path
              set-up between L1 and H2 not covered in this example.)<a href="#section-6.4-4.2" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.4-4.3">The DC controller in data center 1 learns an EVPN MAC
              Advertisement Route for D2, MAC M, next-nop L1. The route has an
              MPLS label of L2, and it carries a color extended community with
              the value green.<a href="#section-6.4-4.3" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.4-4.4">The Interconnect controller computes the path between H1 and L1
              and calculates that the optimal path based on latency is
              R5-R4-R3-AGN1.<a href="#section-6.4-4.4" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.4-4.5">The Interconnect controller advertises a BGP SR Policy to R5
              with BSID 1004, and a segment-list containing segments
              {R4, R3, AGN1}.<a href="#section-6.4-4.5" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.4-4.6">The Interconnect controller advertises a BGP SR Policy to the
              DC controller in data center 1 with BSID 1005 and a segment-list
              containing segments {R5, 1004, 9001}. The policy is identified
              using the tuple [headend = H1, color = green, endpoint = L1].<a href="#section-6.4-4.6" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.4-4.7">The DC controller in data center 1 has a valid SR policy
              [color = green, endpoint = L1] with segment-list {R5, 1004, 9001}
              and BSID 1005. The controller therefore associates the MAC
              Advertisement Route with that policy, and programs the associated
              forwarding rules into S2.<a href="#section-6.4-4.7" class="pilcrow">¶</a>
</li>
          <li class="normal" id="section-6.4-4.8">As in the previous example the Interconnect controller also
              learns the MAC Advertisement Route advertised by D2 in order
              to correlate the service overlay with the underlying transport
              LSPs, creating or optimizing them as required.<a href="#section-6.4-4.8" class="pilcrow">¶</a>
</li>
        </ul>
</section>
</section>
<section id="section-7">
      <h2 id="name-conclusions">
<a href="#section-7" class="section-number selfRef">7. </a><a href="#name-conclusions" class="section-name selfRef">Conclusions</a>
      </h2>
<p id="section-7-1">The NFIX architecture provides an evolutionary path to a unified
         network fabric. It uses the base constructs of seamless-MPLS and
         adds end-to-end LSPs capable of delivering against SLAs, seamless
  data center interconnect, service differentiation, service
         function chaining, and a Layer-2/Layer-3 infrastructure capable of
         interconnecting PNF-to-PNF, PNF-to-VNF, and VNF-to-VNF.<a href="#section-7-1" class="pilcrow">¶</a></p>
<p id="section-7-2">NFIX establishes a dynamic, seamless, and automated connectivity
        model that overcomes the operational barriers and interworking issues
        between data centers and the wide-area network and delivers the
        following using standards-based protocols:<a href="#section-7-2" class="pilcrow">¶</a></p>
<ul class="normal">
<li class="normal" id="section-7-3.1">A unified routing control plane: Multiprotocol BGP (MP-BGP) to
         acquire inter-domain NLRI from the IP/MPLS underlay and the
  virtualized IP-VPN/EVPN service overlay.<a href="#section-7-3.1" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-7-3.2">A unified forwarding control plane: SR provides dynamic service
         tunnels with fast restoration options to meet deterministic
         bandwidth, latency and path diversity constraints. SR utilizes
         the appropriate data path encapsulation for seamless, end-to-end
         connectivity between distributed edge and core data centers across
         the wide-area network.<a href="#section-7-3.2" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-7-3.3">Service Function Chaining: Leverage SFC extensions for BGP and
          segment routing to interconnect network and service functions into
          SFPs, with support for various data path implementations.<a href="#section-7-3.3" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-7-3.4">Service Differentiation: Provide a framework that allows for
          construction of logical end-to-end networks with differentiated
          logical topologies and/or constraints through use of SR policies
          and coloring.<a href="#section-7-3.4" class="pilcrow">¶</a>
</li>
        <li class="normal" id="section-7-3.5">Automation: Facilitates automation of service provisioning and
          avoids heavy service interworking at DCBs.<a href="#section-7-3.5" class="pilcrow">¶</a>
</li>
      </ul>
<p id="section-7-4">NFIX is deployable on existing data center and wide-area network
       infrastructures and allows the underlying data forwarding plane to
       evolve with minimal impact on the services plane.<a href="#section-7-4" class="pilcrow">¶</a></p>
</section>
<section id="section-8">
      <h2 id="name-security-considerations">
<a href="#section-8" class="section-number selfRef">8. </a><a href="#name-security-considerations" class="section-name selfRef">Security Considerations</a>
      </h2>
<p id="section-8-1">The NFIX architecture based on SR-MPLS is subject to the same
         security concerns as any MPLS network. No new protocols are
         introduced, hence security issues of the protocols encompassed by this
         architecture are addressed within the relevant individual standards
         documents.  It is recommended that the security framework for MPLS
         and GMPLS networks defined in <span>[<a href="#RFC5920" class="xref">RFC5920</a>]</span> are adhered to. Although
         [RFC5920] focuses on the use of RSVP-TE and LDP control plane, the
         practices and procedures are extendable to an SR-MPLS domain.<a href="#section-8-1" class="pilcrow">¶</a></p>
<p id="section-8-2">The NFIX architecture makes extensive use of Multiprotocol BGP, and
        it is recommended that the TCP Authentication Option (TCP-AO) <span>[<a href="#RFC5925" class="xref">RFC5925</a>]</span>
        is used to protect the integrity of long-lived BGP sessions and any
        other TCP-based protocols.<a href="#section-8-2" class="pilcrow">¶</a></p>
<p id="section-8-3">Where PCEP is used between controller and path headend the use of
        PCEPS <span>[<a href="#RFC8253" class="xref">RFC8253</a>]</span> is recommended to provide confidentiality to PCEP
        communication using Transport Layer Security (TLS).<a href="#section-8-3" class="pilcrow">¶</a></p>
</section>
<div id="Acknowledgements">
<section id="section-9">
      <h2 id="name-acknowledgements">
<a href="#section-9" class="section-number selfRef">9. </a><a href="#name-acknowledgements" class="section-name selfRef">Acknowledgements</a>
      </h2>
<p id="section-9-1">The authors would like to acknowledge Mustapha Aissaoui, Wim Henderickx, and Gunter Van
   de Velde.<a href="#section-9-1" class="pilcrow">¶</a></p>
</section>
</div>
<section id="section-10">
      <h2 id="name-contributors">
<a href="#section-10" class="section-number selfRef">10. </a><a href="#name-contributors" class="section-name selfRef">Contributors</a>
      </h2>
<p id="section-10-1">The following people contributed to the content of this document and should be considered
   co-authors.<a href="#section-10-1" class="pilcrow">¶</a></p>
<div id="Contributors">
<figure id="figure-6">
        <div class="alignLeft art-text artwork" id="section-10-2.1">
<pre>

        Juan Rodriguez
        Nokia
        United States of America

        Email: juan.rodriguez@nokia.com

        Jorge Rabadan
        Nokia
        United States of America

        Email: jorge.rabadan@nokia.com

        Nick Morris
        Verizon
        United States of America

        Email: nicklous.morris@verizonwireless.com

        Eddie Leyton
        Verizon
        United States of America

        Email: edward.leyton@verizonwireless.com


</pre>
</div>
<figcaption><a href="#figure-6" class="selfRef">Figure 6</a></figcaption></figure>
</div>
</section>
<div id="IANA">
<section id="section-11">
      <h2 id="name-iana-considerations">
<a href="#section-11" class="section-number selfRef">11. </a><a href="#name-iana-considerations" class="section-name selfRef">IANA Considerations</a>
      </h2>
<p id="section-11-1">This memo does not include any requests to IANA for allocation.<a href="#section-11-1" class="pilcrow">¶</a></p>
</section>
</div>
<section id="section-12">
      <h2 id="name-references">
<a href="#section-12" class="section-number selfRef">12. </a><a href="#name-references" class="section-name selfRef">References</a>
      </h2>
<section id="section-12.1">
        <h3 id="name-normative-references">
<a href="#section-12.1" class="section-number selfRef">12.1. </a><a href="#name-normative-references" class="section-name selfRef">Normative References</a>
        </h3>
<dl class="references">
<dt id="RFC2119">[RFC2119]</dt>
        <dd>
<span class="refAuthor">Bradner, S.</span>, <span class="refTitle">"Key words for use in RFCs to Indicate Requirement Levels"</span>, <span class="seriesInfo">BCP 14</span>, <span class="seriesInfo">RFC 2119</span>, <time datetime="1997-03" class="refDate">March 1997</time>, <span>&lt;<a href="http://xml.resource.org/public/rfc/html/rfc2119.html">http://xml.resource.org/public/rfc/html/rfc2119.html</a>&gt;</span>. </dd>
<dd class="break"></dd><dt id="RFC8174">[RFC8174]</dt>
      <dd>
<span class="refAuthor">Leiba, B.</span>, <span class="refTitle">"Ambiguity of Uppercase vs Lowercase in RFC 2119 Key Words"</span>, <span class="seriesInfo">BCP 14</span>, <span class="seriesInfo">RFC 8174</span>, <span class="seriesInfo">DOI 10.17487/RFC8174</span>, <time datetime="2017-05" class="refDate">May 2017</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc8174">https://www.rfc-editor.org/info/rfc8174</a>&gt;</span>. </dd>
<dd class="break"></dd>
</dl>
</section>
<section id="section-12.2">
        <h3 id="name-informative-references">
<a href="#section-12.2" class="section-number selfRef">12.2. </a><a href="#name-informative-references" class="section-name selfRef">Informative References</a>
        </h3>
<dl class="references">
<dt id="I-D.ietf-nvo3-geneve">[I-D.ietf-nvo3-geneve]</dt>
        <dd>
<span class="refAuthor">Gross, J.</span>, <span class="refAuthor">Ganga, I.</span>, and <span class="refAuthor">T. Sridhar</span>, <span class="refTitle">"Geneve: Generic Network Virtualization Encapsulation"</span>, <span class="refContent">Work in Progress</span>, <span class="seriesInfo">Internet-Draft, draft-ietf-nvo3-geneve-16</span>, <time datetime="2020-03-07" class="refDate">7 March 2020</time>, <span>&lt;<a href="https://www.ietf.org/archive/id/draft-ietf-nvo3-geneve-16.txt">https://www.ietf.org/archive/id/draft-ietf-nvo3-geneve-16.txt</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="I-D.ietf-mpls-seamless-mpls">[I-D.ietf-mpls-seamless-mpls]</dt>
        <dd>
<span class="refAuthor">Leymann, N.</span>, <span class="refAuthor">Decraene, B.</span>, <span class="refAuthor">Filsfils, C.</span>, <span class="refAuthor">Konstantynowicz, M.</span>, and <span class="refAuthor">D. Steinberg</span>, <span class="refTitle">"Seamless MPLS Architecture"</span>, <span class="refContent">Work in Progress</span>, <span class="seriesInfo">Internet-Draft, draft-ietf-mpls-seamless-mpls-07</span>, <time datetime="2014-06-28" class="refDate">28 June 2014</time>, <span>&lt;<a href="https://www.ietf.org/archive/id/draft-ietf-mpls-seamless-mpls-07.txt">https://www.ietf.org/archive/id/draft-ietf-mpls-seamless-mpls-07.txt</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="I-D.ietf-bess-evpn-ipvpn-interworking">[I-D.ietf-bess-evpn-ipvpn-interworking]</dt>
        <dd>
<span class="refAuthor">Rabadan, J.</span>, <span class="refAuthor">Sajassi, A.</span>, <span class="refAuthor">Rosen, E.</span>, <span class="refAuthor">Drake, J.</span>, <span class="refAuthor">Lin, W.</span>, <span class="refAuthor">Uttaro, J.</span>, and <span class="refAuthor">A. Simpson</span>, <span class="refTitle">"EVPN Interworking with IPVPN"</span>, <span class="refContent">Work in Progress</span>, <span class="seriesInfo">Internet-Draft, draft-ietf-bess-evpn-ipvpn-interworking-06</span>, <time datetime="2021-09-22" class="refDate">22 September 2021</time>, <span>&lt;<a href="https://www.ietf.org/archive/id/draft-ietf-bess-evpn-ipvpn-interworking-06.txt">https://www.ietf.org/archive/id/draft-ietf-bess-evpn-ipvpn-interworking-06.txt</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="I-D.ietf-spring-segment-routing-policy">[I-D.ietf-spring-segment-routing-policy]</dt>
        <dd>
<span class="refAuthor">Filsfils, C.</span>, <span class="refAuthor">Talaulikar, K.</span>, <span class="refAuthor">Voyer, D.</span>, <span class="refAuthor">Bogdanov, A.</span>, and <span class="refAuthor">P. Mattes</span>, <span class="refTitle">"Segment Routing Policy Architecture"</span>, <span class="refContent">Work in Progress</span>, <span class="seriesInfo">Internet-Draft, draft-ietf-spring-segment-routing-policy-14</span>, <time datetime="2021-10-25" class="refDate">25 October 2021</time>, <span>&lt;<a href="https://www.ietf.org/archive/id/draft-ietf-spring-segment-routing-policy-14.txt">https://www.ietf.org/archive/id/draft-ietf-spring-segment-routing-policy-14.txt</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="I-D.ietf-rtgwg-segment-routing-ti-lfa">[I-D.ietf-rtgwg-segment-routing-ti-lfa]</dt>
        <dd>
<span class="refAuthor">Litkowski, S.</span>, <span class="refAuthor">Bashandy, A.</span>, <span class="refAuthor">Filsfils, C.</span>, <span class="refAuthor">Francois, P.</span>, <span class="refAuthor">Decraene, B.</span>, and <span class="refAuthor">D. Voyer</span>, <span class="refTitle">"Topology Independent Fast Reroute using Segment Routing"</span>, <span class="refContent">Work in Progress</span>, <span class="seriesInfo">Internet-Draft, draft-ietf-rtgwg-segment-routing-ti-lfa-07</span>, <time datetime="2021-06-29" class="refDate">29 June 2021</time>, <span>&lt;<a href="https://www.ietf.org/archive/id/draft-ietf-rtgwg-segment-routing-ti-lfa-07.txt">https://www.ietf.org/archive/id/draft-ietf-rtgwg-segment-routing-ti-lfa-07.txt</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="I-D.ietf-bess-nsh-bgp-control-plane">[I-D.ietf-bess-nsh-bgp-control-plane]</dt>
        <dd>
<span class="refAuthor">Farrel, A.</span>, <span class="refAuthor">Drake, J.</span>, <span class="refAuthor">Rosen, E.</span>, <span class="refAuthor">Uttaro, J.</span>, and <span class="refAuthor">L. Jalil</span>, <span class="refTitle">"BGP Control Plane for the Network Service Header in Service Function Chaining"</span>, <span class="refContent">Work in Progress</span>, <span class="seriesInfo">Internet-Draft, draft-ietf-bess-nsh-bgp-control-plane-18</span>, <time datetime="2020-08-21" class="refDate">21 August 2020</time>, <span>&lt;<a href="https://www.ietf.org/archive/id/draft-ietf-bess-nsh-bgp-control-plane-18.txt">https://www.ietf.org/archive/id/draft-ietf-bess-nsh-bgp-control-plane-18.txt</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="I-D.ietf-idr-te-lsp-distribution">[I-D.ietf-idr-te-lsp-distribution]</dt>
        <dd>
<span class="refAuthor">Previdi, S.</span>, <span class="refAuthor">Talaulikar, K.</span>, <span class="refAuthor">Dong, J.</span>, <span class="refAuthor">Chen, M.</span>, <span class="refAuthor">Gredler, H.</span>, and <span class="refAuthor">J. Tantsura</span>, <span class="refTitle">"Distribution of Traffic Engineering (TE) Policies and State using BGP-LS"</span>, <span class="refContent">Work in Progress</span>, <span class="seriesInfo">Internet-Draft, draft-ietf-idr-te-lsp-distribution-16</span>, <time datetime="2021-10-22" class="refDate">22 October 2021</time>, <span>&lt;<a href="https://www.ietf.org/archive/id/draft-ietf-idr-te-lsp-distribution-16.txt">https://www.ietf.org/archive/id/draft-ietf-idr-te-lsp-distribution-16.txt</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="I-D.barth-pce-segment-routing-policy-cp">[I-D.barth-pce-segment-routing-policy-cp]</dt>
        <dd>
<span class="refAuthor">Koldychev, M.</span>, <span class="refAuthor">Sivabalan, S.</span>, <span class="refAuthor">Barth, C.</span>, <span class="refAuthor">Peng, S.</span>, and <span class="refAuthor">H. Bidgoli</span>, <span class="refTitle">"PCEP extension to support Segment Routing Policy Candidate Paths"</span>, <span class="refContent">Work in Progress</span>, <span class="seriesInfo">Internet-Draft, draft-barth-pce-segment-routing-policy-cp-06</span>, <time datetime="2020-06-02" class="refDate">2 June 2020</time>, <span>&lt;<a href="https://www.ietf.org/archive/id/draft-barth-pce-segment-routing-policy-cp-06.txt">https://www.ietf.org/archive/id/draft-barth-pce-segment-routing-policy-cp-06.txt</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="I-D.filsfils-spring-sr-policy-considerations">[I-D.filsfils-spring-sr-policy-considerations]</dt>
        <dd>
<span class="refAuthor">Filsfils, C.</span>, <span class="refAuthor">Talaulikar, K.</span>, <span class="refAuthor">Krol, P.</span>, <span class="refAuthor">Horneffer, M.</span>, and <span class="refAuthor">P. Mattes</span>, <span class="refTitle">"SR Policy Implementation and Deployment Considerations"</span>, <span class="refContent">Work in Progress</span>, <span class="seriesInfo">Internet-Draft, draft-filsfils-spring-sr-policy-considerations-08</span>, <time datetime="2021-10-22" class="refDate">22 October 2021</time>, <span>&lt;<a href="https://www.ietf.org/archive/id/draft-filsfils-spring-sr-policy-considerations-08.txt">https://www.ietf.org/archive/id/draft-filsfils-spring-sr-policy-considerations-08.txt</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="I-D.ietf-rtgwg-bgp-pic">[I-D.ietf-rtgwg-bgp-pic]</dt>
        <dd>
<span class="refAuthor">Bashandy, A.</span>, <span class="refAuthor">Filsfils, C.</span>, and <span class="refAuthor">P. Mohapatra</span>, <span class="refTitle">"BGP Prefix Independent Convergence"</span>, <span class="refContent">Work in Progress</span>, <span class="seriesInfo">Internet-Draft, draft-ietf-rtgwg-bgp-pic-17</span>, <time datetime="2021-10-12" class="refDate">12 October 2021</time>, <span>&lt;<a href="https://www.ietf.org/archive/id/draft-ietf-rtgwg-bgp-pic-17.txt">https://www.ietf.org/archive/id/draft-ietf-rtgwg-bgp-pic-17.txt</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="I-D.ietf-isis-mpls-elc">[I-D.ietf-isis-mpls-elc]</dt>
        <dd>
<span class="refAuthor">Xu, X.</span>, <span class="refAuthor">Kini, S.</span>, <span class="refAuthor">Psenak, P.</span>, <span class="refAuthor">Filsfils, C.</span>, <span class="refAuthor">Litkowski, S.</span>, and <span class="refAuthor">M. Bocci</span>, <span class="refTitle">"Signaling Entropy Label Capability and Entropy Readable Label Depth Using IS-IS"</span>, <span class="refContent">Work in Progress</span>, <span class="seriesInfo">Internet-Draft, draft-ietf-isis-mpls-elc-13</span>, <time datetime="2020-05-28" class="refDate">28 May 2020</time>, <span>&lt;<a href="https://www.ietf.org/archive/id/draft-ietf-isis-mpls-elc-13.txt">https://www.ietf.org/archive/id/draft-ietf-isis-mpls-elc-13.txt</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="I-D.ietf-ospf-mpls-elc">[I-D.ietf-ospf-mpls-elc]</dt>
        <dd>
<span class="refAuthor">Xu, X.</span>, <span class="refAuthor">Kini, S.</span>, <span class="refAuthor">Psenak, P.</span>, <span class="refAuthor">Filsfils, C.</span>, <span class="refAuthor">Litkowski, S.</span>, and <span class="refAuthor">M. Bocci</span>, <span class="refTitle">"Signaling Entropy Label Capability and Entropy Readable Label Depth Using OSPF"</span>, <span class="refContent">Work in Progress</span>, <span class="seriesInfo">Internet-Draft, draft-ietf-ospf-mpls-elc-15</span>, <time datetime="2020-06-01" class="refDate">1 June 2020</time>, <span>&lt;<a href="https://www.ietf.org/archive/id/draft-ietf-ospf-mpls-elc-15.txt">https://www.ietf.org/archive/id/draft-ietf-ospf-mpls-elc-15.txt</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="I-D.ietf-idr-next-hop-capability">[I-D.ietf-idr-next-hop-capability]</dt>
        <dd>
<span class="refAuthor">Decraene, B.</span>, <span class="refAuthor">Kompella, K.</span>, and <span class="refAuthor">W. Henderickx</span>, <span class="refTitle">"BGP Next-Hop dependent capabilities"</span>, <span class="refContent">Work in Progress</span>, <span class="seriesInfo">Internet-Draft, draft-ietf-idr-next-hop-capability-07</span>, <time datetime="2021-12-08" class="refDate">8 December 2021</time>, <span>&lt;<a href="https://www.ietf.org/archive/id/draft-ietf-idr-next-hop-capability-07.txt">https://www.ietf.org/archive/id/draft-ietf-idr-next-hop-capability-07.txt</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="I-D.ietf-spring-segment-routing-central-epe">[I-D.ietf-spring-segment-routing-central-epe]</dt>
        <dd>
<span class="refAuthor">Filsfils, C.</span>, <span class="refAuthor">Previdi, S.</span>, <span class="refAuthor">Dawra, G.</span>, <span class="refAuthor">Aries, E.</span>, and <span class="refAuthor">D. Afanasiev</span>, <span class="refTitle">"Segment Routing Centralized BGP Egress Peer Engineering"</span>, <span class="refContent">Work in Progress</span>, <span class="seriesInfo">Internet-Draft, draft-ietf-spring-segment-routing-central-epe-10</span>, <time datetime="2017-12-21" class="refDate">21 December 2017</time>, <span>&lt;<a href="https://www.ietf.org/archive/id/draft-ietf-spring-segment-routing-central-epe-10.txt">https://www.ietf.org/archive/id/draft-ietf-spring-segment-routing-central-epe-10.txt</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="I-D.ietf-idr-long-lived-gr">[I-D.ietf-idr-long-lived-gr]</dt>
        <dd>
<span class="refAuthor">Uttaro, J.</span>, <span class="refAuthor">Chen, E.</span>, <span class="refAuthor">Decraene, B.</span>, and <span class="refAuthor">J. G. Scudder</span>, <span class="refTitle">"Support for Long-lived BGP Graceful Restart"</span>, <span class="refContent">Work in Progress</span>, <span class="seriesInfo">Internet-Draft, draft-ietf-idr-long-lived-gr-00</span>, <time datetime="2019-09-05" class="refDate">5 September 2019</time>, <span>&lt;<a href="https://www.ietf.org/archive/id/draft-ietf-idr-long-lived-gr-00.txt">https://www.ietf.org/archive/id/draft-ietf-idr-long-lived-gr-00.txt</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC7938">[RFC7938]</dt>
        <dd>
<span class="refAuthor">Lapukhov, P.</span>, <span class="refAuthor">Premji, A.</span>, and <span class="refAuthor">J. Mitchell, Ed.</span>, <span class="refTitle">"Use of BGP for Routing in Large-Scale Data Centers"</span>, <span class="seriesInfo">RFC 7938</span>, <span class="seriesInfo">DOI 10.17487/RFC7938</span>, <time datetime="2016-08" class="refDate">August 2016</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc7938">https://www.rfc-editor.org/info/rfc7938</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC7752">[RFC7752]</dt>
        <dd>
<span class="refAuthor">Gredler, H., Ed.</span>, <span class="refAuthor">Medved, J.</span>, <span class="refAuthor">Previdi, S.</span>, <span class="refAuthor">Farrel, A.</span>, and <span class="refAuthor">S. Ray</span>, <span class="refTitle">"North-Bound Distribution of Link-State and Traffic Engineering (TE) Information Using BGP"</span>, <span class="seriesInfo">RFC 7752</span>, <span class="seriesInfo">DOI 10.17487/RFC7752</span>, <time datetime="2016-03" class="refDate">March 2016</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc7752">https://www.rfc-editor.org/info/rfc7752</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC8277">[RFC8277]</dt>
        <dd>
<span class="refAuthor">Rosen, E.</span>, <span class="refTitle">"Using BGP to Bind MPLS Labels to Address Prefixes"</span>, <span class="seriesInfo">RFC 8277</span>, <span class="seriesInfo">DOI 10.17487/RFC8277</span>, <time datetime="2017-10" class="refDate">October 2017</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc8277">https://www.rfc-editor.org/info/rfc8277</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC8667">[RFC8667]</dt>
        <dd>
<span class="refAuthor">Previdi, S., Ed.</span>, <span class="refAuthor">Ginsberg, L., Ed.</span>, <span class="refAuthor">Filsfils, C.</span>, <span class="refAuthor">Bashandy, A.</span>, <span class="refAuthor">Gredler, H.</span>, and <span class="refAuthor">B. Decraene</span>, <span class="refTitle">"IS-IS Extensions for Segment Routing"</span>, <span class="seriesInfo">RFC 8667</span>, <span class="seriesInfo">DOI 10.17487/RFC8667</span>, <time datetime="2019-12" class="refDate">December 2019</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc8667">https://www.rfc-editor.org/info/rfc8667</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC8665">[RFC8665]</dt>
        <dd>
<span class="refAuthor">Psenak, P., Ed.</span>, <span class="refAuthor">Previdi, S., Ed.</span>, <span class="refAuthor">Filsfils, C.</span>, <span class="refAuthor">Gredler, H.</span>, <span class="refAuthor">Shakir, R.</span>, <span class="refAuthor">Henderickx, W.</span>, and <span class="refAuthor">J. Tantsura</span>, <span class="refTitle">"OSPF Extensions for Segment Routing"</span>, <span class="seriesInfo">RFC 8665</span>, <span class="seriesInfo">DOI 10.17487/RFC8665</span>, <time datetime="2019-12" class="refDate">December 2019</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc8665">https://www.rfc-editor.org/info/rfc8665</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC8669">[RFC8669]</dt>
        <dd>
<span class="refAuthor">Previdi, S.</span>, <span class="refAuthor">Filsfils, C.</span>, <span class="refAuthor">Lindem, A., Ed.</span>, <span class="refAuthor">Sreekantiah, A.</span>, and <span class="refAuthor">H. Gredler</span>, <span class="refTitle">"Segment Routing Prefix Segment Identifier Extensions for BGP"</span>, <span class="seriesInfo">RFC 8669</span>, <span class="seriesInfo">DOI 10.17487/RFC8669</span>, <time datetime="2019-12" class="refDate">December 2019</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc8669">https://www.rfc-editor.org/info/rfc8669</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC8663">[RFC8663]</dt>
        <dd>
<span class="refAuthor">Xu, X.</span>, <span class="refAuthor">Bryant, S.</span>, <span class="refAuthor">Farrel, A.</span>, <span class="refAuthor">Hassan, S.</span>, <span class="refAuthor">Henderickx, W.</span>, and <span class="refAuthor">Z. Li</span>, <span class="refTitle">"MPLS Segment Routing over IP"</span>, <span class="seriesInfo">RFC 8663</span>, <span class="seriesInfo">DOI 10.17487/RFC8663</span>, <time datetime="2019-12" class="refDate">December 2019</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc8663">https://www.rfc-editor.org/info/rfc8663</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC7911">[RFC7911]</dt>
        <dd>
<span class="refAuthor">Walton, D.</span>, <span class="refAuthor">Retana, A.</span>, <span class="refAuthor">Chen, E.</span>, and <span class="refAuthor">J. Scudder</span>, <span class="refTitle">"Advertisement of Multiple Paths in BGP"</span>, <span class="seriesInfo">RFC 7911</span>, <span class="seriesInfo">DOI 10.17487/RFC7911</span>, <time datetime="2016-07" class="refDate">July 2016</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc7911">https://www.rfc-editor.org/info/rfc7911</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC7880">[RFC7880]</dt>
        <dd>
<span class="refAuthor">Pignataro, C.</span>, <span class="refAuthor">Ward, D.</span>, <span class="refAuthor">Akiya, N.</span>, <span class="refAuthor">Bhatia, M.</span>, and <span class="refAuthor">S. Pallagatti</span>, <span class="refTitle">"Seamless Bidirectional Forwarding Detection (S-BFD)"</span>, <span class="seriesInfo">RFC 7880</span>, <span class="seriesInfo">DOI 10.17487/RFC7880</span>, <time datetime="2016-07" class="refDate">July 2016</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc7880">https://www.rfc-editor.org/info/rfc7880</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC4364">[RFC4364]</dt>
        <dd>
<span class="refAuthor">Rosen, E.</span> and <span class="refAuthor">Y. Rekhter</span>, <span class="refTitle">"BGP/MPLS IP Virtual Private Networks (VPNs)"</span>, <span class="seriesInfo">RFC 4364</span>, <span class="seriesInfo">DOI 10.17487/RFC4364</span>, <time datetime="2006-02" class="refDate">February 2006</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc4364">https://www.rfc-editor.org/info/rfc4364</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC5920">[RFC5920]</dt>
        <dd>
<span class="refAuthor">Fang, L., Ed.</span>, <span class="refTitle">"Security Framework for MPLS and GMPLS Networks"</span>, <span class="seriesInfo">RFC 5920</span>, <span class="seriesInfo">DOI 10.17487/RFC5920</span>, <time datetime="2010-07" class="refDate">July 2010</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc5920">https://www.rfc-editor.org/info/rfc5920</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC7011">[RFC7011]</dt>
        <dd>
<span class="refAuthor">Claise, B., Ed.</span>, <span class="refAuthor">Trammell, B., Ed.</span>, and <span class="refAuthor">P. Aitken</span>, <span class="refTitle">"Specification of the IP Flow Information Export (IPFIX) Protocol for the Exchange of Flow Information"</span>, <span class="seriesInfo">STD 77</span>, <span class="seriesInfo">RFC 7011</span>, <span class="seriesInfo">DOI 10.17487/RFC7011</span>, <time datetime="2013-09" class="refDate">September 2013</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc7011">https://www.rfc-editor.org/info/rfc7011</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC6241">[RFC6241]</dt>
        <dd>
<span class="refAuthor">Enns, R., Ed.</span>, <span class="refAuthor">Bjorklund, M., Ed.</span>, <span class="refAuthor">Schoenwaelder, J., Ed.</span>, and <span class="refAuthor">A. Bierman, Ed.</span>, <span class="refTitle">"Network Configuration Protocol (NETCONF)"</span>, <span class="seriesInfo">RFC 6241</span>, <span class="seriesInfo">DOI 10.17487/RFC6241</span>, <time datetime="2011-06" class="refDate">June 2011</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc6241">https://www.rfc-editor.org/info/rfc6241</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC6020">[RFC6020]</dt>
        <dd>
<span class="refAuthor">Bjorklund, M., Ed.</span>, <span class="refTitle">"YANG - A Data Modeling Language for the Network Configuration Protocol (NETCONF)"</span>, <span class="seriesInfo">RFC 6020</span>, <span class="seriesInfo">DOI 10.17487/RFC6020</span>, <time datetime="2010-10" class="refDate">October 2010</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc6020">https://www.rfc-editor.org/info/rfc6020</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC7854">[RFC7854]</dt>
        <dd>
<span class="refAuthor">Scudder, J., Ed.</span>, <span class="refAuthor">Fernando, R.</span>, and <span class="refAuthor">S. Stuart</span>, <span class="refTitle">"BGP Monitoring Protocol (BMP)"</span>, <span class="seriesInfo">RFC 7854</span>, <span class="seriesInfo">DOI 10.17487/RFC7854</span>, <time datetime="2016-06" class="refDate">June 2016</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc7854">https://www.rfc-editor.org/info/rfc7854</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC8300">[RFC8300]</dt>
        <dd>
<span class="refAuthor">Quinn, P., Ed.</span>, <span class="refAuthor">Elzur, U., Ed.</span>, and <span class="refAuthor">C. Pignataro, Ed.</span>, <span class="refTitle">"Network Service Header (NSH)"</span>, <span class="seriesInfo">RFC 8300</span>, <span class="seriesInfo">DOI 10.17487/RFC8300</span>, <time datetime="2018-01" class="refDate">January 2018</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc8300">https://www.rfc-editor.org/info/rfc8300</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC5440">[RFC5440]</dt>
        <dd>
<span class="refAuthor">Vasseur, JP., Ed.</span> and <span class="refAuthor">JL. Le Roux, Ed.</span>, <span class="refTitle">"Path Computation Element (PCE) Communication Protocol (PCEP)"</span>, <span class="seriesInfo">RFC 5440</span>, <span class="seriesInfo">DOI 10.17487/RFC5440</span>, <time datetime="2009-03" class="refDate">March 2009</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc5440">https://www.rfc-editor.org/info/rfc5440</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC7348">[RFC7348]</dt>
        <dd>
<span class="refAuthor">Mahalingam, M.</span>, <span class="refAuthor">Dutt, D.</span>, <span class="refAuthor">Duda, K.</span>, <span class="refAuthor">Agarwal, P.</span>, <span class="refAuthor">Kreeger, L.</span>, <span class="refAuthor">Sridhar, T.</span>, <span class="refAuthor">Bursell, M.</span>, and <span class="refAuthor">C. Wright</span>, <span class="refTitle">"Virtual eXtensible Local Area Network (VXLAN): A Framework for Overlaying Virtualized Layer 2 Networks over Layer 3 Networks"</span>, <span class="seriesInfo">RFC 7348</span>, <span class="seriesInfo">DOI 10.17487/RFC7348</span>, <time datetime="2014-08" class="refDate">August 2014</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc7348">https://www.rfc-editor.org/info/rfc7348</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC7637">[RFC7637]</dt>
        <dd>
<span class="refAuthor">Garg, P., Ed.</span> and <span class="refAuthor">Y. Wang, Ed.</span>, <span class="refTitle">"NVGRE: Network Virtualization Using Generic Routing Encapsulation"</span>, <span class="seriesInfo">RFC 7637</span>, <span class="seriesInfo">DOI 10.17487/RFC7637</span>, <time datetime="2015-09" class="refDate">September 2015</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc7637">https://www.rfc-editor.org/info/rfc7637</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC3031">[RFC3031]</dt>
        <dd>
<span class="refAuthor">Rosen, E.</span>, <span class="refAuthor">Viswanathan, A.</span>, and <span class="refAuthor">R. Callon</span>, <span class="refTitle">"Multiprotocol Label Switching Architecture"</span>, <span class="seriesInfo">RFC 3031</span>, <span class="seriesInfo">DOI 10.17487/RFC3031</span>, <time datetime="2001-01" class="refDate">January 2001</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc3031">https://www.rfc-editor.org/info/rfc3031</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC8014">[RFC8014]</dt>
        <dd>
<span class="refAuthor">Black, D.</span>, <span class="refAuthor">Hudson, J.</span>, <span class="refAuthor">Kreeger, L.</span>, <span class="refAuthor">Lasserre, M.</span>, and <span class="refAuthor">T. Narten</span>, <span class="refTitle">"An Architecture for Data-Center Network Virtualization over Layer 3 (NVO3)"</span>, <span class="seriesInfo">RFC 8014</span>, <span class="seriesInfo">DOI 10.17487/RFC8014</span>, <time datetime="2016-12" class="refDate">December 2016</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc8014">https://www.rfc-editor.org/info/rfc8014</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC8402">[RFC8402]</dt>
        <dd>
<span class="refAuthor">Filsfils, C., Ed.</span>, <span class="refAuthor">Previdi, S., Ed.</span>, <span class="refAuthor">Ginsberg, L.</span>, <span class="refAuthor">Decraene, B.</span>, <span class="refAuthor">Litkowski, S.</span>, and <span class="refAuthor">R. Shakir</span>, <span class="refTitle">"Segment Routing Architecture"</span>, <span class="seriesInfo">RFC 8402</span>, <span class="seriesInfo">DOI 10.17487/RFC8402</span>, <time datetime="2018-07" class="refDate">July 2018</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc8402">https://www.rfc-editor.org/info/rfc8402</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC5883">[RFC5883]</dt>
        <dd>
<span class="refAuthor">Katz, D.</span> and <span class="refAuthor">D. Ward</span>, <span class="refTitle">"Bidirectional Forwarding Detection (BFD) for Multihop Paths"</span>, <span class="seriesInfo">RFC 5883</span>, <span class="seriesInfo">DOI 10.17487/RFC5883</span>, <time datetime="2010-06" class="refDate">June 2010</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc5883">https://www.rfc-editor.org/info/rfc5883</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC8231">[RFC8231]</dt>
        <dd>
<span class="refAuthor">Crabbe, E.</span>, <span class="refAuthor">Minei, I.</span>, <span class="refAuthor">Medved, J.</span>, and <span class="refAuthor">R. Varga</span>, <span class="refTitle">"Path Computation Element Communication Protocol (PCEP) Extensions for Stateful PCE"</span>, <span class="seriesInfo">RFC 8231</span>, <span class="seriesInfo">DOI 10.17487/RFC8231</span>, <time datetime="2017-09" class="refDate">September 2017</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc8231">https://www.rfc-editor.org/info/rfc8231</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC8281">[RFC8281]</dt>
        <dd>
<span class="refAuthor">Crabbe, E.</span>, <span class="refAuthor">Minei, I.</span>, <span class="refAuthor">Sivabalan, S.</span>, and <span class="refAuthor">R. Varga</span>, <span class="refTitle">"Path Computation Element Communication Protocol (PCEP) Extensions for PCE-Initiated LSP Setup in a Stateful PCE Model"</span>, <span class="seriesInfo">RFC 8281</span>, <span class="seriesInfo">DOI 10.17487/RFC8281</span>, <time datetime="2017-12" class="refDate">December 2017</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc8281">https://www.rfc-editor.org/info/rfc8281</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC5925">[RFC5925]</dt>
        <dd>
<span class="refAuthor">Touch, J.</span>, <span class="refAuthor">Mankin, A.</span>, and <span class="refAuthor">R. Bonica</span>, <span class="refTitle">"The TCP Authentication Option"</span>, <span class="seriesInfo">RFC 5925</span>, <span class="seriesInfo">DOI 10.17487/RFC5925</span>, <time datetime="2010-06" class="refDate">June 2010</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc5925">https://www.rfc-editor.org/info/rfc5925</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC8253">[RFC8253]</dt>
        <dd>
<span class="refAuthor">Lopez, D.</span>, <span class="refAuthor">Gonzalez de Dios, O.</span>, <span class="refAuthor">Wu, Q.</span>, and <span class="refAuthor">D. Dhody</span>, <span class="refTitle">"PCEPS: Usage of TLS to Provide a Secure Transport for the Path Computation Element Communication Protocol (PCEP)"</span>, <span class="seriesInfo">RFC 8253</span>, <span class="seriesInfo">DOI 10.17487/RFC8253</span>, <time datetime="2017-10" class="refDate">October 2017</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc8253">https://www.rfc-editor.org/info/rfc8253</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC6790">[RFC6790]</dt>
        <dd>
<span class="refAuthor">Kompella, K.</span>, <span class="refAuthor">Drake, J.</span>, <span class="refAuthor">Amante, S.</span>, <span class="refAuthor">Henderickx, W.</span>, and <span class="refAuthor">L. Yong</span>, <span class="refTitle">"The Use of Entropy Labels in MPLS Forwarding"</span>, <span class="seriesInfo">RFC 6790</span>, <span class="seriesInfo">DOI 10.17487/RFC6790</span>, <time datetime="2012-11" class="refDate">November 2012</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc6790">https://www.rfc-editor.org/info/rfc6790</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC8662">[RFC8662]</dt>
        <dd>
<span class="refAuthor">Kini, S.</span>, <span class="refAuthor">Kompella, K.</span>, <span class="refAuthor">Sivabalan, S.</span>, <span class="refAuthor">Litkowski, S.</span>, <span class="refAuthor">Shakir, R.</span>, and <span class="refAuthor">J. Tantsura</span>, <span class="refTitle">"Entropy Label for Source Packet Routing in Networking (SPRING) Tunnels"</span>, <span class="seriesInfo">RFC 8662</span>, <span class="seriesInfo">DOI 10.17487/RFC8662</span>, <time datetime="2019-12" class="refDate">December 2019</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc8662">https://www.rfc-editor.org/info/rfc8662</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC8491">[RFC8491]</dt>
        <dd>
<span class="refAuthor">Tantsura, J.</span>, <span class="refAuthor">Chunduri, U.</span>, <span class="refAuthor">Aldrin, S.</span>, and <span class="refAuthor">L. Ginsberg</span>, <span class="refTitle">"Signaling Maximum SID Depth (MSD) Using IS-IS"</span>, <span class="seriesInfo">RFC 8491</span>, <span class="seriesInfo">DOI 10.17487/RFC8491</span>, <time datetime="2018-11" class="refDate">November 2018</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc8491">https://www.rfc-editor.org/info/rfc8491</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC8476">[RFC8476]</dt>
      <dd>
<span class="refAuthor">Tantsura, J.</span>, <span class="refAuthor">Chunduri, U.</span>, <span class="refAuthor">Aldrin, S.</span>, and <span class="refAuthor">P. Psenak</span>, <span class="refTitle">"Signaling Maximum SID Depth (MSD) Using OSPF"</span>, <span class="seriesInfo">RFC 8476</span>, <span class="seriesInfo">DOI 10.17487/RFC8476</span>, <time datetime="2018-12" class="refDate">December 2018</time>, <span>&lt;<a href="https://www.rfc-editor.org/info/rfc8476">https://www.rfc-editor.org/info/rfc8476</a>&gt;</span>. </dd>
<dd class="break"></dd>
</dl>
</section>
</section>
<div id="authors-addresses">
<section id="appendix-A">
      <h2 id="name-authors-addresses">
<a href="#name-authors-addresses" class="section-name selfRef">Authors' Addresses</a>
      </h2>
<address class="vcard">
        <div dir="auto" class="left"><span class="fn nameRole">Colin Bookham (<span class="role">editor</span>)</span></div>
<div dir="auto" class="left"><span class="org">Nokia</span></div>
<div dir="auto" class="left"><span class="street-address">740 Waterside Drive</span></div>
<div dir="auto" class="left"><span class="locality">Almondsbury, Bristol</span></div>
<div dir="auto" class="left"><span class="country-name">United Kingdom</span></div>
<div class="email">
<span>Email:</span>
<a href="mailto:colin.bookham@nokia.com" class="email">colin.bookham@nokia.com</a>
</div>
</address>
<address class="vcard">
        <div dir="auto" class="left"><span class="fn nameRole">Andrew Stone</span></div>
<div dir="auto" class="left"><span class="org">Nokia</span></div>
<div dir="auto" class="left"><span class="street-address">600 March Road</span></div>
<div dir="auto" class="left">
<span class="locality">Kanata, Ontario</span>  </div>
<div dir="auto" class="left"><span class="country-name">Canada</span></div>
<div class="email">
<span>Email:</span>
<a href="mailto:andrew.stone@nokia.com" class="email">andrew.stone@nokia.com</a>
</div>
</address>
<address class="vcard">
        <div dir="auto" class="left"><span class="fn nameRole">Jeff Tantsura</span></div>
<div dir="auto" class="left"><span class="org">Microsoft</span></div>
<div class="email">
<span>Email:</span>
<a href="mailto:jefftant.ietf@gmail.com" class="email">jefftant.ietf@gmail.com</a>
</div>
</address>
<address class="vcard">
        <div dir="auto" class="left"><span class="fn nameRole">Muhammad Durrani</span></div>
<div dir="auto" class="left"><span class="org">Equinix Inc</span></div>
<div dir="auto" class="left"><span class="street-address">1188 Arques Ave</span></div>
<div dir="auto" class="left">
<span class="locality">Sunnyvale CA</span>,  </div>
<div dir="auto" class="left"><span class="country-name">United States of America</span></div>
<div class="email">
<span>Email:</span>
<a href="mailto:mdurrani@equinix.com" class="email">mdurrani@equinix.com</a>
</div>
</address>
<address class="vcard">
        <div dir="auto" class="left"><span class="fn nameRole">Bruno Decraene</span></div>
<div dir="auto" class="left"><span class="org">Orange</span></div>
<div dir="auto" class="left"><span class="street-address">38-40 Rue de General Leclerc</span></div>
<div dir="auto" class="left"><span class="locality">92794 Issey Moulineaux cedex 9</span></div>
<div dir="auto" class="left"><span class="country-name">France</span></div>
<div class="email">
<span>Email:</span>
<a href="mailto:bruno.decraene@orange.com" class="email">bruno.decraene@orange.com</a>
</div>
</address>
</section>
</div>
<script>const toc = document.getElementById("toc");
toc.querySelector("h2").addEventListener("click", e => {
  toc.classList.toggle("active");
});
toc.querySelector("nav").addEventListener("click", e => {
  toc.classList.remove("active");
});
</script>
</body>
</html>
