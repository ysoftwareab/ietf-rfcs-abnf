<!DOCTYPE html>
<html lang="en" class="Internet-Draft">
<head>
<meta charset="utf-8">
<meta content="Common,Latin" name="scripts">
<meta content="initial-scale=1.0" name="viewport">
<title>Reinforcement Learning-Based Virtual Network Embedding: Problem Statement</title>
<meta content="Ihsan Ullah" name="author">
<meta content="Youn-Hee Han" name="author">
<meta content="TaeYeon Kim" name="author">
<meta content="
        In Network virtualization (NV) technology, Virtual Network Embedding (VNE) is a problem to map a virtual network to the substrate network.
                It has a great impact on the performance of virtual network and resource utilization of the substrate network.
                An efficient embedding strategy can maximize the acceptance ratio of virtual networks to increase the revenue for Internet service provider.
                Several works have been appeared on the design of VNE solutions, however, it has becomes a challenging issues for researchers.
                To solved the VNE problem, reinforcement learning (RL) can play a vital role to make the VNE problem more intelligent and efficient.
                Moreover, RL has been merged with deep learning techniques to develop adaptive models with effective strategies for various complex problems.
                In RL, agents can learn desired behaviors (e.g, optimal VNE strategies), and after learning and completing training, it can embed the virtual network to the subtract network very quickly and efficiently.
                RL can reduce the complexity of the VNE method, however, it is too difficult to apply RL techniques directly to VNE problems and need more research study.
                In this document, we are presenting a problem statement to motivate the research community to solve the VNE problem using reinforcement learning.

       
    " name="description">
<meta content="xml2rfc 3.8.0" name="generator">
<meta content="virtual network embedding, machine learning" name="keyword">
<meta content="draft-ihsan-nmrg-rl-vne-ps-00" name="ietf.draft">
<!-- Generator version information:
  xml2rfc 3.8.0
    Python 3.6.12
    appdirs 1.4.4
    ConfigArgParse 1.4.1
    google-i18n-address 2.4.0
    html5lib 1.1
    intervaltree 3.1.0
    Jinja2 2.11.3
    kitchen 1.2.6
    lxml 4.6.3
    pycountry 20.7.3
    pyflakes 2.3.1
    PyYAML 5.4.1
    requests 2.25.1
    setuptools 57.0.0
    six 1.16.0
-->
<link href="/tmp/draft-ihsan-nmrg-rl-vne-ps-00-j_wnyp2l.xml" rel="alternate" type="application/rfc+xml">
<link href="#copyright" rel="license">
<style type="text/css">/*

  NOTE: Changes at the bottom of this file overrides some earlier settings.

  Once the style has stabilized and has been adopted as an official RFC style,
  this can be consolidated so that style settings occur only in one place, but
  for now the contents of this file consists first of the initial CSS work as
  provided to the RFC Formatter (xml2rfc) work, followed by itemized and
  commented changes found necssary during the development of the v3
  formatters.

*/

/* fonts */
@import url('https://fonts.googleapis.com/css?family=Noto+Sans'); /* Sans-serif */
@import url('https://fonts.googleapis.com/css?family=Noto+Serif'); /* Serif (print) */
@import url('https://fonts.googleapis.com/css?family=Roboto+Mono'); /* Monospace */

@viewport {
  zoom: 1.0;
  width: extend-to-zoom;
}
@-ms-viewport {
  width: extend-to-zoom;
  zoom: 1.0;
}
/* general and mobile first */
html {
}
body {
  max-width: 90%;
  margin: 1.5em auto;
  color: #222;
  background-color: #fff;
  font-size: 14px;
  font-family: 'Noto Sans', Arial, Helvetica, sans-serif;
  line-height: 1.6;
  scroll-behavior: smooth;
}
.ears {
  display: none;
}

/* headings */
#title, h1, h2, h3, h4, h5, h6 {
  margin: 1em 0 0.5em;
  font-weight: bold;
  line-height: 1.3;
}
#title {
  clear: both;
  border-bottom: 1px solid #ddd;
  margin: 0 0 0.5em 0;
  padding: 1em 0 0.5em;
}
.author {
  padding-bottom: 4px;
}
h1 {
  font-size: 26px;
  margin: 1em 0;
}
h2 {
  font-size: 22px;
  margin-top: -20px;  /* provide offset for in-page anchors */
  padding-top: 33px;
}
h3 {
  font-size: 18px;
  margin-top: -36px;  /* provide offset for in-page anchors */
  padding-top: 42px;
}
h4 {
  font-size: 16px;
  margin-top: -36px;  /* provide offset for in-page anchors */
  padding-top: 42px;
}
h5, h6 {
  font-size: 14px;
}
#n-copyright-notice {
  border-bottom: 1px solid #ddd;
  padding-bottom: 1em;
  margin-bottom: 1em;
}
/* general structure */
p {
  padding: 0;
  margin: 0 0 1em 0;
  text-align: left;
}
div, span {
  position: relative;
}
div {
  margin: 0;
}
.alignRight.art-text {
  background-color: #f9f9f9;
  border: 1px solid #eee;
  border-radius: 3px;
  padding: 1em 1em 0;
  margin-bottom: 1.5em;
}
.alignRight.art-text pre {
  padding: 0;
}
.alignRight {
  margin: 1em 0;
}
.alignRight > *:first-child {
  border: none;
  margin: 0;
  float: right;
  clear: both;
}
.alignRight > *:nth-child(2) {
  clear: both;
  display: block;
  border: none;
}
svg {
  display: block;
}
.alignCenter.art-text {
  background-color: #f9f9f9;
  border: 1px solid #eee;
  border-radius: 3px;
  padding: 1em 1em 0;
  margin-bottom: 1.5em;
}
.alignCenter.art-text pre {
  padding: 0;
}
.alignCenter {
  margin: 1em 0;
}
.alignCenter > *:first-child {
  border: none;
  /* this isn't optimal, but it's an existence proof.  PrinceXML doesn't
     support flexbox yet.
  */
  display: table;
  margin: 0 auto;
}

/* lists */
ol, ul {
  padding: 0;
  margin: 0 0 1em 2em;
}
ol ol, ul ul, ol ul, ul ol {
  margin-left: 1em;
}
li {
  margin: 0 0 0.25em 0;
}
.ulCompact li {
  margin: 0;
}
ul.empty, .ulEmpty {
  list-style-type: none;
}
ul.empty li, .ulEmpty li {
  margin-top: 0.5em;
}
ul.compact, .ulCompact,
ol.compact, .olCompact {
  line-height: 100%;
  margin: 0 0 0 2em;
}

/* definition lists */
dl {
}
dl > dt {
  float: left;
  margin-right: 1em;
}
/* 
dl.nohang > dt {
  float: none;
}
*/
dl > dd {
  margin-bottom: .8em;
  min-height: 1.3em;
}
dl.compact > dd, .dlCompact > dd {
  margin-bottom: 0em;
}
dl > dd > dl {
  margin-top: 0.5em;
  margin-bottom: 0em;
}

/* links */
a {
  text-decoration: none;
}
a[href] {
  color: #22e; /* Arlen: WCAG 2019 */
}
a[href]:hover {
  background-color: #f2f2f2;
}
figcaption a[href],
a[href].selfRef {
  color: #222;
}
/* XXX probably not this:
a.selfRef:hover {
  background-color: transparent;
  cursor: default;
} */

/* Figures */
tt, code, pre, code {
  background-color: #f9f9f9;
  font-family: 'Roboto Mono', monospace;
}
pre {
  border: 1px solid #eee;
  margin: 0;
  padding: 1em;
}
img {
  max-width: 100%;
}
figure {
  margin: 0;
}
figure blockquote {
  margin: 0.8em 0.4em 0.4em;
}
figcaption {
  font-style: italic;
  margin: 0 0 1em 0;
}
@media screen {
  pre {
    overflow-x: auto;
    max-width: 100%;
    max-width: calc(100% - 22px);
  }
}

/* aside, blockquote */
aside, blockquote {
  margin-left: 0;
  padding: 1.2em 2em;
}
blockquote {
  background-color: #f9f9f9;
  color: #111; /* Arlen: WCAG 2019 */
  border: 1px solid #ddd;
  border-radius: 3px;
  margin: 1em 0;
}
cite {
  display: block;
  text-align: right;
  font-style: italic;
}

/* tables */
table {
  width: 100%;
  margin: 0 0 1em;
  border-collapse: collapse;
  border: 1px solid #eee;
}
th, td {
  text-align: left;
  vertical-align: top;
  padding: 0.5em 0.75em;
}
th {
  text-align: left;
  background-color: #e9e9e9;
}
tr:nth-child(2n+1) > td {
  background-color: #f5f5f5;
}
table caption {
  font-style: italic;
  margin: 0;
  padding: 0;
  text-align: left;
}
table p {
  /* XXX to avoid bottom margin on table row signifiers. If paragraphs should
     be allowed within tables more generally, it would be far better to select on a class. */
  margin: 0;
}

/* pilcrow */
a.pilcrow {
  color: #666; /* Arlen: AHDJ 2019 */
  text-decoration: none;
  visibility: hidden;
  user-select: none;
  -ms-user-select: none;
  -o-user-select:none;
  -moz-user-select: none;
  -khtml-user-select: none;
  -webkit-user-select: none;
  -webkit-touch-callout: none;
}
@media screen {
  aside:hover > a.pilcrow,
  p:hover > a.pilcrow,
  blockquote:hover > a.pilcrow,
  div:hover > a.pilcrow,
  li:hover > a.pilcrow,
  pre:hover > a.pilcrow {
    visibility: visible;
  }
  a.pilcrow:hover {
    background-color: transparent;
  }
}

/* misc */
hr {
  border: 0;
  border-top: 1px solid #eee;
}
.bcp14 {
  font-variant: small-caps;
}

.role {
  font-variant: all-small-caps;
}

/* info block */
#identifiers {
  margin: 0;
  font-size: 0.9em;
}
#identifiers dt {
  width: 3em;
  clear: left;
}
#identifiers dd {
  float: left;
  margin-bottom: 0;
}
#identifiers .authors .author {
  display: inline-block;
  margin-right: 1.5em;
}
#identifiers .authors .org {
  font-style: italic;
}

/* The prepared/rendered info at the very bottom of the page */
.docInfo {
  color: #666; /* Arlen: WCAG 2019 */
  font-size: 0.9em;
  font-style: italic;
  margin-top: 2em;
}
.docInfo .prepared {
  float: left;
}
.docInfo .prepared {
  float: right;
}

/* table of contents */
#toc  {
  padding: 0.75em 0 2em 0;
  margin-bottom: 1em;
}
nav.toc ul {
  margin: 0 0.5em 0 0;
  padding: 0;
  list-style: none;
}
nav.toc li {
  line-height: 1.3em;
  margin: 0.75em 0;
  padding-left: 1.2em;
  text-indent: -1.2em;
}
/* references */
.references dt {
  text-align: right;
  font-weight: bold;
  min-width: 7em;
}
.references dd {
  margin-left: 8em;
  overflow: auto;
}

.refInstance {
  margin-bottom: 1.25em;
}

.references .ascii {
  margin-bottom: 0.25em;
}

/* index */
.index ul {
  margin: 0 0 0 1em;
  padding: 0;
  list-style: none;
}
.index ul ul {
  margin: 0;
}
.index li {
  margin: 0;
  text-indent: -2em;
  padding-left: 2em;
  padding-bottom: 5px;
}
.indexIndex {
  margin: 0.5em 0 1em;
}
.index a {
  font-weight: 700;
}
/* make the index two-column on all but the smallest screens */
@media (min-width: 600px) {
  .index ul {
    -moz-column-count: 2;
    -moz-column-gap: 20px;
  }
  .index ul ul {
    -moz-column-count: 1;
    -moz-column-gap: 0;
  }
}

/* authors */
address.vcard {
  font-style: normal;
  margin: 1em 0;
}

address.vcard .nameRole {
  font-weight: 700;
  margin-left: 0;
}
address.vcard .label {
  font-family: "Noto Sans",Arial,Helvetica,sans-serif;
  margin: 0.5em 0;
}
address.vcard .type {
  display: none;
}
.alternative-contact {
  margin: 1.5em 0 1em;
}
hr.addr {
  border-top: 1px dashed;
  margin: 0;
  color: #ddd;
  max-width: calc(100% - 16px);
}

/* temporary notes */
.rfcEditorRemove::before {
  position: absolute;
  top: 0.2em;
  right: 0.2em;
  padding: 0.2em;
  content: "The RFC Editor will remove this note";
  color: #9e2a00; /* Arlen: WCAG 2019 */
  background-color: #ffd; /* Arlen: WCAG 2019 */
}
.rfcEditorRemove {
  position: relative;
  padding-top: 1.8em;
  background-color: #ffd; /* Arlen: WCAG 2019 */
  border-radius: 3px;
}
.cref {
  background-color: #ffd; /* Arlen: WCAG 2019 */
  padding: 2px 4px;
}
.crefSource {
  font-style: italic;
}
/* alternative layout for smaller screens */
@media screen and (max-width: 1023px) {
  body {
    padding-top: 2em;
  }
  #title {
    padding: 1em 0;
  }
  h1 {
    font-size: 24px;
  }
  h2 {
    font-size: 20px;
    margin-top: -18px;  /* provide offset for in-page anchors */
    padding-top: 38px;
  }
  #identifiers dd {
    max-width: 60%;
  }
  #toc {
    position: fixed;
    z-index: 2;
    top: 0;
    right: 0;
    padding: 0;
    margin: 0;
    background-color: inherit;
    border-bottom: 1px solid #ccc;
  }
  #toc h2 {
    margin: -1px 0 0 0;
    padding: 4px 0 4px 6px;
    padding-right: 1em;
    min-width: 190px;
    font-size: 1.1em;
    text-align: right;
    background-color: #444;
    color: white;
    cursor: pointer;
  }
  #toc h2::before { /* css hamburger */
    float: right;
    position: relative;
    width: 1em;
    height: 1px;
    left: -164px;
    margin: 6px 0 0 0;
    background: white none repeat scroll 0 0;
    box-shadow: 0 4px 0 0 white, 0 8px 0 0 white;
    content: "";
  }
  #toc nav {
    display: none;
    padding: 0.5em 1em 1em;
    overflow: auto;
    height: calc(100vh - 48px);
    border-left: 1px solid #ddd;
  }
}

/* alternative layout for wide screens */
@media screen and (min-width: 1024px) {
  body {
    max-width: 724px;
    margin: 42px auto;
    padding-left: 1.5em;
    padding-right: 29em;
  }
  #toc {
    position: fixed;
    top: 42px;
    right: 42px;
    width: 25%;
    margin: 0;
    padding: 0 1em;
    z-index: 1;
  }
  #toc h2 {
    border-top: none;
    border-bottom: 1px solid #ddd;
    font-size: 1em;
    font-weight: normal;
    margin: 0;
    padding: 0.25em 1em 1em 0;
  }
  #toc nav {
    display: block;
    height: calc(90vh - 84px);
    bottom: 0;
    padding: 0.5em 0 0;
    overflow: auto;
  }
  img { /* future proofing */
    max-width: 100%;
    height: auto;
  }
}

/* pagination */
@media print {
  body {

    width: 100%;
  }
  p {
    orphans: 3;
    widows: 3;
  }
  #n-copyright-notice {
    border-bottom: none;
  }
  #toc, #n-introduction {
    page-break-before: always;
  }
  #toc {
    border-top: none;
    padding-top: 0;
  }
  figure, pre {
    page-break-inside: avoid;
  }
  figure {
    overflow: scroll;
  }
  h1, h2, h3, h4, h5, h6 {
    page-break-after: avoid;
  }
  h2+*, h3+*, h4+*, h5+*, h6+* {
    page-break-before: avoid;
  }
  pre {
    white-space: pre-wrap;
    word-wrap: break-word;
    font-size: 10pt;
  }
  table {
    border: 1px solid #ddd;
  }
  td {
    border-top: 1px solid #ddd;
  }
}

/* This is commented out here, as the string-set: doesn't
   pass W3C validation currently */
/*
.ears thead .left {
  string-set: ears-top-left content();
}

.ears thead .center {
  string-set: ears-top-center content();
}

.ears thead .right {
  string-set: ears-top-right content();
}

.ears tfoot .left {
  string-set: ears-bottom-left content();
}

.ears tfoot .center {
  string-set: ears-bottom-center content();
}

.ears tfoot .right {
  string-set: ears-bottom-right content();
}
*/

@page :first {
  padding-top: 0;
  @top-left {
    content: normal;
    border: none;
  }
  @top-center {
    content: normal;
    border: none;
  }
  @top-right {
    content: normal;
    border: none;
  }
}

@page {
  size: A4;
  margin-bottom: 45mm;
  padding-top: 20px;
  /* The follwing is commented out here, but set appropriately by in code, as
     the content depends on the document */
  /*
  @top-left {
    content: 'Internet-Draft';
    vertical-align: bottom;
    border-bottom: solid 1px #ccc;
  }
  @top-left {
    content: string(ears-top-left);
    vertical-align: bottom;
    border-bottom: solid 1px #ccc;
  }
  @top-center {
    content: string(ears-top-center);
    vertical-align: bottom;
    border-bottom: solid 1px #ccc;
  }
  @top-right {
    content: string(ears-top-right);
    vertical-align: bottom;
    border-bottom: solid 1px #ccc;
  }
  @bottom-left {
    content: string(ears-bottom-left);
    vertical-align: top;
    border-top: solid 1px #ccc;
  }
  @bottom-center {
    content: string(ears-bottom-center);
    vertical-align: top;
    border-top: solid 1px #ccc;
  }
  @bottom-right {
      content: '[Page ' counter(page) ']';
      vertical-align: top;
      border-top: solid 1px #ccc;
  }
  */

}

/* Changes introduced to fix issues found during implementation */
/* Make sure links are clickable even if overlapped by following H* */
a {
  z-index: 2;
}
/* Separate body from document info even without intervening H1 */
section {
  clear: both;
}


/* Top align author divs, to avoid names without organization dropping level with org names */
.author {
  vertical-align: top;
}

/* Leave room in document info to show Internet-Draft on one line */
#identifiers dt {
  width: 8em;
}

/* Don't waste quite as much whitespace between label and value in doc info */
#identifiers dd {
  margin-left: 1em;
}

/* Give floating toc a background color (needed when it's a div inside section */
#toc {
  background-color: white;
}

/* Make the collapsed ToC header render white on gray also when it's a link */
@media screen and (max-width: 1023px) {
  #toc h2 a,
  #toc h2 a:link,
  #toc h2 a:focus,
  #toc h2 a:hover,
  #toc a.toplink,
  #toc a.toplink:hover {
    color: white;
    background-color: #444;
    text-decoration: none;
  }
}

/* Give the bottom of the ToC some whitespace */
@media screen and (min-width: 1024px) {
  #toc {
    padding: 0 0 1em 1em;
  }
}

/* Style section numbers with more space between number and title */
.section-number {
  padding-right: 0.5em;
}

/* prevent monospace from becoming overly large */
tt, code, pre, code {
  font-size: 95%;
}

/* Fix the height/width aspect for ascii art*/
pre.sourcecode,
.art-text pre {
  line-height: 1.12;
}


/* Add styling for a link in the ToC that points to the top of the document */
a.toplink {
  float: right;
  margin-right: 0.5em;
}

/* Fix the dl styling to match the RFC 7992 attributes */
dl > dt,
dl.dlParallel > dt {
  float: left;
  margin-right: 1em;
}
dl.dlNewline > dt {
  float: none;
}

/* Provide styling for table cell text alignment */
table td.text-left,
table th.text-left {
  text-align: left;
}
table td.text-center,
table th.text-center {
  text-align: center;
}
table td.text-right,
table th.text-right {
  text-align: right;
}

/* Make the alternative author contact informatio look less like just another
   author, and group it closer with the primary author contact information */
.alternative-contact {
  margin: 0.5em 0 0.25em 0;
}
address .non-ascii {
  margin: 0 0 0 2em;
}

/* With it being possible to set tables with alignment
  left, center, and right, { width: 100%; } does not make sense */
table {
  width: auto;
}

/* Avoid reference text that sits in a block with very wide left margin,
   because of a long floating dt label.*/
.references dd {
  overflow: visible;
}

/* Control caption placement */
caption {
  caption-side: bottom;
}

/* Limit the width of the author address vcard, so names in right-to-left
   script don't end up on the other side of the page. */

address.vcard {
  max-width: 30em;
  margin-right: auto;
}

/* For address alignment dependent on LTR or RTL scripts */
address div.left {
  text-align: left;
}
address div.right {
  text-align: right;
}

/* Provide table alignment support.  We can't use the alignX classes above
   since they do unwanted things with caption and other styling. */
table.right {
 margin-left: auto;
 margin-right: 0;
}
table.center {
 margin-left: auto;
 margin-right: auto;
}
table.left {
 margin-left: 0;
 margin-right: auto;
}

/* Give the table caption label the same styling as the figcaption */
caption a[href] {
  color: #222;
}

@media print {
  .toplink {
    display: none;
  }

  /* avoid overwriting the top border line with the ToC header */
  #toc {
    padding-top: 1px;
  }

  /* Avoid page breaks inside dl and author address entries */
  .vcard {
    page-break-inside: avoid;
  }

}
/* Tweak the bcp14 keyword presentation */
.bcp14 {
  font-variant: small-caps;
  font-weight: bold;
  font-size: 0.9em;
}
/* Tweak the invisible space above H* in order not to overlay links in text above */
 h2 {
  margin-top: -18px;  /* provide offset for in-page anchors */
  padding-top: 31px;
 }
 h3 {
  margin-top: -18px;  /* provide offset for in-page anchors */
  padding-top: 24px;
 }
 h4 {
  margin-top: -18px;  /* provide offset for in-page anchors */
  padding-top: 24px;
 }
/* Float artwork pilcrow to the right */
@media screen {
  .artwork a.pilcrow {
    display: block;
    line-height: 0.7;
    margin-top: 0.15em;
  }
}
/* Make pilcrows on dd visible */
@media screen {
  dd:hover > a.pilcrow {
    visibility: visible;
  }
}
/* Make the placement of figcaption match that of a table's caption
   by removing the figure's added bottom margin */
.alignLeft.art-text,
.alignCenter.art-text,
.alignRight.art-text {
   margin-bottom: 0;
}
.alignLeft,
.alignCenter,
.alignRight {
  margin: 1em 0 0 0;
}
/* In print, the pilcrow won't show on hover, so prevent it from taking up space,
   possibly even requiring a new line */
@media print {
  a.pilcrow {
    display: none;
  }
}
/* Styling for the external metadata */
div#external-metadata {
  background-color: #eee;
  padding: 0.5em;
  margin-bottom: 0.5em;
  display: none;
}
div#internal-metadata {
  padding: 0.5em;                       /* to match the external-metadata padding */
}
/* Styling for title RFC Number */
h1#rfcnum {
  clear: both;
  margin: 0 0 -1em;
  padding: 1em 0 0 0;
}
/* Make .olPercent look the same as <ol><li> */
dl.olPercent > dd {
  margin-bottom: 0.25em;
  min-height: initial;
}
/* Give aside some styling to set it apart */
aside {
  border-left: 1px solid #ddd;
  margin: 1em 0 1em 2em;
  padding: 0.2em 2em;
}
aside > dl,
aside > ol,
aside > ul,
aside > table,
aside > p {
  margin-bottom: 0.5em;
}
/* Additional page break settings */
@media print {
  figcaption, table caption {
    page-break-before: avoid;
  }
}
/* Font size adjustments for print */
@media print {
  body  { font-size: 10pt;      line-height: normal; max-width: 96%; }
  h1    { font-size: 1.72em;    padding-top: 1.5em; } /* 1*1.2*1.2*1.2 */
  h2    { font-size: 1.44em;    padding-top: 1.5em; } /* 1*1.2*1.2 */
  h3    { font-size: 1.2em;     padding-top: 1.5em; } /* 1*1.2 */
  h4    { font-size: 1em;       padding-top: 1.5em; }
  h5, h6 { font-size: 1em;      margin: initial; padding: 0.5em 0 0.3em; }
}
/* Sourcecode margin in print, when there's no pilcrow */
@media print {
  .artwork,
  .sourcecode {
    margin-bottom: 1em;
  }
}
/* Avoid narrow tables forcing too narrow table captions, which may render badly */
table {
  min-width: 20em;
}
/* ol type a */
ol.type-a { list-style-type: lower-alpha; }
ol.type-A { list-style-type: upper-alpha; }
ol.type-i { list-style-type: lower-roman; }
ol.type-I { list-style-type: lower-roman; }
/* Apply the print table and row borders in general, on request from the RPC,
and increase the contrast between border and odd row background sligthtly */
table {
  border: 1px solid #ddd;
}
td {
  border-top: 1px solid #ddd;
}
tr:nth-child(2n+1) > td {
  background-color: #f8f8f8;
}
/* Use style rules to govern display of the TOC. */
@media screen and (max-width: 1023px) {
  #toc nav { display: none; }
  #toc.active nav { display: block; }
}
/* Add support for keepWithNext */
.keepWithNext {
  break-after: avoid-page;
  break-after: avoid-page;
}
/* Add support for keepWithPrevious */
.keepWithPrevious {
  break-before: avoid-page;
}
/* Change the approach to avoiding breaks inside artwork etc. */
figure, pre, table, .artwork, .sourcecode  {
  break-before: avoid-page;
  break-after: auto;
}
/* Avoid breaks between <dt> and <dd> */
dl {
  break-before: auto;
  break-inside: auto;
}
dt {
  break-before: auto;
  break-after: avoid-page;
}
dd {
  break-before: avoid-page;
  break-after: auto;
  orphans: 3;
  widows: 3
}
span.break, dd.break {
  margin-bottom: 0;
  min-height: 0;
  break-before: auto;
  break-inside: auto;
  break-after: auto;
}
/* Undo break-before ToC */
@media print {
  #toc {
    break-before: auto;
  }
}
/* Text in compact lists should not get extra bottim margin space,
   since that would makes the list not compact */
ul.compact p, .ulCompact p,
ol.compact p, .olCompact p {
 margin: 0;
}
/* But the list as a whole needs the extra space at the end */
section ul.compact,
section .ulCompact,
section ol.compact,
section .olCompact {
  margin-bottom: 1em;                    /* same as p not within ul.compact etc. */
}
/* The tt and code background above interferes with for instance table cell
   backgrounds.  Changed to something a bit more selective. */
tt, code {
  background-color: transparent;
}
p tt, p code, li tt, li code {
  background-color: #f8f8f8;
}
/* Tweak the pre margin -- 0px doesn't come out well */
pre {
   margin-top: 0.5px;
}
/* Tweak the comact list text */
ul.compact, .ulCompact,
ol.compact, .olCompact,
dl.compact, .dlCompact {
  line-height: normal;
}
/* Don't add top margin for nested lists */
li > ul, li > ol, li > dl,
dd > ul, dd > ol, dd > dl,
dl > dd > dl {
  margin-top: initial;
}
/* Elements that should not be rendered on the same line as a <dt> */
/* This should match the element list in writer.text.TextWriter.render_dl() */
dd > div.artwork:first-child,
dd > aside:first-child,
dd > figure:first-child,
dd > ol:first-child,
dd > div:first-child > pre.sourcecode,
dd > table:first-child,
dd > ul:first-child {
  clear: left;
}
/* fix for weird browser behaviour when <dd/> is empty */
dt+dd:empty::before{
  content: "\00a0";
}
/* Make paragraph spacing inside <li> smaller than in body text, to fit better within the list */
li > p {
  margin-bottom: 0.5em
}
/* Don't let p margin spill out from inside list items */
li > p:last-of-type {
  margin-bottom: 0;
}
</style>
<link href="rfc-local.css" rel="stylesheet" type="text/css">
<script type="application/javascript">async function addMetadata(){try{const e=document.styleSheets[0].cssRules;for(let t=0;t<e.length;t++)if(/#identifiers/.exec(e[t].selectorText)){const a=e[t].cssText.replace("#identifiers","#external-updates");document.styleSheets[0].insertRule(a,document.styleSheets[0].cssRules.length)}}catch(e){console.log(e)}const e=document.getElementById("external-metadata");if(e)try{var t,a="",o=function(e){const t=document.getElementsByTagName("meta");for(let a=0;a<t.length;a++)if(t[a].getAttribute("name")===e)return t[a].getAttribute("content");return""}("rfc.number");if(o){t="https://www.rfc-editor.org/rfc/rfc"+o+".json";try{const e=await fetch(t);a=await e.json()}catch(e){t=document.URL.indexOf("html")>=0?document.URL.replace(/html$/,"json"):document.URL+".json";const o=await fetch(t);a=await o.json()}}if(!a)return;e.style.display="block";const s="",d="https://datatracker.ietf.org/doc",n="https://datatracker.ietf.org/ipr/search",c="https://www.rfc-editor.org/info",l=a.doc_id.toLowerCase(),i=a.doc_id.slice(0,3).toLowerCase(),f=a.doc_id.slice(3).replace(/^0+/,""),u={status:"Status",obsoletes:"Obsoletes",obsoleted_by:"Obsoleted By",updates:"Updates",updated_by:"Updated By",see_also:"See Also",errata_url:"Errata"};let h="<dl style='overflow:hidden' id='external-updates'>";["status","obsoletes","obsoleted_by","updates","updated_by","see_also","errata_url"].forEach(e=>{if("status"==e){a[e]=a[e].toLowerCase();var t=a[e].split(" "),o=t.length,w="",p=1;for(let e=0;e<o;e++)p<o?w=w+r(t[e])+" ":w+=r(t[e]),p++;a[e]=w}else if("obsoletes"==e||"obsoleted_by"==e||"updates"==e||"updated_by"==e){var g,m="",b=1;g=a[e].length;for(let t=0;t<g;t++)a[e][t]&&(a[e][t]=String(a[e][t]).toLowerCase(),m=b<g?m+"<a href='"+s+"/rfc/".concat(a[e][t])+"'>"+a[e][t].slice(3)+"</a>, ":m+"<a href='"+s+"/rfc/".concat(a[e][t])+"'>"+a[e][t].slice(3)+"</a>",b++);a[e]=m}else if("see_also"==e){var y,L="",C=1;y=a[e].length;for(let t=0;t<y;t++)if(a[e][t]){a[e][t]=String(a[e][t]);var _=a[e][t].slice(0,3),v=a[e][t].slice(3).replace(/^0+/,"");L=C<y?"RFC"!=_?L+"<a href='"+s+"/info/"+_.toLowerCase().concat(v.toLowerCase())+"'>"+_+" "+v+"</a>, ":L+"<a href='"+s+"/info/"+_.toLowerCase().concat(v.toLowerCase())+"'>"+v+"</a>, ":"RFC"!=_?L+"<a href='"+s+"/info/"+_.toLowerCase().concat(v.toLowerCase())+"'>"+_+" "+v+"</a>":L+"<a href='"+s+"/info/"+_.toLowerCase().concat(v.toLowerCase())+"'>"+v+"</a>",C++}a[e]=L}else if("errata_url"==e){var R="";R=a[e]?R+"<a href='"+a[e]+"'>Errata exist</a> | <a href='"+d+"/"+l+"'>Datatracker</a>| <a href='"+n+"/?"+i+"="+f+"&submit="+i+"'>IPR</a> | <a href='"+c+"/"+l+"'>Info page</a>":"<a href='"+d+"/"+l+"'>Datatracker</a> | <a href='"+n+"/?"+i+"="+f+"&submit="+i+"'>IPR</a> | <a href='"+c+"/"+l+"'>Info page</a>",a[e]=R}""!=a[e]?"Errata"==u[e]?h+=`<dt>More info:</dt><dd>${a[e]}</dd>`:h+=`<dt>${u[e]}:</dt><dd>${a[e]}</dd>`:"Errata"==u[e]&&(h+=`<dt>More info:</dt><dd>${a[e]}</dd>`)}),h+="</dl>",e.innerHTML=h}catch(e){console.log(e)}else console.log("Could not locate metadata <div> element");function r(e){return e.charAt(0).toUpperCase()+e.slice(1)}}window.removeEventListener("load",addMetadata),window.addEventListener("load",addMetadata);</script>
</head>
<body>
<script src="metadata.min.js"></script>
<table class="ears">
<thead><tr>
<td class="left">Internet-Draft</td>
<td class="center">ML-based Virtual Network Embedding</td>
<td class="right">June 2021</td>
</tr></thead>
<tfoot><tr>
<td class="left">Ullah, et al.</td>
<td class="center">Expires 16 December 2021</td>
<td class="right">[Page]</td>
</tr></tfoot>
</table>
<div id="external-metadata" class="document-information"></div>
<div id="internal-metadata" class="document-information">
<dl id="identifiers">
<dt class="label-workgroup">Workgroup:</dt>
<dd class="workgroup">Internet Engineering Task Force</dd>
<dt class="label-internet-draft">Internet-Draft:</dt>
<dd class="internet-draft">draft-ihsan-nmrg-ml-vne-00</dd>
<dt class="label-published">Published:</dt>
<dd class="published">
<time datetime="2021-06-14" class="published">14 June 2021</time>
    </dd>
<dt class="label-intended-status">Intended Status:</dt>
<dd class="intended-status">Informational</dd>
<dt class="label-expires">Expires:</dt>
<dd class="expires"><time datetime="2021-12-16">16 December 2021</time></dd>
<dt class="label-authors">Authors:</dt>
<dd class="authors">
<div class="author">
      <div class="author-name">I. Ullah</div>
<div class="org">KOREATECH</div>
</div>
<div class="author">
      <div class="author-name">Y-H. Han</div>
<div class="org">KOREATECH</div>
</div>
<div class="author">
      <div class="author-name">TY. Kim</div>
<div class="org">ETRI</div>
</div>
</dd>
</dl>
</div>
<h1 id="title">Reinforcement Learning-Based Virtual Network Embedding: Problem Statement</h1>
<section id="section-abstract">
      <h2 id="abstract"><a href="#abstract" class="selfRef">Abstract</a></h2>
<p id="section-abstract-1"> In Network virtualization (NV) technology, Virtual Network Embedding (VNE) is a problem to map a virtual network to the substrate network.
                It has a great impact on the performance of virtual network and resource utilization of the substrate network.
                An efficient embedding strategy can maximize the acceptance ratio of virtual networks to increase the revenue for Internet service provider.
                Several works have been appeared on the design of VNE solutions, however, it has becomes a challenging issues for researchers.
                To solved the VNE problem, reinforcement learning (RL) can play a vital role to make the VNE problem more intelligent and efficient.
                Moreover, RL has been merged with deep learning techniques to develop adaptive models with effective strategies for various complex problems.
                In RL, agents can learn desired behaviors (e.g, optimal VNE strategies), and after learning and completing training, it can embed the virtual network to the subtract network very quickly and efficiently.
                RL can reduce the complexity of the VNE method, however, it is too difficult to apply RL techniques directly to VNE problems and need more research study.
                In this document, we are presenting a problem statement to motivate the research community to solve the VNE problem using reinforcement learning.<a href="#section-abstract-1" class="pilcrow">¶</a></p>
</section>
<div id="status-of-memo">
<section id="section-boilerplate.1">
        <h2 id="name-status-of-this-memo">
<a href="#name-status-of-this-memo" class="section-name selfRef">Status of This Memo</a>
        </h2>
<p id="section-boilerplate.1-1">
        This Internet-Draft is submitted in full conformance with the
        provisions of BCP 78 and BCP 79.<a href="#section-boilerplate.1-1" class="pilcrow">¶</a></p>
<p id="section-boilerplate.1-2">
        Internet-Drafts are working documents of the Internet Engineering Task
        Force (IETF). Note that other groups may also distribute working
        documents as Internet-Drafts. The list of current Internet-Drafts is
        at <span><a href="https://datatracker.ietf.org/drafts/current/">https://datatracker.ietf.org/drafts/current/</a></span>.<a href="#section-boilerplate.1-2" class="pilcrow">¶</a></p>
<p id="section-boilerplate.1-3">
        Internet-Drafts are draft documents valid for a maximum of six months
        and may be updated, replaced, or obsoleted by other documents at any
        time. It is inappropriate to use Internet-Drafts as reference
        material or to cite them other than as "work in progress."<a href="#section-boilerplate.1-3" class="pilcrow">¶</a></p>
<p id="section-boilerplate.1-4">
        This Internet-Draft will expire on 16 December 2021.<a href="#section-boilerplate.1-4" class="pilcrow">¶</a></p>
</section>
</div>
<div id="copyright">
<section id="section-boilerplate.2">
        <h2 id="name-copyright-notice">
<a href="#name-copyright-notice" class="section-name selfRef">Copyright Notice</a>
        </h2>
<p id="section-boilerplate.2-1">
            Copyright (c) 2021 IETF Trust and the persons identified as the
            document authors. All rights reserved.<a href="#section-boilerplate.2-1" class="pilcrow">¶</a></p>
<p id="section-boilerplate.2-2">
            This document is subject to BCP 78 and the IETF Trust's Legal
            Provisions Relating to IETF Documents
            (<span><a href="https://trustee.ietf.org/license-info">https://trustee.ietf.org/license-info</a></span>) in effect on the date of
            publication of this document. Please review these documents
            carefully, as they describe your rights and restrictions with
            respect to this document. Code Components extracted from this
            document must include Simplified BSD License text as described in
            Section 4.e of the Trust Legal Provisions and are provided without
            warranty as described in the Simplified BSD License.<a href="#section-boilerplate.2-2" class="pilcrow">¶</a></p>
</section>
</div>
<div id="toc">
<section id="section-toc.1">
        <a href="#" onclick="scroll(0,0)" class="toplink">▲</a><h2 id="name-table-of-contents">
<a href="#name-table-of-contents" class="section-name selfRef">Table of Contents</a>
        </h2>
<nav class="toc"><ul class="compact ulEmpty toc">
<li class="compact ulEmpty toc" id="section-toc.1-1.1">
            <p id="section-toc.1-1.1.1" class="keepWithNext"><a href="#section-1" class="xref">1</a>.  <a href="#name-introduction-and-scope" class="xref">Introduction and Scope</a></p>
</li>
          <li class="compact ulEmpty toc" id="section-toc.1-1.2">
            <p id="section-toc.1-1.2.1" class="keepWithNext"><a href="#section-2" class="xref">2</a>.  <a href="#name-reinforcement-learning-base" class="xref">Reinforcement Learning-based VNE Solutions</a></p>
</li>
          <li class="compact ulEmpty toc" id="section-toc.1-1.3">
            <p id="section-toc.1-1.3.1" class="keepWithNext"><a href="#section-3" class="xref">3</a>.  <a href="#name-terminology" class="xref">Terminology</a></p>
</li>
          <li class="compact ulEmpty toc" id="section-toc.1-1.4">
            <p id="section-toc.1-1.4.1"><a href="#section-4" class="xref">4</a>.  <a href="#name-problem-space" class="xref">Problem Space</a></p>
<ul class="compact ulEmpty toc">
<li class="compact ulEmpty toc" id="section-toc.1-1.4.2.1">
                <p id="section-toc.1-1.4.2.1.1"><a href="#section-4.1" class="xref">4.1</a>.  <a href="#name-state-representation" class="xref">State Representation</a></p>
</li>
              <li class="compact ulEmpty toc" id="section-toc.1-1.4.2.2">
                <p id="section-toc.1-1.4.2.2.1"><a href="#section-4.2" class="xref">4.2</a>.  <a href="#name-action-space" class="xref">Action Space</a></p>
</li>
              <li class="compact ulEmpty toc" id="section-toc.1-1.4.2.3">
                <p id="section-toc.1-1.4.2.3.1"><a href="#section-4.3" class="xref">4.3</a>.  <a href="#name-reward-description" class="xref">Reward Description</a></p>
</li>
              <li class="compact ulEmpty toc" id="section-toc.1-1.4.2.4">
                <p id="section-toc.1-1.4.2.4.1"><a href="#section-4.4" class="xref">4.4</a>.  <a href="#name-policy-and-rl-methods" class="xref">Policy and RL methods</a></p>
</li>
              <li class="compact ulEmpty toc" id="section-toc.1-1.4.2.5">
                <p id="section-toc.1-1.4.2.5.1"><a href="#section-4.5" class="xref">4.5</a>.  <a href="#name-training-environment" class="xref">Training Environment</a></p>
</li>
              <li class="compact ulEmpty toc" id="section-toc.1-1.4.2.6">
                <p id="section-toc.1-1.4.2.6.1"><a href="#section-4.6" class="xref">4.6</a>.  <a href="#name-sim2real-gap" class="xref">Sim2Real Gap</a></p>
</li>
              <li class="compact ulEmpty toc" id="section-toc.1-1.4.2.7">
                <p id="section-toc.1-1.4.2.7.1"><a href="#section-4.7" class="xref">4.7</a>.  <a href="#name-generalization" class="xref">Generalization</a></p>
</li>
            </ul>
</li>
          <li class="compact ulEmpty toc" id="section-toc.1-1.5">
            <p id="section-toc.1-1.5.1"><a href="#section-5" class="xref">5</a>.  <a href="#name-iana-considerations" class="xref">IANA Considerations</a></p>
</li>
          <li class="compact ulEmpty toc" id="section-toc.1-1.6">
            <p id="section-toc.1-1.6.1"><a href="#section-6" class="xref">6</a>.  <a href="#name-security-considerations" class="xref">Security Considerations</a></p>
</li>
          <li class="compact ulEmpty toc" id="section-toc.1-1.7">
            <p id="section-toc.1-1.7.1"><a href="#section-7" class="xref">7</a>.  <a href="#name-informative-references" class="xref">Informative References</a></p>
</li>
          <li class="compact ulEmpty toc" id="section-toc.1-1.8">
            <p id="section-toc.1-1.8.1"><a href="#section-appendix.a" class="xref"></a><a href="#name-authors-addresses" class="xref">Authors' Addresses</a></p>
</li>
        </ul>
</nav>
</section>
</div>
<section id="section-1">
      <h2 id="name-introduction-and-scope">
<a href="#section-1" class="section-number selfRef">1. </a><a href="#name-introduction-and-scope" class="section-name selfRef">Introduction and Scope</a>
      </h2>
<p id="section-1-1"> Recently, Network virtualization (NV) technology has received a lot of attention from academics and industry.
           It allows multiple heterogeneous virtual networks to share resources on the same substrate network (SN) <span>[<a href="#RFC7364" class="xref">RFC7364</a>]</span>, <span>[<a href="#ASNVT2020" class="xref">ASNVT2020</a>]</span>.
           The current large-size fixed substrate network architecture is no longer efficient and not extendable due to network ossification.
           To overcome this limitations, traditional Internet Service Providers (ISPs) are divided into two independent parts which work together.
           One is the Service Providers (SPs) who create and own the different number of the VNs, and the other one is the Infrastructure Providers (InPs) who own the SN devices and links as underlying resources.
           SPs generate and construct the customized Virtual Network Requests (VNRs), and lease the resources from InPs based on that requests.
           In addition, two types of mediators can enter into the industry domain for better coordination of SPs and InPs.
           One is the Virtual Network Providers (VNPs) who assemble and coordinate diverse virtual resources from one or more InPs, the other one is the Virtual Network Operators (VNOs) who create, manage, and operate the VN according to the demand of the SPs.
           VNPs and VNOs could enable efficient use of the physical network and increase the commercial revenue of both SPs and InPs. NV can increase network agility, flexibility and scalability while creating significant cost savings.
           Greater network workload mobility, increased availability of network resources with good performance, and automated operations, are all the benefits of NV.<a href="#section-1-1" class="pilcrow">¶</a></p>
<p id="section-1-2">
          Virtual Network Embedding (VNE) <span>[<a href="#VNESURV2013" class="xref">VNESURV2013</a>]</span> is one of the main problem to map a virtual network to the substrate network.
          The VNE method has two main parts, Node embedding: where virtual nodes of VN have to be mapped to the SN nodes, and Link embedding: where virtual links between the VNs have to be mapped to the physical paths in the substrate network.
          It has been proven to be NP-Hard, and both node and link embeddings have become challenging for the researchers.
          A virtual node and link should be efficiently embedded into a given SN, so that more VNR can be accepted with minimum cost.
          The distance of the virtual nodes from each other in a given SN is a big contribution to the link failures and causes the rejection of VNRs.
          Hence, an efficient and intelligent technique is required for VNE problem to reduce VNRs rejection <span>[<a href="#ENViNE2021" class="xref">ENViNE2021</a>]</span>.
          In the perspective of the InPs, the efficient VNE performs better mostly in terms of revenue, acceptance ratio, and revenue-to-cost ratio.<a href="#section-1-2" class="pilcrow">¶</a></p>
<p id="section-1-3">Figure 1 shows the the example of two virtual network request VNR1 and VNR2 to embed them in the given substrate network.
          VNR1 contain three virtual nodes (a, b, and c) with cpu demands (15, 30, and 10) respectively, and the link between virtual the nodes a-b,b-c, and c-a with bandwidth demands 15,20, and 35 respectively.
          Similarly, VNR2 contains virtual nodes and links with cpu and bandwidth demand respectively.
          The purpose of the VNE method to map the virtual nodes and links of the VNRs to the physical nodes and links of the given substrate as shown in Figure 1. <span>[<a href="#ENViNE2021" class="xref">ENViNE2021</a>]</span>.<a href="#section-1-3" class="pilcrow">¶</a></p>
<span id="name-substrate-network-with-embe"></span><figure id="figure-1">
        <div class="artwork art-text alignCenter" id="section-1-4.1">
<pre>
           +----+                +----+         +----+          +----+
           | a  |                | d  |         | e  |          | f  |
           | 15 |                | 25 |__ _25___| 30 |__ _35_ __| 45 |
           +----+                +----+         +----+          +----+
          /      \                \                                 /
        15        35               30                              20
        /          \                \                             /
  +----+            +----+           +----+                 +----+
  | b  |            | c  |           | g  |                 | h  |
  | 30 |__ _20_ __ _| 10 |           | 15 |__ _ __10__ __ __| 35 |
  +----+            +----+           +----+                 +----+

           (VNR1)                                 (VNR2)
             ||   Embedding                         ||    Embedding
             VV                                     VV

        +----+              +----+       +----+                  +----+
 .......| a  |......35......| c  |       | d  |........25........| e  |
:  _____| 15 |              | 10 |_______| 25 |          ________| 30 |
: |     +----+              +----+       +----+         |        +----+
: |   A      |                | :   B      | :          |   C      |  :
: |   50     |__ ___50__ __ __| :   60     |_:_ __30 _ _|   40     |  :
: +__________+                +_:_________+  :          +__________+  :
:      |                        :     |      :                |       :
15     |                        :     |      :                |      35
:     40                       20     60     :               50       :
:      |                        :     |     30                |       :
:      |                       _:_____|_     :                |       :
+----:..............20........|.:       |    :                |   +----+
| b  | |   +----+.....30......|.........|....:                |   | f  |
| 30 |_|___| g  |             |       +----+                __|___| 45 |
+----+     | 15 |.....10......|.......| h  |........20.....|......+----+
 |   D     +____+             |    E  | 35 |               |     F    |
 |   50     |__ __ __ 70 _____|    40 +____+ ___ __ 50_ ___|     60   |
 +__________+                 +_________+                  +__________+

</pre>
</div>
<figcaption><a href="#figure-1" class="selfRef">Figure 1</a>:
<a href="#name-substrate-network-with-embe" class="selfRef">Substrate network with embedded virtual network, VNR1 and VNR2</a>
        </figcaption></figure>
<p id="section-1-5">
          Recently, artificial intelligence and machine learning technologies have been widely used to solve networking problems <span>[<a href="#SUR2018" class="xref">SUR2018</a>]</span>, <span>[<a href="#MLCNM2018" class="xref">MLCNM2018</a>]</span>, <span>[<a href="#MVNNML2021" class="xref">MVNNML2021</a>]</span>.
          There has been a surge in research efforts,specially,reinforcement learning (RL) which has been contributed much more in the many complex tasks, e.g. video games and auto-driving etc.
          The main goal of an RL to learn better policies for sequential decision making problems (e.g., VNE) and solve them very efficiently.
          Several works have appeared on the design of VNE solutions using RL, which focuses on how to interact with the environment to achieve maximum cumulative return <span>[<a href="#VNEQS2021" class="xref">VNEQS2021</a>]</span>, <span>[<a href="#NRRL2020" class="xref">NRRL2020</a>]</span>, <span>[<a href="#MVNE2020" class="xref">MVNE2020</a>]</span>, <span>[<a href="#CDVNE2020" class="xref">CDVNE2020</a>]</span>, <span>[<a href="#PPRL2020" class="xref">PPRL2020</a>]</span>, <span>[<a href="#RLVNEWSN2020" class="xref">RLVNEWSN2020</a>]</span>, <span>[<a href="#QLDC2019" class="xref">QLDC2019</a>]</span>, <span>[<a href="#VNFFG2020" class="xref">VNFFG2020</a>]</span>, <span>[<a href="#VNEGCN2020" class="xref">VNEGCN2020</a>]</span>, <span>[<a href="#NFVDeep2019" class="xref">NFVDeep2019</a>]</span>, <span>[<a href="#DeepViNE2019" class="xref">DeepViNE2019</a>]</span>, <span>[<a href="#VNETD2019" class="xref">VNETD2019</a>]</span>, <span>[<a href="#RDAM2018" class="xref">RDAM2018</a>]</span>, <span>[<a href="#MOQL2018" class="xref">MOQL2018</a>]</span>, <span>[<a href="#ZTORCH2018" class="xref">ZTORCH2018</a>]</span>, <span>[<a href="#NeuroViNE2018" class="xref">NeuroViNE2018</a>]</span>, <span>[<a href="#QVNE2020" class="xref">QVNE2020</a>]</span>.
          This document outlines the problems encountered when designing and applying RL-based VNE solutions.
          Section 2 describes how to design RL-based VNE solutions. Section 3 gives terminology, and Section 4 describes the problem space details.<a href="#section-1-5" class="pilcrow">¶</a></p>
</section>
<div id="terminology">
<section id="section-2">
      <h2 id="name-reinforcement-learning-base">
<a href="#section-2" class="section-number selfRef">2. </a><a href="#name-reinforcement-learning-base" class="section-name selfRef">Reinforcement Learning-based VNE Solutions</a>
      </h2>
<p id="section-2-1"> As we discussed that RL has been studied in various fields (such as game, control system, operation research, information theory, multi-agent system, network system, etc.) and shows better performance than humans.
                Unlike deep learning, RL trains a policy model by receiving rewards through interaction with the environment without training label data.<a href="#section-2-1" class="pilcrow">¶</a></p>
<p id="section-2-2">
                Recently, there have been several attempts to solve VNE problems using RL.
                When applying RL-based methods to solve VNE problems, the RL agent automatically learns without human intervention through interaction with the environment.
                Once the agent completed the learning process, it can generate the most appropriate embeddings decision (action) based on the state of the network.
                Based on the embedding or action the agent get reward from the environments to  adaptively train its policy for future action.
                The RL agent gets the most optimized model based on the reward function defined according to each objective (revenue, cost, revenue to cost ratio and acceptance ratio).
                The optimal RL policy model provides the VNE strategy appropriately according to the objective of the network operator.
                Designing and applying RL techniques directly into VNE problems is not yet trivial, but may face several challenges. This document describes the problems.<a href="#section-2-2" class="pilcrow">¶</a></p>
<p id="section-2-3">
                Figure 2 shows the virtual network embedding solution based on RL method.
                The RL is divided into a training process and an inference process.
                In the training process, state information is composed of various substrate networks and VNRs (Environment), which are used as suitable inputs for RL models through feature extraction.
                After that, the RL model is updated by model updater using a feature extracted state and reward.
                In the inference process, using the trained RL model, the embedding result is provided to the operating network in real time.<a href="#section-2-3" class="pilcrow">¶</a></p>
<p id="section-2-4">
                  The following figure shows the detail about RL-based VNE solutions.<a href="#section-2-4" class="pilcrow">¶</a></p>
<span id="name-two-processes-for-rl-based-"></span><figure id="figure-2">
        <div class="artwork art-text alignCenter" id="section-2-5.1">
<pre>
RL Model Training Process
+--------------------------------------------------------------------+
| Training Environment                                               |
| +-------------------+         RL-based VNE Agent                   |
| | +---------+       |         +----------------------------------+ |
| | | +---------+     |         |                   Action         | |
| | | | +----------+  |&lt;----------------------------------+        | |
| | + | | Substrate|  |         |                         |        | |
| |   | | Networks |  |         |  +----------+      +----------+  | |
| |   + +----------+  |  State  |  | Feature  |      |    RL    |  | |
| |                   |-----------&gt;|Extraction|-----&gt;|   Model  |  | |
| | +--------+        |         |  +----------+      | (Policy) |  | |
| | | +---------+     |         |       |            +----------+  | |
| | + | +---------+   |         |       |   +---------+     A      | |
| |   + |  VNRs   |   | Reward  |       +--&gt;|  Model  |     |      | |
| |     +---------+   |--------------------&gt;| Updater |-----+      | |
| +-------------------+         |           +---------+            | |
|                               +----------------------------------+ |
+--------------------------------------------------------------------+
                                  |
Inference Process                 |
+---------------------------------V----------------------------------+
|                         + - - - - - - - +                          |
| Operating Network       |   RL Model    |    Trained RL Model      |
| (Inference Environment) |   Training    |------------------+       |
| +-------------------+   |   Process     |                  |       |
| |   +-----------+   |   + - - - - - - - +                  |       |
| |   |           |   |         RL-based VNE Agent           |       |
| |   | Substrate |   |         +----------------------------|-----+ |
| |   |  Network  |   |         |                   Action   |     | |
| |   |           |   |&lt;---------------------------------+   |     | |
| |   +-----------+   |         |                        |   V     | |
| | +---------+       |         |  +------------+     +---------+  | |
| | | +---------+     | State   |  |  Feature   |     | Trained |  | |
| | + | +----------+  |-----------&gt;| Extraction |----&gt;|   RL    |  | |
| |   + |   VNRs   |  |         |  +------------+     |  Model  |  | |
| |     +----------+  |         |                     +---------+  | |
| +-------------------+         +----------------------------------+ |
+--------------------------------------------------------------------+
</pre>
</div>
<figcaption><a href="#figure-2" class="selfRef">Figure 2</a>:
<a href="#name-two-processes-for-rl-based-" class="selfRef">Two processes for RL-based VNE solutions</a>
        </figcaption></figure>
</section>
</div>
<section id="section-3">
      <h2 id="name-terminology">
<a href="#section-3" class="section-number selfRef">3. </a><a href="#name-terminology" class="section-name selfRef">Terminology</a>
      </h2>
<span class="break"></span><dl class="dlNewline" id="section-3-1">
        <dt id="section-3-1.1">Network Virtualization</dt>
        <dd style="margin-left: 1.5em" id="section-3-1.2">Network virtualization is the process of combining hardware and software network resources and network functionality into a single, software-based administrative entity, a virtual network <span>[<a href="#RFC7364" class="xref">RFC7364</a>]</span>.<a href="#section-3-1.2" class="pilcrow">¶</a>
</dd>
      <dd class="break"></dd>
</dl>
<span class="break"></span><dl class="dlNewline" id="section-3-2">
        <dt id="section-3-2.1">Virtual Network Embedding (VNE)</dt>
        <dd style="margin-left: 1.5em" id="section-3-2.2">Virtual Network Embedding (VNE) <span>[<a href="#VNESURV2013" class="xref">VNESURV2013</a>]</span> is one of the main techniques used to map a virtual network to the substrate network.<a href="#section-3-2.2" class="pilcrow">¶</a>
</dd>
      <dd class="break"></dd>
</dl>
<span class="break"></span><dl class="dlNewline" id="section-3-3">
        <dt id="section-3-3.1">Substrate Network (SN)</dt>
        <dd style="margin-left: 1.5em" id="section-3-3.2">The underlying physical network which contains the resources such as CPU and bandwidth for virtual networks is called substrate network.<a href="#section-3-3.2" class="pilcrow">¶</a>
</dd>
      <dd class="break"></dd>
</dl>
<span class="break"></span><dl class="dlNewline" id="section-3-4">
        <dt id="section-3-4.1">Virtual Network Request (VNR)</dt>
        <dd style="margin-left: 1.5em" id="section-3-4.2">Virtual Network Request is a complete single Virtual network request containing virtual nodes and virtual links.<a href="#section-3-4.2" class="pilcrow">¶</a>
</dd>
      <dd class="break"></dd>
</dl>
<span class="break"></span><dl class="dlNewline" id="section-3-5">
        <dt id="section-3-5.1">Agent</dt>
        <dd style="margin-left: 1.5em" id="section-3-5.2">In RL, an agent is the component that makes the decision abd take action  (i.e., embedding decision).<a href="#section-3-5.2" class="pilcrow">¶</a>
</dd>
      <dd class="break"></dd>
</dl>
<span class="break"></span><dl class="dlNewline" id="section-3-6">
        <dt id="section-3-6.1">State</dt>
        <dd style="margin-left: 1.5em" id="section-3-6.2">State is a representation (e.g., remaining SN capacity and requested VN resource) of the current environment, and it tells the agent what situation it is in currently.<a href="#section-3-6.2" class="pilcrow">¶</a>
</dd>
      <dd class="break"></dd>
</dl>
<span class="break"></span><dl class="dlNewline" id="section-3-7">
        <dt id="section-3-7.1">Action</dt>
        <dd style="margin-left: 1.5em" id="section-3-7.2">Actions (i.e., node and link embedding) are behavior an RL agent can do to change the states of the environment.<a href="#section-3-7.2" class="pilcrow">¶</a>
</dd>
      <dd class="break"></dd>
</dl>
<span class="break"></span><dl class="dlNewline" id="section-3-8">
        <dt id="section-3-8.1">Policy</dt>
        <dd style="margin-left: 1.5em" id="section-3-8.2">A policy defines an agent's way of behaving at a given time.
                        It is a mapping from perceived states of environment to actions to be taken when in those states.
                        It is usually implemented as a deep learning model because the state and action spaces are too large to be completely known.<a href="#section-3-8.2" class="pilcrow">¶</a>
</dd>
      <dd class="break"></dd>
</dl>
<span class="break"></span><dl class="dlNewline" id="section-3-9">
        <dt id="section-3-9.1">Reward</dt>
        <dd style="margin-left: 1.5em" id="section-3-9.2">A reward is the feedback which provides an agent to the agent for taking actions that lead to good outcomes (i.g., achieve the objective of the network operator).<a href="#section-3-9.2" class="pilcrow">¶</a>
</dd>
      <dd class="break"></dd>
</dl>
<span class="break"></span><dl class="dlNewline" id="section-3-10">
        <dt id="section-3-10.1">Environment</dt>
        <dd style="margin-left: 1.5em" id="section-3-10.2">An environment is the agent's world in which it lives and interacts.
                    The agent can interact with the environment by performing some action but cannot influence the rules of the environment by those actions.<a href="#section-3-10.2" class="pilcrow">¶</a>
</dd>
      <dd class="break"></dd>
</dl>
</section>
<div id="problem_statements">
<section id="section-4">
      <h2 id="name-problem-space">
<a href="#section-4" class="section-number selfRef">4. </a><a href="#name-problem-space" class="section-name selfRef">Problem Space</a>
      </h2>
<p id="section-4-1">RL contains three main components: state representation, action space, and reward description.
                For solving a VNE problem, we need to consider how to design the three main RL components.
                In addition, a specific RL method, training environment, sim2real gap, and generalization are also important issues that should be considered and addressed.
                We will describe each one in detail as follows.<a href="#section-4-1" class="pilcrow">¶</a></p>
<section id="section-4.1">
        <h3 id="name-state-representation">
<a href="#section-4.1" class="section-number selfRef">4.1. </a><a href="#name-state-representation" class="section-name selfRef">State Representation</a>
        </h3>
<p id="section-4.1-1">The way to understand and observe the VNE problem is crucial for an RL agent to establish a thorough knowledge of the network status and generate efficient embedding decisions.
                        Therefore, it is essential to firstly design the state representation that serves as the input to the agent.
                        The state representation is the information which an agent can receive from the environment, and consists of a set of values representing the current situation in the environment.
                        Based on the state representation, the RL agent selects the most appropriate action through its policy model.
                        In the VNE problem, an RL agent needs to know the information of the overall SN entities and their current status in order to use the resources of the nodes and edges of the substrate network.
                        Also it must know the requirements of the VNR.<a href="#section-4.1-1" class="pilcrow">¶</a></p>
<p id="section-4.1-2">
                        Therefore, in the VNE problem, the state usually should represent the current resource state of the nodes and edges of the substrate network (ie, CPU, memory, storage, bandwidth, delay, loss rate, etc.) and the requirements of the virtual node and link of the VNR.
                        The collected status information is used as raw input, or refined status information through the feature extraction process is used as input for the RL agent.
                        The state representation may vary depending on the operator's objective and VNE strategy.
                        The method of determining such feature extraction and representation greatly affects the performance of the agent.<a href="#section-4.1-2" class="pilcrow">¶</a></p>
</section>
<section id="section-4.2">
        <h3 id="name-action-space">
<a href="#section-4.2" class="section-number selfRef">4.2. </a><a href="#name-action-space" class="section-name selfRef">Action Space</a>
        </h3>
<p id="section-4.2-1">In RL, an action represents a decision that an RL agent can take based on current state representation.
                        The set of all possible actions is called an action space.
                        In the VNE problems, actions are generally divided into node embedding and link embedding. The action for node embedding means the VNR's nodes are assigned to which nodes in the SN.<a href="#section-4.2-1" class="pilcrow">¶</a></p>
<p id="section-4.2-2">
                        Also, for link embedding, the action represents the selected paths between the selected substrate network nodes from the node embedding result.
                        If the policy model of the RL agent is well trained, it will select the embedding result to maximize the reward appropriate for the operator's objectives.
                        The output actions generated from the agent will indicate the adjustment of allocated resources.<a href="#section-4.2-2" class="pilcrow">¶</a></p>
<p id="section-4.2-3">
                        It is noted that, at each point of time step, an RL method may decide to 1) embed each virtual node onto substrate nodes and then embed each virtual link onto substrate paths separately, or 2) embed the given whole VNR onto substrate nodes and links in the SN at once.
                        In the former case, at every single step, a learning agent focuses on exactly one virtual node from the current VNR, and it generates a certain substrate node to host the virtual node.
                        Link embedding is then performed separately in the same time step.
                        To solve the VNE problem efficiently, mapping of virtual nodes and links are considered together, although they are performed separately.
                        Link mapping is considering more complex than node mapping, because a virtual link can be mapped onto a physical path with different hops.
                        On the other hand, at every single step, a learning agent can try to embed the given whole VNR, i.e., all virtual nodes and links in the given VNR, onto a subset of SN components.
                        The whole VNR embedding should be handled as a graph embedding, so that the action space is huge and the design of the RL method is usually more difficult than the one with each node and link embedding.<a href="#section-4.2-3" class="pilcrow">¶</a></p>
</section>
<section id="section-4.3">
        <h3 id="name-reward-description">
<a href="#section-4.3" class="section-number selfRef">4.3. </a><a href="#name-reward-description" class="section-name selfRef">Reward Description</a>
        </h3>
<p id="section-4.3-1">Designing rewards is an important issue for an RL method.
                       In general, the reward is the benefit that an RL agent follows when performing its determined action.
                       Reward is an immediate value that evaluates only the current state and action.
                       The value of reward depends on success or failure of each step.
                       In order to select the action that gives the best results in the long run, an RL agent needs to select the action with the highest cumulative reward.<a href="#section-4.3-1" class="pilcrow">¶</a></p>
<p id="section-4.3-2">
                        The reward is calculated through the reward function according to the objective of the environment, and even in the same environment, it may be different depending on the operator's objective.
                       Based on the given reward the agent can evaluate the effectiveness to improve the policy.
                       Hence, the reward function play a important rules in the training process of RL.
                       In the VNE problem, the overall objectives are to reduce the VNE rejection, embed them with minimum cost, maximize the revenue, and increase the resource utilization of physical resources.
                       Reward function should be designed to achieve one or multiple ones of these objectives.
                       Each objective and its correspondent reward design are outlined as follows:<a href="#section-4.3-2" class="pilcrow">¶</a></p>
<span class="break"></span><dl class="dlNewline" id="section-4.3-3">
          <dt id="section-4.3-3.1">Revenue</dt>
          <dd style="margin-left: 1.5em" id="section-4.3-3.2">Revenue is the sum of the virtual resources requested by the VN, and calculated to determine the total cost of the resources.
                        Typically, a successful action (e.g., VNR is embedded without violation) is treated to be a good reward which also increases the revenue.
                        Otherwise, a failed action (e.g., VNR is rejected) leads that the agent will receive a negative reward as well as decreasing the revenue.<a href="#section-4.3-3.2" class="pilcrow">¶</a>
</dd>
        <dd class="break"></dd>
</dl>
<span class="break"></span><dl class="dlNewline" id="section-4.3-4">
          <dt id="section-4.3-4.1">Acceptance Ratio</dt>
          <dd style="margin-left: 1.5em" id="section-4.3-4.2">Acceptance ratio is the ratio measured by the number of successfully embedded virtual network requests divided by total number of virtual network requests.
                          To achieve a high acceptance ratio, the agent is trying to embed maximum VNR and get a good reward. Getting a good reward is usually proportional to the acceptance ratio.<a href="#section-4.3-4.2" class="pilcrow">¶</a>
</dd>
        <dd class="break"></dd>
</dl>
<span class="break"></span><dl class="dlNewline" id="section-4.3-5">
          <dt id="section-4.3-5.1">Revenue-to-cost ratio</dt>
          <dd style="margin-left: 1.5em" id="section-4.3-5.2">To balance and compare the cost of resources for embedding VNR, the revenue is divided by cost.
                          Revenue-to-cost ratio compares the embedding methods with respect to their embedding results in terms of the cost and revenue.
                          Since most VNOs are most interested in this objective, a reward function should be made to relate to this performance metric.<a href="#section-4.3-5.2" class="pilcrow">¶</a>
</dd>
        <dd class="break"></dd>
</dl>
</section>
<div id="use_cases">
<section id="section-4.4">
        <h3 id="name-policy-and-rl-methods">
<a href="#section-4.4" class="section-number selfRef">4.4. </a><a href="#name-policy-and-rl-methods" class="section-name selfRef">Policy and RL methods</a>
        </h3>
<p id="section-4.4-1">The policy is the strategy that the agent employs to determine the next action based on the current state.
               It maps states to actions that promise the highest reward.
               Therefore, an RL agent updates its policy repeatedly in the learning phase to maximize the expected cumulative reward.
               Unlike supervised learning, in which each sample has a corresponding label indicating the preferred output of the learning model, an RL agent relies on reward signals to evaluate the effectiveness of actions and further improve the policy.
               From the perspective of RL, the goal of VNE is to find an optimal policy to embed an VNR onto the given SN in any state at any time.
               There are two types of RL methods: on-policy and off-policy.
               In on-policy RL methods,  the (behaviour) policy of the exploration step to select an action and the policy to learn are the same.
               On-policy methods work with a single policy, and require any observations (state, action, reward, next state) to have been generated using that policy.
               Representative on-policy methods include A2C, A3C, TRPO, and PPO. On the other hand, off-policy RL methods work with two policies.<a href="#section-4.4-1" class="pilcrow">¶</a></p>
<p id="section-4.4-2">
                These are a policy being learned, called the target policy, and the policy being followed that generates the observations, called the behaviour policy.
               In off-policy RL methods, the learning policy and the behaviour policy are not necessarily the same. It allows the use of exploratory policies for collecting the experience, since learning and behavior policies are separated.
               In the VNE problem, various experiences can be accumulated by extracting embedding results using various behavior policies. Representative off-policy methods include Q-learning, DQN, DDPG, and SAC.
               There are different classifications for RL methods: model-based and model-free. In model-based RL methods, an RL agent learns its optimal behavior indirectly by learning a model of the environment by taking actions and observing the outcomes that include the next state and the immediate reward.<a href="#section-4.4-2" class="pilcrow">¶</a></p>
<p id="section-4.4-3">
             The models predict the outcomes of actions. The model is used instead of the environment or in addition to interaction with it to learn optimal policies.
               This becomes, however, impractical when the state and action space is large. Unlike model-based methods, model-free RL methods learn directly by trial and error with the environment and do not require the relatively large memory.
               Since data efficiency or safety is very important even in VNE problems, the use of model-based methods can be actively considered. However, since it is not easy to build a good model that mimics a real network environment, a model-free RL method may be more suitable for VNE problems.
               In conclusion, a good RL method selection plays an important role in solving the VNE problem, and VNE performance metrics vary depending on the selected RL method.<a href="#section-4.4-3" class="pilcrow">¶</a></p>
</section>
</div>
<section id="section-4.5">
        <h3 id="name-training-environment">
<a href="#section-4.5" class="section-number selfRef">4.5. </a><a href="#name-training-environment" class="section-name selfRef">Training Environment</a>
        </h3>
<p id="section-4.5-1">Simulation is the use of software to simulate an interacting environment that is difficult to actually execute and test.
                        An RL method learns by iteratively interacting with the environment. However, in the real environment, various variables such as failure and component consumption exist.
                        Therefore, it is necessary to learn through a simulation that simulates the real environment.
                        In order to solve the VNE problem, we need to use a network simulator similar to the real environment because it is difficult to repeatedly experiment with real network environments using an RL method, and it is very challenging and overwhelming to directly apply an RL method to real-world environments.
                        When solving VNE problems, a network simulation environment similar to a real network is required. The network simulation environment should have a general SN environment and VNR required by the operator.
                        The SN has nodes and links between nodes, and each has capacity such as CPU and Bandwidth.
                        In the case of VNR, there are virtual nodes and links required by the operator, and each must have its own requirements.<a href="#section-4.5-1" class="pilcrow">¶</a></p>
</section>
<section id="section-4.6">
        <h3 id="name-sim2real-gap">
<a href="#section-4.6" class="section-number selfRef">4.6. </a><a href="#name-sim2real-gap" class="section-name selfRef">Sim2Real Gap</a>
        </h3>
<p id="section-4.6-1">An RL method iteratively learns through a simulation environment to train a model of the desired policy.
                        The trained model is then applied to the real environment and/or tuned more for adapting to the real one.
                        However, when the trained model is applied in the simulation to the real environment, sim2real gap problem arises. Obviously, the simulation environment does not match perfectly to the real environment which mostly fails in the tuning process and gives poor performance in the model because of the Sim2Real gap.
                        The sim2real gap is caused by the difference between the simulation and the real environment.<a href="#section-4.6-1" class="pilcrow">¶</a></p>
<p id="section-4.6-2">
                        It is because the simulation environment cannot perfectly simulate the real environment, and there are many variables in the real environment.
                        In a real network environment for VNE, the SN's nodes and links may fail due to external factors, or capacity such as CPU may change suddenly.
                        In order to solve this problem, the simulation environment should be more robust or the trained RL model should be generalized.
                        To reduce the gap between sim and real network environments we need to train our model with an efficient and large number of  VNR and keep learning the agent not only depend on previous memorization.<a href="#section-4.6-2" class="pilcrow">¶</a></p>
</section>
<section id="section-4.7">
        <h3 id="name-generalization">
<a href="#section-4.7" class="section-number selfRef">4.7. </a><a href="#name-generalization" class="section-name selfRef">Generalization</a>
        </h3>
<p id="section-4.7-1">Generalization refers to the trained model's ability to adapt properly to previously unseen new observations.
                            An RL method tries to learn a model that optimizes some objective with the purpose of performing well on data that has never been seen by the model during training.
                            In terms of VNE problems, the generalization is a measure of how the agent's policy model performs on predicting unseen VNR.
                            The RL agent not only has to memorize all the previous variance of the VNR but also to learn and explore more possible variance.
                            It is important to have good and efficient training data for VNR with good variance and train the model with all possible VNRs.<a href="#section-4.7-1" class="pilcrow">¶</a></p>
</section>
</section>
</div>
<div id="IANA">
<section id="section-5">
      <h2 id="name-iana-considerations">
<a href="#section-5" class="section-number selfRef">5. </a><a href="#name-iana-considerations" class="section-name selfRef">IANA Considerations</a>
      </h2>
<p id="section-5-1">This memo includes no request to IANA.<a href="#section-5-1" class="pilcrow">¶</a></p>
</section>
</div>
<div id="Security">
<section id="section-6">
      <h2 id="name-security-considerations">
<a href="#section-6" class="section-number selfRef">6. </a><a href="#name-security-considerations" class="section-name selfRef">Security Considerations</a>
      </h2>
<p id="section-6-1">This is an Informational draft that details research challenges. It does not introduce any security threat.<a href="#section-6-1" class="pilcrow">¶</a></p>
</section>
</div>
<section id="section-7">
      <h2 id="name-informative-references">
<a href="#section-7" class="section-number selfRef">7. </a><a href="#name-informative-references" class="section-name selfRef">Informative References</a>
      </h2>
<dl class="references">
<dt id="ASNVT2020">[ASNVT2020]</dt>
      <dd>
<span class="refAuthor">Sharif, Kashif.</span>, <span class="refAuthor">Li, Fan.</span>, <span class="refAuthor">Latif, Zohaib.</span>, <span class="refAuthor">Karim, MM.</span>, and <span class="refAuthor">Sujit. Biswas</span>, <span class="refTitle">"A Survey of Network Virtualization Techniques for Internet of Things using SND and NFV"</span>, <span class="seriesInfo">DOI 10.1145/3379444</span>, <time datetime="2020-04" class="refDate">April 2020</time>, <span>&lt;<a href="https://doi.org/10.1145/3379444">https://doi.org/10.1145/3379444</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="CDVNE2020">[CDVNE2020]</dt>
      <dd>
<span class="refTitle">"A Continuous-Decision Virtual Network Embedding Scheme Relying on Reinforcement Learning"</span>, <span class="seriesInfo">DOI 10.1109/TNSM.2020.2971543</span>, <time datetime="2020-02" class="refDate">February 2020</time>, <span>&lt;<a href="https://ieeexplore.ieee.org/document/8982091">https://ieeexplore.ieee.org/document/8982091</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="DeepViNE2019">[DeepViNE2019]</dt>
      <dd>
<span class="refAuthor">Dolati, M.</span>, <span class="refAuthor">Hassanpour, S. B.</span>, <span class="refAuthor">Ghaderi, M.</span>, and <span class="refAuthor">A. Khonsari</span>, <span class="refTitle">"DeepViNE: Virtual Network Embedding with Deep Reinforcement Learning"</span>, <span class="seriesInfo">DOI 10.1109/INFCOMW.2019.8845171</span>, <time datetime="2019-09" class="refDate">September 2019</time>, <span>&lt;<a href="https://ieeexplore.ieee.org/document/8845171">https://ieeexplore.ieee.org/document/8845171</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="ENViNE2021">[ENViNE2021]</dt>
      <dd>
<span class="refAuthor">ULLAH, IHSAN.</span>, <span class="refAuthor">Lim, Hyun-Kyo.</span>, and <span class="refAuthor">Youn-Hee. Han</span>, <span class="refTitle">"Ego Network-Based Virtual Network Embedding Scheme for Revenue Maximization"</span>, <span class="seriesInfo">DOI 10.1109/ICAIIC51459.2021.9415185</span>, <time datetime="2021-04" class="refDate">April 2021</time>, <span>&lt;<a href="https://ieeexplore.ieee.org/document/9415185">https://ieeexplore.ieee.org/document/9415185</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="MLCNM2018">[MLCNM2018]</dt>
      <dd>
<span class="refAuthor">Ayoubi, Sara.</span>, <span class="refAuthor">Noura, Limam.</span>, <span class="refAuthor">Salahuddin, Mohammad.</span>, <span class="refAuthor">Shahriar, Nashid.</span>, <span class="refAuthor">Boutaba, NRaouf.</span>, <span class="refAuthor">Estrada-Solano, Felipe.</span>, and <span class="refAuthor">Oscar. M. Caicedo</span>, <span class="refTitle">"Machine Learning for Cognitive Network Management"</span>, <span class="seriesInfo">DOI 10.1109/MCOM.2018.1700560</span>, <time datetime="2018-01" class="refDate">January 2018</time>, <span>&lt;<a href="https://ieeexplore.ieee.org/document/8255757">https://ieeexplore.ieee.org/document/8255757</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="MOQL2018">[MOQL2018]</dt>
      <dd>
<span class="refTitle">"Multi-Objective Virtual Network Embedding Algorithm Based on Q-learning and Curiosity-Driven"</span>, <span class="seriesInfo">DOI 10.1109/TETC.2018.2871549</span>, <time datetime="2018-06" class="refDate">June 2018</time>, <span>&lt;<a href="https://jwcn-eurasipjournals.springeropen.com/articles/10.1186/s13638-018-1170-x">https://jwcn-eurasipjournals.springeropen.com/articles/10.1186/s13638-018-1170-x</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="MVNE2020">[MVNE2020]</dt>
      <dd>
<span class="refTitle">"Modeling on Virtual Network Embedding using Reinforcement Learning"</span>, <span class="seriesInfo">DOI 10.1002/cpe.6020</span>, <time datetime="2020-09" class="refDate">September 2020</time>, <span>&lt;<a href="https://doi.org/10.1002/cpe.6020">https://doi.org/10.1002/cpe.6020</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="MVNNML2021">[MVNNML2021]</dt>
      <dd>
<span class="refAuthor">Boutaba, Raouf.</span>, <span class="refAuthor">Shahriar, Nashid.</span>, <span class="refAuthor">A, Mohammad.</span>, and <span class="refAuthor">Noura. Limam</span>, <span class="refTitle">"Managing Virtualized Networks and Services with Machine Learning"</span>, <span class="seriesInfo">DOI 48b8fc73c1609d4632d7db5e67e373a62a3cc1f6</span>, <time datetime="2021-01" class="refDate">January 2021</time>, <span>&lt;<a href="https://www.semanticscholar.org/paper/Managing-Virtualized-Networks-and-Services-with-Boutaba-Shahriar/48b8fc73c1609d4632d7db5e67e373a62a3cc1f6">https://www.semanticscholar.org/paper/Managing-Virtualized-Networks-and-Services-with-Boutaba-Shahriar/48b8fc73c1609d4632d7db5e67e373a62a3cc1f6</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="NeuroViNE2018">[NeuroViNE2018]</dt>
      <dd>
<span class="refTitle">"NeuroViNE: A Neural Preprocessor for Your Virtual Network Embedding Algorithm"</span>, <span class="seriesInfo">DOI 10.1109/INFOCOM.2018.8486263</span>, <time datetime="2018-06" class="refDate">June 2018</time>, <span>&lt;<a href="https://ieeexplore.ieee.org/document/8486263">https://ieeexplore.ieee.org/document/8486263</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="NFVDeep2019">[NFVDeep2019]</dt>
      <dd>
<span class="refAuthor">Xiao, Y.</span>, <span class="refAuthor">Zhang, Q.</span>, <span class="refAuthor">Liu, F.</span>, <span class="refAuthor">Wang, J.</span>, <span class="refAuthor">Zhao, M.</span>, <span class="refAuthor">Zhang, Z.</span>, and <span class="refAuthor">J. Zhang</span>, <span class="refTitle">"NFVdeep: Adaptive Online Service Function Chain Deployment with Deep Reinforcement Learning"</span>, <span class="seriesInfo">RFC 1129</span>, <span class="seriesInfo">DOI 10.1145/3326285.3329056</span>, <time datetime="2019-06" class="refDate">June 2019</time>, <span>&lt;<a href="https://doi.org/10.1145/3326285.3329056">https://doi.org/10.1145/3326285.3329056</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="NRRL2020">[NRRL2020]</dt>
      <dd>
<span class="refTitle">"Network Resource Allocation Strategy Based on Deep Reinforcement Learning"</span>, <span class="seriesInfo">DOI 10.1109/OJCS.2020.3000330</span>, <time datetime="2020-06" class="refDate">June 2020</time>, <span>&lt;<a href="https://ieeexplore.ieee.org/document/9109671">https://ieeexplore.ieee.org/document/9109671</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="PPRL2020">[PPRL2020]</dt>
      <dd>
<span class="refTitle">"A Privacy-Preserving Reinforcement Learning Algorithm for Multi-Domain Virtual Network Embedding"</span>, <span class="seriesInfo">DOI 10.1109/TNSM.2020.2971543</span>, <time datetime="2020-09" class="refDate">September 2020</time>, <span>&lt;<a href="https://ieeexplore.ieee.org/document/8982091">https://ieeexplore.ieee.org/document/8982091</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="QLDC2019">[QLDC2019]</dt>
      <dd>
<span class="refTitle">"A Q-Learning-Based Approach for Virtual Network Embedding in Data Center"</span>, <span class="seriesInfo">DOI 10.1007/s00521-019-04376</span>, <time datetime="2019-07" class="refDate">July 2019</time>, <span>&lt;<a href="https://link.springer.com/article/10.1007/s00521-019-04376-6">https://link.springer.com/article/10.1007/s00521-019-04376-6</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="QVNE2020">[QVNE2020]</dt>
      <dd>
<span class="refAuthor">Yuan, Y.</span>, <span class="refAuthor">Tian, Z.</span>, <span class="refAuthor">Wang, C.</span>, <span class="refAuthor">Zheng, F.</span>, and <span class="refAuthor">Y. Lv</span>, <span class="refTitle">"A Q-learning-Based Approach for Virtual Network Embedding in Data Center"</span>, <span class="seriesInfo">DOI 10.1007/s00521-019-04376-6</span>, <time datetime="2020-07" class="refDate">July 2020</time>, <span>&lt;<a href="https://link.springer.com/article/10.1007/s00521-019-04376-6">https://link.springer.com/article/10.1007/s00521-019-04376-6</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RDAM2018">[RDAM2018]</dt>
      <dd>
<span class="refTitle">"RDAM: A Reinforcement Learning Based Dynamic Attribute Matrix Representation for Virtual Network Embedding"</span>, <span class="seriesInfo">DOI 10.1109/TETC.2018.2871549</span>, <time datetime="2018-09" class="refDate">September 2018</time>, <span>&lt;<a href="https://ieeexplore.ieee.org/document/8469054">https://ieeexplore.ieee.org/document/8469054</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RFC7364">[RFC7364]</dt>
      <dd>
<span class="refAuthor">Thomas, P.T.</span>, <span class="refAuthor">Eric, Y.</span>, <span class="refAuthor">David, A.</span>, <span class="refAuthor">Luyuan, A.</span>, <span class="refAuthor">Larry, A.</span>, and <span class="refAuthor">A. Maria Napierala</span>, <span class="refTitle">"Problem Statement: Overlays for Network Virtualization"</span>, <time datetime="2015-10" class="refDate">October 2015</time>, <span>&lt;<a href="https://https://datatracker.ietf.org/doc/rfc7364/">https://https://datatracker.ietf.org/doc/rfc7364/</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="RLVNEWSN2020">[RLVNEWSN2020]</dt>
      <dd>
<span class="refTitle">"Reinforcement Learning for Virtual Network Embedding in Wireless Sensor Networks"</span>, <span class="seriesInfo">DOI 10.1109/WiMob50308.2020.9253442</span>, <time datetime="2020-10" class="refDate">October 2020</time>, <span>&lt;<a href="https://ieeexplore.ieee.org/document/9253442">https://ieeexplore.ieee.org/document/9253442</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="SUR2018">[SUR2018]</dt>
      <dd>
<span class="refAuthor">Boutaba, Raouf.</span>, <span class="refAuthor">Salahuddin, Mohammad.</span>, <span class="refAuthor">Limam, Noura.</span>, <span class="refAuthor">Ayoubi, Sara.</span>, <span class="refAuthor">Shahriar, Nashid.</span>, <span class="refAuthor">Estrada-Solano, Felipe.</span>, and <span class="refAuthor">Oscar. M. Caicedo</span>, <span class="refTitle">"A Comprehensive survey on Machine Learning for Networking: Evolution, Applications and Research Opportunities"</span>, <span class="seriesInfo">DOI 10.1186/s13174-018-0087-2</span>, <time datetime="2018-06" class="refDate">June 2018</time>, <span>&lt;<a href="https://link.springer.com/article/10.1186/s13174-018-0087-2">https://link.springer.com/article/10.1186/s13174-018-0087-2</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="VNEGCN2020">[VNEGCN2020]</dt>
      <dd>
<span class="refAuthor">Yan, Z.</span>, <span class="refAuthor">Ge, J.</span>, <span class="refAuthor">Wu, Y.</span>, <span class="refAuthor">Li, L.</span>, and <span class="refAuthor">T. Li</span>, <span class="refTitle">"Automatic Virtual Network Embedding: A Deep Reinforcement Learning Approach With Graph Convolutional Networks"</span>, <span class="seriesInfo">DOI 10.1109/JSAC.2020.2986662</span>, <time datetime="2020-04" class="refDate">April 2020</time>, <span>&lt;<a href="https://ieeexplore.ieee.org/document/9060910">https://ieeexplore.ieee.org/document/9060910</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="VNEQS2021">[VNEQS2021]</dt>
      <dd>
<span class="refAuthor">Wang, Chao.</span>, <span class="refAuthor">Batth, Ranbir Singh.</span>, <span class="refAuthor">Zhang, Peiying.</span>, <span class="refAuthor">Aujla, Gagangeet.</span>, <span class="refAuthor">Duan, Youxiang.</span>, and <span class="refAuthor">Lihua. Ren</span>, <span class="refTitle">"VNE Solution for Network Differentiated QoS and Security Requirements: From the Perspective of Deep Reinforcement Learning"</span>, <span class="seriesInfo">DOI 10.1007/s00607-020-00883-w</span>, <time datetime="2021-01" class="refDate">January 2021</time>, <span>&lt;<a href="https://link.springer.com/article/10.1007/s00607-020-00883-w">https://link.springer.com/article/10.1007/s00607-020-00883-w</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="VNESURV2013">[VNESURV2013]</dt>
      <dd>
<span class="refAuthor">Fischer, Fischer.</span>, <span class="refAuthor">Botero, Juan Felipe.</span>, <span class="refAuthor">Till Beck, Michael;.</span>, <span class="refAuthor">Karim, MM.</span>, <span class="refAuthor">De Meer, Hermann.</span>, and <span class="refAuthor">Xavier. Hesselbach</span>, <span class="refTitle">"Virtual Network Embedding: A Survey"</span>, <span class="seriesInfo">DOI 10.1109/SURV.2013.013013.00155</span>, <time datetime="2020-04" class="refDate">April 2020</time>, <span>&lt;<a href="https://doi.org/10.1109/SURV.2013.013013.00155">https://doi.org/10.1109/SURV.2013.013013.00155</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="VNETD2019">[VNETD2019]</dt>
      <dd>
<span class="refAuthor">Wang, S.</span>, <span class="refAuthor">Bi, J.</span>, <span class="refAuthor">V.Vasilakos, A.</span>, and <span class="refAuthor">Q. Fan</span>, <span class="refTitle">"VNE-TD: A Virtual Network Embedding Algorithm Based on Temporal-Difference Learning"</span>, <span class="seriesInfo">DOI 10.1016/j.comnet.2019.05.004</span>, <time datetime="2019-10" class="refDate">October 2019</time>, <span>&lt;<a href="https://doi.org/10.1016/j.comnet.2019.05.004">https://doi.org/10.1016/j.comnet.2019.05.004</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="VNFFG2020">[VNFFG2020]</dt>
      <dd>
<span class="refAuthor">Anh Quang, P.T.</span>, <span class="refAuthor">Hadjadj-Aoul, Y.</span>, and <span class="refAuthor">A. Outtagarts</span>, <span class="refTitle">"Evolutionary Actor-Multi-Critic Model for VNF-FG Embedding"</span>, <span class="seriesInfo">DOI 10.1109/CCNC46108.2020.9045434</span>, <time datetime="2020-01" class="refDate">January 2020</time>, <span>&lt;<a href="https://ieeexplore.ieee.org/document/9045434">https://ieeexplore.ieee.org/document/9045434</a>&gt;</span>. </dd>
<dd class="break"></dd>
<dt id="ZTORCH2018">[ZTORCH2018]</dt>
    <dd>
<span class="refAuthor">Sciancalepore, V.</span>, <span class="refAuthor">Chen, X.</span>, <span class="refAuthor">Yousaf, F. Z.</span>, and <span class="refAuthor">X. Costa-Perez</span>, <span class="refTitle">"Z-TORCH: An Automated NFV Orchestration and Monitoring Solution"</span>, <span class="seriesInfo">BCP 72</span>, <span class="seriesInfo">RFC 3552</span>, <span class="seriesInfo">DOI 10.1109/TNSM.2018.2867827</span>, <time datetime="2018-08" class="refDate">August 2018</time>, <span>&lt;<a href="https://ieeexplore.ieee.org/document/8450000">https://ieeexplore.ieee.org/document/8450000</a>&gt;</span>. </dd>
<dd class="break"></dd>
</dl>
</section>
<div id="authors-addresses">
<section id="section-appendix.a">
      <h2 id="name-authors-addresses">
<a href="#name-authors-addresses" class="section-name selfRef">Authors' Addresses</a>
      </h2>
<address class="vcard">
        <div dir="auto" class="left"><span class="fn nameRole">Ihsan Ullah</span></div>
<div dir="auto" class="left"><span class="org">KOREATECH</span></div>
<div dir="auto" class="left"><span class="street-address">1600, Chungjeol-ro, Byeongcheon-myeon, Dongnam-gu</span></div>
<div dir="auto" class="left"><span class="locality">Cheonan</span></div>
<div dir="auto" class="left"><span class="region">Chungcheongnam-do</span></div>
<div dir="auto" class="left"><span class="postal-code">31253</span></div>
<div dir="auto" class="left"><span class="country-name">Republic of Korea</span></div>
<div class="email">
<span>Email:</span>
<a href="mailto:ihsan@koreatech.ac.kr" class="email">ihsan@koreatech.ac.kr</a>
</div>
</address>
<address class="vcard">
        <div dir="auto" class="left"><span class="fn nameRole">Youn-Hee Han</span></div>
<div dir="auto" class="left"><span class="org">KOREATECH</span></div>
<div dir="auto" class="left"><span class="street-address">1600, Chungjeol-ro, Byeongcheon-myeon, Dongnam-gu</span></div>
<div dir="auto" class="left"><span class="locality">Cheonan</span></div>
<div dir="auto" class="left"><span class="region">Chungcheongnam-do</span></div>
<div dir="auto" class="left"><span class="postal-code">31253</span></div>
<div dir="auto" class="left"><span class="country-name">Republic of Korea</span></div>
<div class="email">
<span>Email:</span>
<a href="mailto:yhhan@koreatech.ac.kr" class="email">yhhan@koreatech.ac.kr</a>
</div>
</address>
<address class="vcard">
        <div dir="auto" class="left"><span class="fn nameRole">TaeYeon Kim</span></div>
<div dir="auto" class="left"><span class="org">ETRI</span></div>
<div dir="auto" class="left"><span class="street-address">218 Gajeong-ro, Yuseong-gu</span></div>
<div dir="auto" class="left"><span class="locality">Daejeon</span></div>
<div dir="auto" class="left"><span class="postal-code">34129</span></div>
<div dir="auto" class="left"><span class="country-name">Republic of Korea</span></div>
<div class="email">
<span>Email:</span>
<a href="mailto:tykim@etri.re.kr" class="email">tykim@etri.re.kr</a>
</div>
</address>
</section>
</div>
<script>const toc = document.getElementById("toc");
toc.querySelector("h2").addEventListener("click", e => {
  toc.classList.toggle("active");
});
toc.querySelector("nav").addEventListener("click", e => {
  toc.classList.remove("active");
});
</script>
</body>
</html>
