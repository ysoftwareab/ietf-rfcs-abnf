Packets =>| Classifier |=>| Marker |=>| Dropper |=>| Scheduler |=> 
          |            |  |        |  |         |  |           | 
          +------------+  +--------+  +---------+  |-----------+ 
               Figure 1 - DiffServ Model 

   For classifiers, DiffServ uses code-points to describe the service 
   level and drop level of packets. The service level and drop level 
   are essentially relative performance description (or priority) for 
   different classes of traffic flows. One key factor for latency is 
   congestion, and congestion management such as packet queue 
   scheduling further introduces latency uncertainty. A deterministic 
   latency performance (e.g. guarantee or bound) description is still 
   missing while necessary for latency sensitive traffic flows. 

   For traffic conditioning policies, PHBs of DiffServ are applied, 
   including BE (Best Effort), EF (Expedited Forwarding), CS (Class 
   Selector) and AF (Assured Forwarding). These PHBs are designed for 
   only a few flow aggregates, which imply that they cannot provide 
   differentiated services for a potentially large amount of latency 
   sensitive traffic flows with different latency requirements. 

   For service provisioning (or packet scheduling) policies, they are 
   decoupled from traffic conditioning policies and haven't received 
   much attention. For example, in the case where an EF packet 
   arrives at a network device that contains other EF packets in the 
   queue, the latency of scheduling the EF packet is impacted by the 
   size of the queue. Moreover, the order of different latency aware 
 
 
    

   flows arriving the EF queue hasn't been considered. Hence the 
   specific packet scheduling policy in specific network device is 
   import to the latency performance of the packet. 

   In summary, the current DiffServ model is not sufficient for the 
   application that requires guaranteed End-to-End latency. The 
   problems can be listed as below. 

   a) Current QoS mechanism like DiffServ is not well defined to 
      support deterministic latency performance. For example, 
      relative service level or packet priority can not address the 
      congestion factor for latency, as well as the congestion 
      management such as packet scheduling that introduces latency 
      uncertainty. Current PHBs can only support a few flow 
      aggregates which are not sufficient for different latency 
      requirements. 
   b) There is no user-/device-specific latency performance 
      specification, or no control plane mechanism to assign user-
      /device-specific latency requirement to the network devices 
      along the path. As a result, network device has no idea how 
      fast a packet must be forwarded, and cannot adopt a suitable 
      mechanism (e.g. queue scheduling) to guarantee latency. 
   c) There is no mechanism supporting the measurement of flow 
      latency inside of a network device, especially given certain 
      PHB type and code-points of the flow. Such measurement will 
      make End-to-End latency more visible, and thus is crucial for 
      End-to-End latency oriented OAM. 
   d) Service provisioning (or packet scheduling) policies are not 
      specified. Packet scheduling policy and queue status are also 
      key factors of latency and its uncertainty. Therefore packet 
      scheduling policy must be considered to provide deterministic 
      latency service for time sensitive flows. 
   In a nutshell, how to explain the QoS value or how to make sure 
   the QoS value can be used to guarantee latency performance is not 
   well defined yet. Some extension to the current QoS model (e.g. 
   new PHB) could be useful to solve these problems. 

    3.2. Need of Modeling of Latency 

   As mentioned in problem section, QoS value or packet priority 
   cannot guarantee deterministic low latency. In another word, the 
   same QoS value or priority doesn't guarantee same latency 
   performance. In network device, various forwarding mechanisms and 
 
 
    

   interfaces introduce different latency that may be linked to the 
   same priority code. There is still lack of latency performance 
   information that can be used to provide latency guarantee service. 

   The principle of Diffserv is focus on providing, describing 
   differentiated service of traffic flow but not deterministic 
   latency. Instead, queuing and scheduling, as a main part of 
   latency and latency uncertainty, is out of DiffServ's major 
   concern. 

   PHB provides standard way of modeling of device forwarding 
   behavior as well as how to handle each traffic flow. However, PHB 
   description does not include queuing or scheduling information. In 
   reality, latency is dominated by congestion control scheme, which 
   is mainly queuing and scheduling in the network device to take 
   care of multiple traffic flows arriving at the same port 
   simultaneously. 

   Therefore, extension to the current QoS model (e.g. new PHB) is 
   desirable as a standard way to describe the latency performance of 
   network device. With such standard latency model, network device 
   is enabled to better manage the forwarding mechanism (e.g. queue 
   scheduling) for the packet, in order to guarantee End-to-End 
   latency for the service. 

    3.3. Need of Measurement of Flow Latency  

   Flow latency measurement is also very crucial to make sure the 
   latency bound is not violated and useful for End-to-End latency 
   aware OAM mechanism. There is a need to support the measurement of 
   flow latency inside of a network device, especially given certain 
   PHB type and code-points of the flow. 

   Existing technologies such as OWAMP [RFC4656] and TWAMP [RFC5357] 
   is focused on providing one way and two way IP performance metrics. 
   Latency is one of metrics that can be used for End-to-End 
   deterministic latency provisioning. Use OWAMP/TWAMP protocols or 
   extension on that to support measurement of flow latency 
   performance is feasible. 

   Overlay based End-to-End latency measurement is another approach 
   commonly adopted by service providers like CDN vendor. Such 
   approach can be further enhanced by latency measurement inside 
   network device, for better service provisioning, e.g. traffic 
   steering and path selection. 


 
 
    

